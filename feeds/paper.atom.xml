<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Hackthology - Paper</title><link href="https://hackthology.com/" rel="alternate"></link><link href="https://hackthology.com/feeds/paper.atom.xml" rel="self"></link><id>https://hackthology.com/</id><updated>2025-04-02T00:00:00-04:00</updated><entry><title>Speculative Testing at Google with Transition Prediction</title><link href="https://hackthology.com/speculative-testing-at-google-with-transition-prediction.html" rel="alternate"></link><published>2025-04-02T00:00:00-04:00</published><updated>2025-04-02T00:00:00-04:00</updated><author><name>Avi Kondareddy, Sushmita Azad, Eric Nickell, and &lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;</name></author><id>tag:hackthology.com,2025-04-02:/speculative-testing-at-google-with-transition-prediction.html</id><summary type="html">&lt;p&gt;Avi Kondareddy, Sushmita Azad, Eric Nickell, and &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;.
&lt;em&gt;Speculative Testing at Google with Transition Prediction&lt;/em&gt;.  &lt;a href=""&gt;ICST Industry Track 2025&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2025.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/speculative-testing-at-google-with-transition-prediction.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2025.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Google's …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Avi Kondareddy, Sushmita Azad, Eric Nickell, and &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;.
&lt;em&gt;Speculative Testing at Google with Transition Prediction&lt;/em&gt;.  &lt;a href=""&gt;ICST Industry Track 2025&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2025.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/speculative-testing-at-google-with-transition-prediction.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2025.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Google's approach to testing includes both testing prior to code submission
(for fast validation) and after code submission (for comprehensive
validation). However, Google's ever growing testing demand has lead to
increased continuous integration cycle latency and machine costs. When the
post code submission continuous integration cycles get longer, it delays
detecting breakages in the main repository which increases developer friction
and lowers productivity. To mitigate this without increasing resource demand,
Google is implementing Postsubmit Speculative Cycles in their Test Automation
Platform (TAP). Speculative Cycles prioritize finding novel breakages faster.
In this paper we present our new test scheduling architecture and the machine
learning system (Transition Prediction) driving it. Both the ML system and the
end-to-end test scheduling system are empirically evaluated on 3-months of our
production data (120 billion test$\times$cycle pairs, 7.7 million breaking
targets, with $\sim$20 thousand unique breakages). Using Speculative Cycles we
observed a median (p50) reduction of approximately 65\% (from 107 to 37 
minutes) in the time taken to detect novel breaking targets.&lt;/p&gt;
&lt;h1 id="sec:introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Continuous integration (CI) coordinates the development activity of large numbers of
  developers &lt;span class="citation" data-cites="Fowler2006"&gt;(&lt;a href="#ref-Fowler2006" role=
  "doc-biblioref"&gt;Fowler 2006&lt;/a&gt;)&lt;/span&gt;. When two developers are working in the same portion of
  the code base, continuous integration ensures that conflicting changes combine into a
  conflict-free version before reaching the end user. In general, adopters of CI execute builds and
  tests to ensure that the final release artifact (server binary, mobile application, etc...)
  passes all relevant tests. These build and tests are executed frequently to reduce the amount of
  time conflicts are present in the code base. Continuous integration is both a system and a
  practice of automatically merging changes into a source of truth for an organization's source
  code and related artifacts.&lt;/p&gt;
&lt;p&gt;For small software repositories and organizations, the implementation of continuous
  integration is well supported by off-the-shelf software such as Github Actions, CircleCI,
  Jenkins, and numerous other tools. However, as the repository and organization scales up
  challenges emerge.&lt;/p&gt;
&lt;p&gt;As an organization adds projects and engineers, there are two distinct paths that emerge: the
  many-small-repository path and the single-mono-repository path. Both paths have distinct
  challenges and advantages. Neither path will enable organizations to use vanilla off-the-shelf
  solutions for their development environment. This paper is focused on a particular challenge
  faced in a mono-repository environment at Google, but similar problems arise in organizations
  with many small repositories that are strongly coupled together.&lt;/p&gt;
&lt;p&gt;Google, an early adopter of mono-repositories and continuous integration &lt;span class=
  "citation" data-cites="Potvin2016"&gt;(&lt;a href="#ref-Potvin2016" role="doc-biblioref"&gt;Potvin and
  Levenberg 2016&lt;/a&gt;)&lt;/span&gt;, faces an enormous ever expanding code base that is centrally tested
  by the Test Automation Platform (TAP) &lt;span class="citation" data-cites="Micco2012"&gt;(&lt;a href=
  "#ref-Micco2012" role="doc-biblioref"&gt;Micco 2012&lt;/a&gt;)&lt;/span&gt;. As the code base and company grows,
  so does the &lt;em&gt;demand&lt;/em&gt; for compute resources for continuous testing. Left unchecked, we have
  observed a double digit percentage organic demand growth rate year-over-year (compounding). Such
  a growth rate is untenable even for a large company like Google. To prevent this unchecked growth
  in resource demand, TAP has long had features that reduce demand by skipping some tests (before
  submission) and batching many versions together (after submission) to amortize the cost
  &lt;span class="citation" data-cites=
  "Gupta2011 Bland2012 Micco2012 Micco2013 Memon2017 Leong2019 Wang2020 Wang2021 Henderson2023 Henderson2024 Hoang2024"&gt;
  (&lt;a href="#ref-Gupta2011" role="doc-biblioref"&gt;Gupta, Ivey, and Penix 2011&lt;/a&gt;; &lt;a href=
  "#ref-Bland2012" role="doc-biblioref"&gt;Bland 2012&lt;/a&gt;; &lt;a href="#ref-Micco2012" role=
  "doc-biblioref"&gt;Micco 2012&lt;/a&gt;, &lt;a href="#ref-Micco2013" role="doc-biblioref"&gt;2013&lt;/a&gt;; &lt;a href=
  "#ref-Memon2017" role="doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;; &lt;a href="#ref-Leong2019" role=
  "doc-biblioref"&gt;Leong et al. 2019&lt;/a&gt;; &lt;a href="#ref-Wang2020" role="doc-biblioref"&gt;K. Wang et
  al. 2020&lt;/a&gt;, &lt;a href="#ref-Wang2021" role="doc-biblioref"&gt;2021&lt;/a&gt;; &lt;a href="#ref-Henderson2023"
  role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;, &lt;a href="#ref-Henderson2024" role=
  "doc-biblioref"&gt;2024&lt;/a&gt;; &lt;a href="#ref-Hoang2024" role="doc-biblioref"&gt;Hoang and Berding
  2024&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In this paper, we are primarily concerned with testing that occurs after the code has been
  submitted. We examine how to improve the developer experience by finding novel build/test
  breakages faster while continuing to control our resource footprint growth rate.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="the-google-development-environment"&gt;The Google Development Environment&lt;/h2&gt;
&lt;p&gt;Google's development environment uses a centralized repository &lt;span class="citation"
  data-cites="Potvin2016"&gt;(&lt;a href="#ref-Potvin2016" role="doc-biblioref"&gt;Potvin and Levenberg
  2016&lt;/a&gt;)&lt;/span&gt; where code changes are managed with Blaze/Bazel&lt;a href="#fn1" class=
  "footnote-ref" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and automatically tested,
  primarily by TAP. Developers write code, build and test locally, then submit it for review via
  Critique &lt;span class="citation" data-cites="Sadowski2018"&gt;(&lt;a href="#ref-Sadowski2018" role=
  "doc-biblioref"&gt;Sadowski et al. 2018&lt;/a&gt;)&lt;/span&gt;, triggering Presubmit checks. After review and
  approval, changes undergo further checks before merging. Due to high submission rates, Presubmit
  testing is limited to avoid excessive resource consumption and delays.&lt;/p&gt;
&lt;h2 id="tap-postsubmit"&gt;TAP Postsubmit&lt;/h2&gt;
&lt;p&gt;When a developer's change gets submitted it may still have a bug that breaks an existing test
  or causes a compile breakage in another part of the repository. To find these bugs that have
  slipped through the pre-submission testing and validation process, TAP also has a
  "post-submission" mode (TAP Postsubmit).&lt;/p&gt;
&lt;p&gt;In Postsubmit, TAP periodically (subject to compute resource availability in the Build system)
  schedules &lt;em&gt;all&lt;/em&gt; tests that have been &lt;em&gt;affected&lt;/em&gt; (based on build dependencies) since
  the last run. We refer this execution cycle as &lt;em&gt;Comprehensive Testing Cycle&lt;/em&gt;. Previously,
  this cycle has also been referred as Milestones &lt;span class="citation" data-cites=
  "Memon2017"&gt;(&lt;a href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;)&lt;/span&gt;. There
  are two primary objectives for the Comprehensive Cycle:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Uncover all test failures at the version comprehensive testing was conducted at.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Provide project health signals for downstream services to trigger more expensive testing
      and/or start production rollout by triggering release automation&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;p&gt;As of today's writing, it takes &lt;span class="math inline"&gt;∼&lt;/span&gt;1-2 hours for a test broken
  by a developer's change to start failing in TAP Postsubmit. Once the failure is detected
  automatic culprit finders &lt;span class="citation" data-cites="Henderson2023"&gt;(&lt;a href=
  "#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt; and rollback system
  &lt;span class="citation" data-cites="Henderson2024"&gt;(&lt;a href="#ref-Henderson2024" role=
  "doc-biblioref"&gt;Henderson et al. 2024&lt;/a&gt;)&lt;/span&gt; will spring into action to help keep the
  repository healthy and our developers productive. Automatically rolling back the change (after
  the first breakage was detected) might take as little as 30 minutes or as long as several hours
  depending on a number of factors. Thus, it could be 4 hours or more before our developer learns
  they have broken a test and either been notified or automatically had their change rolled
  back.&lt;/p&gt;
&lt;h2 id="cost-of-breakages"&gt;Cost of Breakages&lt;/h2&gt;
&lt;p&gt;The longer it takes to identify and fix a code breakage, the more challenging and expensive
  the remediation process becomes. This delay can lead to a loss of context for the original
  developer, potentially requiring teammates to resolve the issue. Additionally, a bad change can
  affect other developers, especially if it occurs in widely used parts of the code base. When
  interrupted by such a failure programmers need to distinguish between the change they are making
  and existing fault in the repository. Ultimately, prolonged breakages can disrupt release
  automation and even delay production releases.&lt;/p&gt;
&lt;h2 id="our-contributions-and-findings"&gt;Our Contributions and Findings&lt;/h2&gt;
&lt;p&gt;In this paper we are looking to increase developer productivity by reducing their friction
  with respect to broken code in the main code base. Specifically we are solving the following
  problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can CI minimize the time between the submission of a faulty change and its
  identification within a continuous integration (CI) system to accelerate the detection and
  mitigation of bugs?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To solve the problem, we propose a new scheduling mode to our CI system, Speculative Testing
  Cycles, that opportunistically runs a small subset of tests to uncover novel failures and reduce
  the mean time to detection (MTTD) for new failures. Speculative Cycle prioritizes finding novel
  breakages by identifying tests that are very likely to fail. The new scheduling mode is driven by
  a &lt;em&gt;Transition Prediction&lt;/em&gt; model that predicts when tests are likely to (newly) fail. It
  utilizes shallow machine-learning to run a smaller batch of tests predicted to be more likely to
  be broken at higher frequency in order to find breakages faster. We discuss both the design and
  constraints on the production Speculative Cycle system and the shallow machine-learning model
  (Transition Prediction) that powers this predictive test selection.&lt;/p&gt;
&lt;p&gt;While the production system is highly tied to Google's unique development environment and
  organizational constraints (i.e.: large monorepos with centralized CI infrastructure), our ML
  model presents a very coarse-grain approach to prediction for test selection using test and
  code-under-test metadata that has equivalents in most development environments and is
  language/framework agnostic.&lt;/p&gt;
&lt;p&gt;Our contributions in this paper are therefore:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Speculative Cycles: A system for frequent / cost-aware batch testing.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Transition Prediction (TRANSPRED): Predicting target status transitions (Pass to Fail) for
      arbitrary build &amp;amp; test targets. With a budget of 25% of the total targets, Transition
      Prediction achieves a 85% recall rate in detecting breakages.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;A very large scale study (utilizing 3-months of production data: 120 billion
      test&lt;span class="math inline"&gt;×&lt;/span&gt;cycle pairs, 7.7 million breaking targets, and
      &lt;span class="math inline"&gt;∼&lt;/span&gt;20,000 unique breakages) on the efficacy of Speculative
      Testing using Transition Prediction evaluating its performance against randomized
      testing.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;A comparative assessment of dataset and training configurations for Transition Prediction,
      determining that the selection of features and the length of the training window
      substantially influence the system's performance.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;h1 id="background"&gt;Background&lt;/h1&gt;
&lt;h2 id="developer-services"&gt;Developer Services&lt;/h2&gt;
&lt;h3 id="bazel"&gt;Bazel&lt;/h3&gt;
&lt;p&gt;Google uses Bazel as its build system monorepo wide. Bazel allows specifying build/test
  "targets" that depend on other targets. Each target is comprised of a set of "actions" to be
  executed. Therefore, the execution of a target corresponds to a Directed Acyclic Graph (DAG) of
  actions. Given Bazel makes dependency management declarative and all dependencies are built at
  the same version of the codebase, builds are ostensibly held to be hermetic - builds at the same
  version of the codebase should produce the same output for each action. This opens up the
  possibility for massive compute savings through caching of intermediate actions across multiple
  builds.&lt;/p&gt;
&lt;h3 id="forge"&gt;Forge&lt;/h3&gt;
&lt;p&gt;Build/Test actions at Google run under a centralized compute cluster called Forge which
  attempts to cache these intermediate actions to avoid recomputation at the same version of the
  codebase. For Postsubmit testing, we make use of this caching by batching builds/tests at single
  versions every hour or so.&lt;/p&gt;
&lt;h2 id="comprehensive-testing-cycles"&gt;Comprehensive Testing Cycles&lt;/h2&gt;
&lt;p&gt;In the standard case, TAP Postsubmit performs each Comprehensive Testing Cycle once the
  previous cycle's active consumption of resources drops below a threshold of their allocated build
  system resources. This cycle consists of picking a recent change(snapshot of the repository), at
  which we batch and run a "comprehensive" set of tests. Comprehensive cycles run all tests that
  are "affected" since the previous cycle. "Affected" is our term for dependence as implied by the
  build system's package graph, allowing us to perform a degree of static test selection at a
  coarse granularity.&lt;/p&gt;
&lt;h2 id="ancillary-systems-for-build-gardening"&gt;Ancillary Systems For Build Gardening&lt;/h2&gt;
&lt;p&gt;The development life cycle explained above relies on Postsubmit testing to make up for the
  lack of comprehensive testing prior to submission. This introduces the concept of &lt;em&gt;Postsubmit
  breakages&lt;/em&gt; which are caught sometime after submission. The process of detecting the breakage,
  identifying the cause of the breakage (culprit finding), and fixing the breakage (sometimes via a
  rollback) is called "Build Gardening" at Google.&lt;/p&gt;
&lt;h3 id="culprit-finding"&gt;Culprit Finding&lt;/h3&gt;
&lt;p&gt;We have previously outlined Google's culprit finding approach in &lt;span class="citation"
  data-cites="Henderson2023 Henderson2024"&gt;(&lt;a href="#ref-Henderson2023" role=
  "doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;, &lt;a href="#ref-Henderson2024" role=
  "doc-biblioref"&gt;2024&lt;/a&gt;)&lt;/span&gt;. To formulate the problem, we are given a test/build "target"
  and must determine at which change &lt;span class="math inline"&gt;&lt;em&gt;C&lt;/em&gt;&lt;/span&gt; over a range of
  sequential changes &lt;span class="math inline"&gt;[&lt;em&gt;A&lt;/em&gt;, &lt;em&gt;B&lt;/em&gt;]&lt;/span&gt; did the target start
  breaking. The change &lt;em&gt;C&lt;/em&gt; is referred as a &lt;em&gt;Culprit Change&lt;/em&gt; or simply
  &lt;em&gt;Culprit&lt;/em&gt;. Given the presence of build/test non-determinism ("flakes"), we want to find
  the change at which the test was "truly" failing but "truly" passing at the previous change. Our
  previous work explores how we do this in a time and run-efficient manner using a historical flake
  rate aware Bayesian model to sample runs.&lt;/p&gt;
&lt;h3 id="sec:verifier"&gt;Culprit Verifier&lt;/h3&gt;
&lt;p&gt;Unfortunately, persistent flakiness and non-determinism can still slip by the culprit finders.
  The pathological case is that of build system non-determinism. Google's build system Bazel
  assumes that builds are hermetic and should produce the same output at the same version. This
  assumption means that unlike tests, failed actions are cached by the build system so immediate
  reruns will continue to assert what could potentially be a flaky build failure. The culprit
  verifier does extensive reruns on a subset of culprit conclusions produced by the culprit
  finders. We sample from two different sampling frames: (1) a simple random sample of all
  conclusions produced and (2) the first conclusion produced for each uniquely blamed culprit. For
  details on the design of the verification system, see the Flake Aware Culprit Finding paper
  &lt;span class="citation" data-cites="Henderson2023"&gt;(&lt;a href="#ref-Henderson2023" role=
  "doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt; and the SafeRevert paper &lt;span class="citation"
  data-cites="Henderson2024"&gt;(&lt;a href="#ref-Henderson2024" role="doc-biblioref"&gt;Henderson et al.
  2024&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Although the verifier is fallible, the dataset it produces is still the most accurate
  representation of true breakages at Google. We know it fails because we have seen specific
  examples of its failures. But, from a measurement perspective, bounding its accuracy is
  challenging because its failure rate is low enough that it exceeds our ability to accurately
  measure. We have recently conducted manual verification while launching a new version of our Auto
  Rollback system on all culprits rolled back during the "dogfood" phase (an opt-in beta). Out of
  250+ hand verified culprits: 5 were incorrect culprits from the culprit finder, 2 incorrect
  culprits were incorrectly verified correct by the verifier. This gives us an estimated culprit
  finding accuracy of 98% (matching verifier produced accuracy dashboard) and a verifier accuracy
  of 99.2%. Given the low sample size the confidence interval on that measurement is &lt;span class=
  "math inline"&gt;99.2 ± 3.3%&lt;/span&gt; which is too wide. To get sufficient power to accurately
  estimate the verifier accuracy, we would need to hand verify at least 5000 culprits.&lt;/p&gt;
&lt;p&gt;Thus, while we are confident in the overall accuracy measurements provided by the verifier, we
  acknowledge that "ground truth" is difficult to obtain. We utilize the dataset produced by the
  verifier to label the culprits to train our prediction model (detailed below). This approach
  vastly reduces the impact of flakiness on our model training data.&lt;/p&gt;
&lt;h3 id="sec:autorollback"&gt;Autorollback&lt;/h3&gt;
&lt;p&gt;At Google, there is a developer guideline to prefer rollbacks to fix forwards. Given this
  context, it is appropriate to automatically roll back a change if someone is confident that the
  change broke the build. Unfortunately, given the accuracy issue of the culprit finder described
  above, we can't immediately rollback upon a culprit finding completion for one target. In the
  SafeRevert paper &lt;span class="citation" data-cites="Henderson2024"&gt;(&lt;a href="#ref-Henderson2024"
  role="doc-biblioref"&gt;Henderson et al. 2024&lt;/a&gt;)&lt;/span&gt;, we discussed how we attempt to determine
  the amount of evidence needed to proceed with a rollback. In practice, we currently restrict
  rollbacks to changes that break at least 10 targets.&lt;/p&gt;
&lt;h2 id="the-evolution-of-tap"&gt;The Evolution of TAP&lt;/h2&gt;
&lt;p&gt;Prior to development of TAP, Google had a more traditional continuous integration system.
  Primarily, the decision to run continuous integration was left up to individual teams and the
  system that existed was federated: teams brought their own capacity (machines, possibly under
  their desks) and had to configure and run the CI system(s). TAP, upon launch in 2009, was a
  massive improvement from setup and continuous maintenance perspective alone. Unlike the previous
  systems, TAP ran all builds centrally and only required a small amount of configuration: it
  simply required users to specify which paths they would like to test and where the (failure)
  notifications should go.&lt;/p&gt;
&lt;p&gt;The central builds and low configuration overhead made TAP immediately successful. Within a
  few years it had completely displaced the prior system. But, success came at a price: it was
  plagued by build capacity limitations and scalability challenges stemming from design decisions
  of running tests on each change in the initial implementation. By 2012 (following a full launch
  in 2010) Google was running low on machine capacity to run TAP. The testing model had naively
  assumed that the build dependence based test selection would be enough to control demand; It was
  not.&lt;/p&gt;
&lt;p&gt;TAP both pre- and post-submit were thus rewritten (multiple times). TAP Presubmit now has
  several different modes, adapts to resource availability, uses machine learning driven test
  selection, and has "advisors" which can ignore or rerun tests on failure &lt;span class="citation"
  data-cites="Hoang2024"&gt;(&lt;a href="#ref-Hoang2024" role="doc-biblioref"&gt;Hoang and Berding
  2024&lt;/a&gt;)&lt;/span&gt;. Unfortunately for you (the reader) discussing these innovations is out of scope
  for this manuscript.&lt;/p&gt;
&lt;p&gt;Today, TAP Postsubmit no longer runs on every change. It waits for there to be capacity in the
  Build System &lt;span class="citation" data-cites="Wang2020 Wang2021"&gt;(&lt;a href="#ref-Wang2020" role=
  "doc-biblioref"&gt;K. Wang et al. 2020&lt;/a&gt;, &lt;a href="#ref-Wang2021" role=
  "doc-biblioref"&gt;2021&lt;/a&gt;)&lt;/span&gt; and then enqueues all tests that have been affected since their
  last definitive status. At Google's scale and growth rate, we have had to continuously innovate
  and rewrite core parts of TAP just to maintain this model. However, we have long known that just
  using build-graph dependency based test selection in Postsubmit was untenable in the long term
  &lt;span class="citation" data-cites="Memon2017 Leong2019"&gt;(&lt;a href="#ref-Memon2017" role=
  "doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;; &lt;a href="#ref-Leong2019" role="doc-biblioref"&gt;Leong et al.
  2019&lt;/a&gt;)&lt;/span&gt;. It is only now that we have both the high-accuracy culprit finding
  infrastructure &lt;span class="citation" data-cites="Henderson2023 Henderson2024"&gt;(&lt;a href=
  "#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;, &lt;a href="#ref-Henderson2024"
  role="doc-biblioref"&gt;2024&lt;/a&gt;)&lt;/span&gt; and the organizational will to fundamentally change (again)
  core assumptions for how TAP works.&lt;/p&gt;
&lt;h2 id="the-current-dilemma"&gt;The Current Dilemma&lt;/h2&gt;
&lt;p&gt;The cost of TAP Postsubmit's &lt;em&gt;comprehensive&lt;/em&gt; testing continues to rise due to
  ever-increasing compute demand, increasing fleet machine diversity, and evolving development
  practices. This means that with no systemic changes in our approach towards Postsubmit
  testing:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Teams start needing to wait longer and longer for signal on their projects' health (or
      "greenness") from TAP blocking their releases which has significant monetary impact&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Developers are now alerted hours after they submit of a breaking change, creating
      developer toil&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;h1 id="speculative-testing-cycles"&gt;Speculative Testing Cycles&lt;/h1&gt;
&lt;p&gt;In order to protect release quality and improve developer productivity, TAP is introducing
  &lt;em&gt;Speculative Testing Cycles&lt;/em&gt; - a process by which we non-deterministically select a
  smaller subset of targets that we deem more likely to fail, and run this smaller set more
  frequently than traditional comprehensive cycles. For traditional comprehensive test cycles, we
  attempt to batch as many builds/tests together at the same change as possible to get the caching
  benefits from Forge. We do the same here for Speculative Cycles, picking a single change
  approximately every 20 minutes, Forge build resources allowing. As with Comprehensive Cycles, we
  first perform static build system level dependency analysis to find all targets which could have
  a status change.&lt;/p&gt;
&lt;h2 id="determining-breakage-likelihood"&gt;Determining Breakage Likelihood&lt;/h2&gt;
&lt;p&gt;We now have a set of statically-filtered targets &lt;span class="math inline"&gt;{&lt;em&gt;T&lt;/em&gt;}&lt;/span&gt;
  and a sequence of commits &lt;span class="math inline"&gt;{&lt;em&gt;C&lt;/em&gt;}&lt;/span&gt; consisting of all commits
  since the last Speculative Cycle. Our problem is then to determine for a given target
  &lt;span class="math inline"&gt;&lt;em&gt;t&lt;/em&gt; ∈ &lt;em&gt;T&lt;/em&gt;&lt;/span&gt;: what is the likelihood it was broken by
  a change &lt;span class="math inline"&gt;&lt;em&gt;c&lt;/em&gt; ∈ &lt;em&gt;C&lt;/em&gt;&lt;/span&gt; affecting it? For compute
  tractability, given the frequency of change submissions and number of targets in our codebase, we
  simplify by choosing to aggregate across C and ask: "Was target &lt;span class=
  "math inline"&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt; broken by any change in &lt;span class=
  "math inline"&gt;&lt;em&gt;C&lt;/em&gt;&lt;/span&gt;?" We identify this problem formulation and approaches to answer
  it as &lt;em&gt;Transition Prediction&lt;/em&gt;, and discuss it in detail in the next section.&lt;/p&gt;
&lt;h2 id="making-scheduling-decisions"&gt;Making Scheduling Decisions&lt;/h2&gt;
&lt;p&gt;Our Transition Prediction model gives us "scores" from 0 to 1 indicating the propensity of
  this target to be newly broken. Given that we rely on machine learning models for these
  predictions, we must be careful to note that these scores do not meaningfully represent
  probabilities. They are the product of an algorithm attempting to minimize a loss function
  relative to its training dataset. While they can be colloquially understood to imply likelihood
  and allow relative comparison, moving the threshold at which scores map onto decisions to (not)
  schedule will produce non-linear changes to the actual observed probability of finding a breakage
  ("recall") or scheduling false positives ("false positive rate").&lt;/p&gt;
&lt;p&gt;Given this fact and our desire to have stable execution cost/latency under our scheduling
  scheme, we choose to primarily rank targets and schedule the top-k highest ranking targets under
  some top K parameter. For the purpose of this paper, this is our main parameter for tuning the
  Speculative Cycle system while the remaining knobs exist in feature selection and training
  configuration for the Transition Prediction ML model.&lt;/p&gt;
&lt;h2 id="goal-of-finding-test-breakages"&gt;Goal of Finding Test Breakages&lt;/h2&gt;
&lt;p&gt;As we've discussed, Postsubmit testing feeds into both Release pipelines and the breakage
  triage/fixing workflow (what we call "Build Gardening"). Release implications of Speculative
  Cycle(s) depend on many several other projects at play which are outside the scope of this paper.
  The direct outcome is that newly detected build/test failures will now propagate to Build
  Gardening consumers faster, namely:&lt;/p&gt;
&lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;Developers directly through emails sent upon detection of a failure&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Automated Culprit Finders that attempt to identify the specific culprit change&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Auto-Rollback reverts changes once it acquires enough evidence from culprit finders&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;

&lt;p&gt;&lt;span id="fig:culprit-prop" label="fig:culprit-prop"&gt;
    &lt;a href="images/icst-2025/fig1.png"&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;&lt;/em&gt;
    &lt;img alt="Figure 1" src="images/icst-2025/fig1.png"&gt;&lt;/a&gt;
  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A developer or the automated culprit finders may only need a single target breakage (We will
  denote this criterion as &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(1)&lt;/span&gt;)
  to consider investigating and to identify a breaking change that needs to be reverted. But given
  the non-determinism issues discussed above, auto-rollback requires a stronger signal - at least
  &lt;span class="math inline"&gt;&lt;em&gt;N&lt;/em&gt;&lt;/span&gt; distinct targets were culprit-found to the same
  change (&lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(&lt;em&gt;N&lt;/em&gt;)&lt;/span&gt;).
  Both these goals are important to the health of the codebase, and while the former is a simple
  and more intuitive metric, there is a strong argument to be made for the latter - in that the
  number of distinct broken targets is a good proxy for the cost of that bad commit. The larger the
  number of broken targets, the larger the likelihood that it falls under a configured Presubmit
  testing directory, impacting developer productivity like we discussed above. Targets could belong
  to multiple release-level project groups, and more targets being broken is a higher cost to
  overall release health with more automation and production pushes being delayed or blocked.
  Figure &lt;a href="#fig:culprit-prop" data-reference-type="ref" data-reference=
  "fig:culprit-prop"&gt;[fig:culprit-prop]&lt;/a&gt; presents the total proportion of culprits remaining as
  we increase &lt;span class="math inline"&gt;&lt;em&gt;N&lt;/em&gt;&lt;/span&gt; which follows a power law
  distribution.&lt;/p&gt;
&lt;h2 id="evaluating-performance"&gt;Evaluating Performance&lt;/h2&gt;
&lt;p&gt;Culprit detection is then split between Comprehensive and Speculative Cycles. Under a specific
  scheduling scheme, Speculative Cycles successfully detect some proportion of culprits before
  Comprehensive Cycles. This proportion is the recall of the system. This recall in combination
  with the actual run frequency and execution runtime of the two cycles determines the actual
  reduction in culprit detection latency, where latency is defined relative to the submission time
  of the culprit change. Depending on the consumer, latency improvements will have non-linear
  marginal payoffs. For example, reduction from half a day to an hour may have measurable impacts
  on development friction caused by failing tests, but reduction from a half hour to 10 minutes may
  be negligible. Therefore, both metrics can be compared to the cost of a specific scheme in order
  to justify value / viability of the system.&lt;/p&gt;
&lt;h1 id="transition-prediction"&gt;Transition Prediction&lt;/h1&gt;
&lt;p&gt;Transition Prediction (or &lt;em&gt;TRANSPRED&lt;/em&gt; for short) attempts to predict transition
  likelihood every 20 minutes for millions of targets with a submission rate of over tens of
  thousands of changes per day. In order to make this tractable and most importantly
  cost-beneficial relative to simply running more tests, our predictions must be cheap and low
  latency. This informs using simple tree-based model techniques like Gradient Boosted Decision
  Trees and Random Forests. Tree-based models are used across industry for being low cost for
  inference, avoid over-fitting, and achieve high performance with simple coarse grained features
  like change and target metadata. We have found consistently throughout our work that Gradient
  Boosted Tree models perform best for Google's coarse grained test selection problems.&lt;/p&gt;
&lt;h2 id="features"&gt;Features&lt;/h2&gt;

&lt;p&gt;&lt;span id="fig:feature-collection" label="fig:feature-collection"&gt;
    &lt;a href="images/icst-2025/fig2.png"&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;&lt;/em&gt;
    &lt;img alt="Figure 2" src="images/icst-2025/fig2.png"&gt;&lt;/a&gt;
  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;a href="#fig:feature-collection" data-reference-type="ref" data-reference=
  "fig:feature-collection"&gt;[fig:feature-collection]&lt;/a&gt; shows an overview of the structure of the
  feature vector. In a production system, only features that can be acquired quickly and reliably
  at scale can be used. The end-to-end latency of deciding which tests to run (including fetching
  the data, assembling the feature vectors, and running the prediction) needs to be well under 20
  minutes such that the current Speculative Cycle finishes before the next one begins. Thus, we are
  using simple, cheap to compute, coarse grained metadata features from targets and commits. The
  predictions are done per target so the features for one target are included in each feature
  vector. However, the predictions are not done for a single commit but across a range of commits.
  To increase the robustness of the commit based features, we only include commits that
  &lt;em&gt;affect&lt;/em&gt; the target. A commit affects a target if the target is reachable in the build
  graph from the files modified in the change.&lt;/p&gt;
&lt;p&gt;Target level features include static and dynamic characteristics. The static items are
  unchanging (or very rarely changing) and include target's programming language, whether it is a
  test or build target, and its Bazel rule type, etc. The dynamic items are based on historical
  execution data, such as the failure count in Postsubmit and Presubmit over several different time
  windows. Commit level features include: change metadata like lines of code modified, number of
  reviewers, number of linked bugs, description, etc. Finally, some features correspond to the
  relationship between the target and the changes in this specific cycle - specifically its build
  graph distance to the files modified and whether the target was run at Presubmit time.&lt;/p&gt;

&lt;p&gt;&lt;span id="table:featureDescription" label="table:featureDescription"&gt;
    &lt;a href="images/icst-2025/table1.png"&gt;&lt;em&gt;&lt;strong&gt;Table 1&lt;/strong&gt;&lt;/em&gt;
    &lt;img alt="Table 1" src="images/icst-2025/table1.png"&gt;&lt;/a&gt;
  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Table &lt;a href="#table:featureDescription" data-reference-type="ref" data-reference=
  "table:featureDescription"&gt;[table:featureDescription]&lt;/a&gt; contains details on the features used
  in the production model ('BASE') and those added for this paper ('AUG'), for augmented).&lt;/p&gt;
&lt;h2 id="labeling"&gt;Labeling&lt;/h2&gt;
&lt;p&gt;Our data set has a row/vector for each target in each Speculative Cycle. In order to train the
  model, we need to label each vector with whether or not that target should be run in the given
  Speculative Cycle. Optimally the targets should only be run when a new failure is introduced into
  the repository. We use the verified culprits data set described previously &lt;span class="citation"
  data-cites="Henderson2023 Henderson2024"&gt;(&lt;a href="#ref-Henderson2023" role=
  "doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;, &lt;a href="#ref-Henderson2024" role=
  "doc-biblioref"&gt;2024&lt;/a&gt;)&lt;/span&gt; as our source of truth on which commits introduced breakages.
  Unfortunately, the automated culprit finders de-prioritize comprehensiveness in favor of finding
  results quickly for active breakages blocking developers. This can leave some target level
  breakages with no culprit-identifying labels for up to two weeks after the failures was
  detected.&lt;/p&gt;
&lt;p&gt;In addition, some flakiness in tests and builds may be caused by something outside of the code
  and configuration checked into the repository. For example, a buggy compiler action may
  non-deterministically miscompile a file causing the linker to fail when it happens. Because the
  compile action "succeeds" (produces an output and no error code), the output of the compiler (the
  object file) will be cached. Subsequent builds will re-use the cached output and fail when
  attempting to link. However, if the cached item expires or the build happens in a different data
  center with a different cache the linker action may succeed (when the compiler behaved
  correctly). This type of flakiness is difficult to diagnose real-time with only build
  re-executions due to the multiple layers of caching. However, build executions separated in time
  have a better chance of identifying the flaky behavior.&lt;/p&gt;
&lt;p&gt;In order to protect against unavailability of data, we perform labeling a full week after the
  inference examples are acquired. As we've described, this still does not completely shield us
  from both incorrect positives and lack of coverage but is the best ground truth we have.&lt;/p&gt;
&lt;h2 id="super-extreme-class-imbalance"&gt;Super Extreme Class Imbalance&lt;/h2&gt;
&lt;p&gt;Postsubmit breakages are exceedingly rare. The only target-vector rows in our dataset that
  need to be scheduled are targets that are newly broken. This only occurs 0.0001% of the time. In
  model training, we refer to the class with fewer examples at the &lt;em&gt;minority class&lt;/em&gt; while
  the class with more examples is the &lt;em&gt;majority class&lt;/em&gt;. In our problem, the minority class
  is these newly broken targets just described.&lt;/p&gt;
&lt;p&gt;When there are too few examples in the minority class, the effectiveness of the many training
  algorithms is reduced. The "loss" (e.g. the function model training is optimizing) becomes biased
  towards the majority class. A classic pathological case is a model that trivially predicts the
  majority class - that is, predicts the majority class every time and never predicts the minority
  class. These models will actually minimize the loss function even though they never make useful
  predictions. Our approach must guard against these biases if we want to usefully predict which
  tests to run during a Speculative Cycle.&lt;/p&gt;
&lt;p&gt;For our system, we downsample the negative class (targets that are not newly failing). This
  reduces the imbalance between the classes. It also implicitly values the True Positives (new
  breakages) higher than the True Negatives. Every false negative (missed breakage) leads to
  developer toil as the detection of the breakage is delayed. Given the super extreme imbalance
  between the classes, we expect most targets run during a Speculative Cycle are to be False
  Negatives (and not indicate a new breakage).&lt;/p&gt;
&lt;h2 id="training-configuration"&gt;Training Configuration&lt;/h2&gt;
&lt;p&gt;For our model, we use the Yggdrasil Decision Forest library (YDF) &lt;span class="citation"
  data-cites="Guillame-Bert2023"&gt;(&lt;a href="#ref-Guillame-Bert2023" role=
  "doc-biblioref"&gt;Guillame-Bert et al. 2023&lt;/a&gt;)&lt;/span&gt;. YDF touts superior decision tree sampling
  and splitting algorithms over prior algorithms (such as XGBoost &lt;span class="citation"
  data-cites="chen2016xgboost"&gt;(&lt;a href="#ref-chen2016xgboost" role=
  "doc-biblioref"&gt;&lt;strong&gt;chen2016xgboost?&lt;/strong&gt;&lt;/a&gt;)&lt;/span&gt;) and provides better integration
  with Google's machine learning infrastructure.&lt;/p&gt;
&lt;p&gt;Conventional training schemes randomly shard their dataset between train, validation, and test
  splits or perhaps perform cross-validation with a set of parallel splits. For our problem,
  randomized splits are harmful and lead to us overestimating our performance as we leak
  time-dependent attributes of the repository across sets. Instead, we create time-ordered splits
  where the train set consists of the first &lt;span class="math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt; days, the
  validation set the next &lt;span class="math inline"&gt;&lt;em&gt;B&lt;/em&gt;&lt;/span&gt; days, and the test set the
  final &lt;span class="math inline"&gt;&lt;em&gt;C&lt;/em&gt;&lt;/span&gt; days.&lt;/p&gt;
&lt;h1 id="sec:evaluation"&gt;Empirical Evaluation&lt;/h1&gt;
&lt;h2 id="evaluation"&gt;Evaluation&lt;/h2&gt;
&lt;p&gt;We empirically evaluated Speculative Cycles against a baseline random scheduler on
  culprit-detection recall and against an optimal algorithm for evaluating culprit detection
  latency improvements. We evaluated different dataset pruning/selection and training
  configurations to discover important hyper-parameters for the Transition Prediction model on a
  standard set of features. We examined three research questions:&lt;/p&gt;
&lt;h2 id="research-questions"&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;What is the performance of Speculative Cycles with Transition Prediction in terms of
      percentage of novel breakages detected (recall) in comparison to a baseline randomized
      testing approach?&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Speculative cycles compete with full testing cycles for machine resources. Do speculative
      cycles improve productivity outcomes for Google developers?&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;What aspects of the model design most impact system performance? Considered aspects
      include features selected, super-extreme class imbalance corrections, and training
      environment.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;h2 id="measuring-performance-recall"&gt;Measuring Performance (Recall)&lt;/h2&gt;
&lt;p&gt;Speculative Cycles as a system attempts to detect bug introducing changes (culprits) through
  running and surfacing at least &lt;span class="math inline"&gt;&lt;em&gt;N&lt;/em&gt;&lt;/span&gt; newly failing targets.
  Specifically, the goal is to capture the majority of (newly failing) target results that detect
  novel bugs introduced into the repository with the faster Speculative Cycles instead of the
  slower Comprehensive Cycles. This naturally lends itself to measurement in the form of recall -
  the percentage of these events we capture.&lt;/p&gt;
&lt;p&gt;We additionally observe that our dataset is imperfect. Not every commit labeled as a "culprit"
  is indeed a bug introducing change. As discussed above, our datasets contains some inaccuracies
  particularly when a particular culprit commit only a few targets "blaming" it.&lt;a href="#fn2"
  class="footnote-ref" id="fnref2" role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; The distribution in the
  number of targets blaming a culprit follows a power law distribution. The majority of culprits
  are blamed by fewer than 10 targets. However, the lower the number of blamed targets the more
  likely it is that the culprit finding was inaccurate and that the commit does not in fact
  introduce a bug. In our labels, the signal is highest for culprits with many targets blaming them
  and lowest when there is only a single target. To protect our system from the noise these "small"
  culprits are filtered out during training.&lt;/p&gt;
&lt;p&gt;We use &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(&lt;em&gt;n&lt;/em&gt;)&lt;/span&gt;
  to denote the subset of culprits that have at least &lt;span class="math inline"&gt;&lt;em&gt;n&lt;/em&gt;&lt;/span&gt;
  targets blaming each culprit. In our evaluation, we provide recall over both (a) &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(1)&lt;/span&gt; -
  the set of all culprits and (b) &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(10)&lt;/span&gt; -
  the set of culprits causing at least 10 target breakages where we successfully detect 10
  breakages. &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(10)&lt;/span&gt;
  also ensures that we meet the (traditional) minimum required threshold (i.e. 10 failing targets)
  to trigger an automatic rollback. Speculative Cycles sees its highest value in terms of mean
  time-to-fix when automatic rollbacks are correctly triggered (especially for commits that have
  wider impact radius). Therefore, by assessing Recall using &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(10)&lt;/span&gt;
  we also assess whether or not the Speculative Cycle can find enough evidence to trigger
  rollbacks.&lt;/p&gt;
&lt;h2 id="measuring-productivity-outcomes-latency-improvement"&gt;Measuring Productivity Outcomes
  (Latency Improvement)&lt;/h2&gt;
&lt;p&gt;Although recall is a concrete metric, our true goal is to improve productivity by reducing
  &lt;em&gt;friction&lt;/em&gt; caused by breakages that slipped into Postsubmit. Speculative Cycles focuses on
  early detection of breakages (and their culprit detection and fix) to reduce the friction caused
  by bad commits in Postsubmit. Its impact is measured by the reduction in "breakage detection
  latency". Early detection and fix also improves the mean time to fix latency for bad commits. We
  will evaluate the "breakage detection latency" decrease as a percentage relative to the current
  latency using empirically observed timings for running both Speculative and Comprehensive over
  the days present in our test dataset.&lt;/p&gt;
&lt;h2 id="high-priority-configuration-of-training-scheme"&gt;High Priority Configuration of Training
  Scheme&lt;/h2&gt;
&lt;p&gt;In general, it is considered good practice in machine learning to prioritize dataset quality
  over attempting to fine tune model level hyper-parameters. We have similarly observed little
  value in specific training parameters and use YDF's defaults for Gradient Boosted Trees for this
  work to show the general case performance expected. The more interesting questions that pertain
  to our dataset configuration are:&lt;/p&gt;
&lt;h3 id="training-split-window-size"&gt;Training Split / Window Size&lt;/h3&gt;
&lt;p&gt;As we use time based splitting to avoid the issue of "time-traveling" across the datasets, our
  training split configuration determines both the training "window size" and the "delay" between
  the training and test datasets.&lt;/p&gt;
&lt;p&gt;We hypothesized that the training data may have some recency bias. A shorter/ more recent
  look-back horizon on the training data may contain more useful signal for predicting the
  likelihood of a target transitioning. A longer window could dilute the usefulness of more recent
  breakages, while introducing noise from old breakages that have since been fixed and are less
  likely to reoccur. In order to test this hypothesis, we ran the model five times using the same
  feature set and configuration, only changing the number of weeks of data used during training.
  Note that our production pipeline uses a 3:1:1 week split for training:test:validation.&lt;/p&gt;
&lt;h3 id="downsampling-and-upweighting"&gt;Downsampling and Upweighting&lt;/h3&gt;
&lt;p&gt;Our extreme class imbalance of &lt;span class="math inline"&gt;40, 000 : 1&lt;/span&gt; necessitates
  downsampling our negative class to a ratio that is compute-efficient, but also avoids trivially
  predicting 0. Upweighting the positive class can be useful to appropriately value positive
  examples with respect to the loss function of the training algorithm. The ratios of down-sampling
  listed above reduced our class imbalance ratio to &lt;span class="math inline"&gt;400 : 1&lt;/span&gt;,
  &lt;span class="math inline"&gt;200 : 1&lt;/span&gt;, and &lt;span class="math inline"&gt;40 : 1&lt;/span&gt;
  respectively. For each case, we then upweighted the positive samples of target breaking commits
  by the new class imbalance ratio to see what effect, if any, it had on improving model
  performance.&lt;/p&gt;
&lt;h2 id="dataset"&gt;Dataset&lt;/h2&gt;
&lt;p&gt;Our evaluation dataset consists of roughly 3 months of Transition Prediction data split across
  Train, Validation, and Test sets. In total, the dataset consisted of 120 Billion target-cycle
  pairs, 7.7 million of which were breaking targets. This corresponds to &lt;span class=
  "math inline"&gt;∼&lt;/span&gt;20 thousand unique culprits. As mentioned above, we ensure that our splits
  are entirely sequential and disjoint to avoid cross contamination. The exact split and training
  configuration is varied while answering RQ3. We then fix our most performant model for evaluating
  RQ1 against the BASELINE random prediction and for RQ2 against latency in the CI environment with
  only Comprehensive Cycles.&lt;/p&gt;
&lt;h1 id="sec:results"&gt;Results&lt;/h1&gt;
&lt;h2 id="summary-of-results"&gt;Summary of Results&lt;/h2&gt;
&lt;li&gt;
  &lt;div class="tcolorbox"&gt;
    &lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: For RQ1 and RQ2, the model was trained using the 'AUG'
    feature set (Table &lt;a href="#table:featureDescription" data-reference-type="ref"
    data-reference="table:featureDescription"&gt;[table:featureDescription]&lt;/a&gt;), a 3-week training
    window, a 0.01 downsampling rate for the majority class, and no upweighting for the minority
    class. This configuration matches our production model, except for the feature set. RQ3
    explores performance across configurations.&lt;/p&gt;
  &lt;/div&gt;
  &lt;/li&gt;
&lt;h2 class="unnumbered" id=
  "rq1-what-is-the-performance-of-speculative-cycles-with-transition-prediction-in-terms-of-percentage-of-novel-breakages-detected-recall-in-comparison-to-a-baseline-randomized-testing-approach"&gt;
  &lt;strong&gt;RQ1&lt;/strong&gt;: What is the performance of Speculative Cycles with Transition Prediction in
  terms of percentage of novel breakages detected (recall) in comparison to a baseline randomized
  testing approach?&lt;/h2&gt;

&lt;p&gt;&lt;span id="fig:cl-level-recall" label="fig:cl-level-recall"&gt;
    &lt;a href="images/icst-2025/fig3.png"&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;&lt;/em&gt;
    &lt;img alt="Figure 3" src="images/icst-2025/fig3.png"&gt;&lt;/a&gt;
  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TRANSPRED demonstrates superior recall compared to the BASELINE model across all
  budgets (see Figure &lt;a href="#fig:cl-level-recall" data-reference-type="ref" data-reference=
  "fig:cl-level-recall"&gt;[fig:cl-level-recall]&lt;/a&gt;). At a budget of 25% of the total targets,
  TRANSPRED achieves a recall of 85%, greater than the BASELINE model's recall of 56%. This
  highlights the model's effectiveness in identifying breaking changes with limited
  resources.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the context of Transition Prediction, recall measures the percentage of breaking changes
  identified using a given budget. The budget parameterizes the recall metric as shown in Figure
  &lt;a href="#fig:cl-level-recall" data-reference-type="ref" data-reference=
  "fig:cl-level-recall"&gt;[fig:cl-level-recall]&lt;/a&gt;. To simplify the complex accounting of machine
  and test costs, we use the percentage of targets scheduled by TRANSPRED (versus a Comprehensive
  Cycle). Our budget for this assessment is 25% of the targets that would have been used by a
  Comprehensive Cycle. Recall is then defined as percentage of culprits identified out of the total
  set of culprits.&lt;/p&gt;
&lt;p&gt;Figure &lt;a href="#fig:cl-level-recall" data-reference-type="ref" data-reference=
  "fig:cl-level-recall"&gt;[fig:cl-level-recall]&lt;/a&gt; provides a visual representation of how recall
  changes as the percentage of scheduled targets varies, comparing the performance of TRANSPRED
  against the BASELINE model. For each model the figure shows the recall of both &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(1)&lt;/span&gt;
  and &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(10)&lt;/span&gt;.
  &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(1)&lt;/span&gt;
  corresponds to finding any target for a breakage or informally, that breakages are at least
  "identified". The recall for &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(10)&lt;/span&gt;
  concentrates on finding breakages with at least 10 targets broken - including scheduling at least
  10 of those broken targets to ensure auto rollback has enough evidence to trigger.&lt;/p&gt;
&lt;p&gt;For TRANSPRED, the &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(10)&lt;/span&gt;
  recall is higher than the &lt;span class=
  "math inline"&gt;&lt;em&gt;A&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;L&lt;/em&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;(1)&lt;/span&gt;
  recall. This indicates the model is better at catching bad commits that break 10 or more targets.
  It's intuitive that larger breakages would be better handled by the model as they are much better
  represented in the dataset, but an interesting result that we have is better coverage of them,
  i.e., consistently finding more individual targets broken by such commits. Culprits that only
  break a few tests are arguably less consequential than the few culprits that break a large number
  of tests. This strong performance on commits that break at least 10 targets indicates that when
  TRANSPRED does find a break, it has a good chance to be rolled back automatically.&lt;/p&gt;
&lt;p&gt;Both configurations for TRANSPRED outperform the BASELINE random model, indicating that the
  model is learning something useful from metadata and that the metadata alone (without considering
  the content of the changes) can be predictive of breakages. We look forward to future experiments
  utilizing the content of commits that compare both the efficacy and the model costs.&lt;/p&gt;
&lt;h2 class="unnumbered" id=
  "rq2-speculative-cycles-compete-with-full-testing-cycles-for-machine-resources-what-configurations-of-speculative-cycles-improve-productivity-outcomes-for-google-developers"&gt;
  &lt;strong&gt;RQ2&lt;/strong&gt;: Speculative Cycles compete with full testing cycles for machine resources,
  what configurations of Speculative Cycles improve productivity outcomes for Google
  developers?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Speculative Cycles reduce the median (p50) breakage detection latency by 65% (70
  minutes) over the existing Comprehensive Cycles.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id="fig:detection-latency" label="fig:detection-latency"&gt;
    &lt;a href="images/icst-2025/fig4.png"&gt;&lt;em&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;&lt;/em&gt;
    &lt;img alt="Figure 4" src="images/icst-2025/fig4.png"&gt;&lt;/a&gt;
  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;a href="#fig:detection-latency" data-reference-type="ref" data-reference=
  "fig:detection-latency"&gt;[fig:detection-latency]&lt;/a&gt; visualizes a histogram of the performance of
  the model in terms of &lt;em&gt;breakage detection latency&lt;/em&gt;. We consider this metric to be the
  "outcome metric" for this work as it is a proxy for improving overall developer productivity. It
  is known from internal work that developer productivity degrades when they need to debug build
  and test breakages they did not cause during interactive development. Productivity also degrades
  when they are interrupted to troubleshoot releases stalled due to breakages. By reducing the
  total time from culprit commit submission to detection, the model reduces the amount of friction
  developers experience from breakages.&lt;/p&gt;
&lt;p&gt;As with RQ1, we have fixed TRANSPRED to represent Speculative Cycles using the same Transition
  Prediction model and scheduling 25% of targets. In the visualization, the model (in black) is
  compared against two alternatives. The first labeled "Only Comprehensive" shows the distribution
  of time (in minutes) it takes the current TAP Postsubmit scheduling algorithm (Comprehensive
  Cycles) to detect breakages. The second alternative, "Optimal" imagines a perfect model that
  always correctly predicts which target need to be scheduled. Our model, TRANSPRED, falls between
  these two extremes of no-improvement to full-improvement. Observe that in comparison to "Only
  Comprehensive" the variance of breakage detection latency for TRANSPRED is substantially reduced.
  While, the new model isn't perfect (as shown in the figure), it is a welcome result that the
  Speculative Cycles reliably finds culprit commits in 37 minutes in the median case (p50), a 65%
  latency reduction over traditional Comprehensive Cycles that (p50) take 107 minutes.&lt;/p&gt;
&lt;h2 class="unnumbered" id=
  "rq3what-aspects-of-the-model-design-most-impact-performance-considered-aspects-include-features-selected-super-extreme-class-imbalance-corrections-and-training-environment."&gt;
  &lt;strong&gt;RQ3&lt;/strong&gt;:What aspects of the model design most impact performance? Considered aspects
  include features selected, super-extreme class imbalance corrections, and training
  environment.&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The feature set used and the training window length had the greatest impact on model
  performance as measured by ROC AUC. Downsampling is critical for training speed but further
  downsampling hurts performance while upweighting has negligible impact.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To better understand how model design impacts performance, we ran multiple experiments across
  the different axes of decision making, including 1) feature selection, 2) training environment.
  and 3) different ways to correct for the super-extreme class imbalance&lt;/p&gt;
&lt;h3 id="feature-selection"&gt;Feature Selection&lt;/h3&gt;

&lt;p&gt;&lt;span id="table:samplingAndWeightTable" label="table:training-length"&gt;
  &lt;span id="table:training-length" label="table:training-length"&gt;
    &lt;a href="images/icst-2025/table2-3.png"&gt;&lt;em&gt;&lt;strong&gt;Tables 2 and 3&lt;/strong&gt;&lt;/em&gt;
    &lt;img alt="Tables 2 and 3" src="images/icst-2025/table2-3.png"&gt;&lt;/a&gt;
  &lt;/span&gt;
  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Table &lt;a href="#table:training-length" data-reference-type="ref" data-reference=
  "table:training-length"&gt;[table:training-length]&lt;/a&gt; shows an improvement in performance when the
  model is trained on the 'AUGMENTED' feature set vs 'BASE' set, regardless of number of weeks
  considered for the training window. Critically, we see a lot of signal coming from the added
  target history features over longer time intervals.&lt;/p&gt;
&lt;h3 id="training-environment"&gt;Training Environment&lt;/h3&gt;
&lt;p&gt;Table &lt;a href="#table:training-length" data-reference-type="ref" data-reference=
  "table:training-length"&gt;[table:training-length]&lt;/a&gt; indicates our hypothesis was correct -
  performance starts to degrade as we increase or decrease the training window length from the
  optimal 3 weeks.&lt;/p&gt;
&lt;h3 id="correcting-for-super-extreme-class-imbalance"&gt;Correcting for Super-Extreme Class
  Imbalance&lt;/h3&gt;
&lt;p&gt;Production systems exhibit extremely low Postsubmit breakage rates (40,000:1 in our case).
  This class imbalance can hinder model training by biasing it towards the majority class. However,
  due to the asymmetric cost of false positives (unnecessary execution) versus false negatives
  (late breakage detection), prioritizing catching newly breaking targets at the cost of
  incorrectly scheduling some healthy targets is acceptable. This lets us experiment with some
  imbalance-correction techniques without being strictly bound to the true breakage rate, as shown
  in Table &lt;a href="#table:samplingAndWeightTable" data-reference-type="ref" data-reference=
  "table:samplingAndWeightTable"&gt;[table:samplingAndWeightTable]&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="related-work"&gt;Related Work&lt;/h1&gt;
&lt;p&gt;This paper explored the problem of reducing the time to discovery of novel test failures in
  continuous integration. This is a variation of the well studied problem of Test Case
  Prioritization and Selection &lt;span class="citation" data-cites=
  "Bates1993 Rothermel1997 Elbaum2000 Leon2003 Engstrom2010 Zhou2010 Singh2012 Gligoric2014 Mondal2015 Musa2015 DeS.CamposJunior2017 Najafi2019 Machalica2019 DeCastro-Cabrera2020 pan2022 Jin2023"&gt;
  (&lt;a href="#ref-Bates1993" role="doc-biblioref"&gt;Bates and Horwitz 1993&lt;/a&gt;; &lt;a href=
  "#ref-Rothermel1997" role="doc-biblioref"&gt;Rothermel and Harrold 1997&lt;/a&gt;; &lt;a href=
  "#ref-Elbaum2000" role="doc-biblioref"&gt;Elbaum, Malishevsky, and Rothermel 2000&lt;/a&gt;; &lt;a href=
  "#ref-Leon2003" role="doc-biblioref"&gt;Leon and Podgurski 2003&lt;/a&gt;; &lt;a href="#ref-Engstrom2010"
  role="doc-biblioref"&gt;Engström, Runeson, and Skoglund 2010&lt;/a&gt;; &lt;a href="#ref-Zhou2010" role=
  "doc-biblioref"&gt;Zhou 2010&lt;/a&gt;; &lt;a href="#ref-Singh2012" role="doc-biblioref"&gt;Singh et al.
  2012&lt;/a&gt;; &lt;a href="#ref-Gligoric2014" role="doc-biblioref"&gt;Gligoric et al. 2014&lt;/a&gt;; &lt;a href=
  "#ref-Mondal2015" role="doc-biblioref"&gt;Mondal, Hemmati, and Durocher 2015&lt;/a&gt;; &lt;a href=
  "#ref-Musa2015" role="doc-biblioref"&gt;Musa et al. 2015&lt;/a&gt;; &lt;a href="#ref-DeS.CamposJunior2017"
  role="doc-biblioref"&gt;de S. Campos Junior et al. 2017&lt;/a&gt;; &lt;a href="#ref-Najafi2019" role=
  "doc-biblioref"&gt;Najafi, Shang, and Rigby 2019&lt;/a&gt;; &lt;a href="#ref-Machalica2019" role=
  "doc-biblioref"&gt;Machalica et al. 2019&lt;/a&gt;; &lt;a href="#ref-DeCastro-Cabrera2020" role=
  "doc-biblioref"&gt;De Castro-Cabrera, García-Dominguez, and Medina-Bulo 2020&lt;/a&gt;; &lt;a href=
  "#ref-pan2022" role="doc-biblioref"&gt;Pan et al. 2022&lt;/a&gt;; &lt;a href="#ref-Jin2023" role=
  "doc-biblioref"&gt;Jin and Servant 2023&lt;/a&gt;)&lt;/span&gt; adapted to the CI environment &lt;span class=
  "citation" data-cites=
  "Chen2020 Saidani2020 Abdalkareem2021 Abdalkareem2021a Al-Sabbagh2022 Saidani2022 Saidani2022a Jin2023 Jin2024 Liu2023 Hong2024 Sun2024 Wang2024 Zeng2024"&gt;
  (&lt;a href="#ref-Chen2020" role="doc-biblioref"&gt;Chen et al. 2020&lt;/a&gt;; &lt;a href="#ref-Saidani2020"
  role="doc-biblioref"&gt;Saidani et al. 2020&lt;/a&gt;; &lt;a href="#ref-Abdalkareem2021" role=
  "doc-biblioref"&gt;Abdalkareem, Mujahid, and Shihab 2021&lt;/a&gt;; &lt;a href="#ref-Abdalkareem2021a" role=
  "doc-biblioref"&gt;Abdalkareem et al. 2021&lt;/a&gt;; &lt;a href="#ref-Al-Sabbagh2022" role=
  "doc-biblioref"&gt;Al-Sabbagh, Staron, and Hebig 2022&lt;/a&gt;; &lt;a href="#ref-Saidani2022" role=
  "doc-biblioref"&gt;Saidani, Ouni, and Mkaouer 2022a&lt;/a&gt;, &lt;a href="#ref-Saidani2022a" role=
  "doc-biblioref"&gt;2022b&lt;/a&gt;; &lt;a href="#ref-Jin2023" role="doc-biblioref"&gt;Jin and Servant 2023&lt;/a&gt;;
  &lt;a href="#ref-Jin2024" role="doc-biblioref"&gt;Jin et al. 2024&lt;/a&gt;; &lt;a href="#ref-Liu2023" role=
  "doc-biblioref"&gt;Liu et al. 2023&lt;/a&gt;; &lt;a href="#ref-Hong2024" role="doc-biblioref"&gt;Hong et al.
  2024&lt;/a&gt;; &lt;a href="#ref-Sun2024" role="doc-biblioref"&gt;Sun, Habchi, and McIntosh 2024&lt;/a&gt;;
  &lt;a href="#ref-Wang2024" role="doc-biblioref"&gt;G. Wang et al. 2024&lt;/a&gt;; &lt;a href="#ref-Zeng2024"
  role="doc-biblioref"&gt;Zeng et al. 2024&lt;/a&gt;)&lt;/span&gt;. Our approach is predicated on having highly
  accurate historical information on what causes breakages in the Google environment. We achieve
  this through culprit finding and verification &lt;span class="citation" data-cites=
  "Henderson2023 Henderson2024"&gt;(&lt;a href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al.
  2023&lt;/a&gt;, &lt;a href="#ref-Henderson2024" role="doc-biblioref"&gt;2024&lt;/a&gt;)&lt;/span&gt;. Culprit finding
  &lt;span class="citation" data-cites=
  "Couder2008 Ziftci2013a Ziftci2017 Saha2017 Najafi2019a Beheshtian2022 keenan2019 An2021 Ocariza2022"&gt;
  (&lt;a href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;; &lt;a href="#ref-Ziftci2013a" role=
  "doc-biblioref"&gt;Ziftci and Ramavajjala 2013&lt;/a&gt;; &lt;a href="#ref-Ziftci2017" role=
  "doc-biblioref"&gt;Ziftci and Reardon 2017&lt;/a&gt;; &lt;a href="#ref-Saha2017" role="doc-biblioref"&gt;Saha
  and Gligoric 2017&lt;/a&gt;; &lt;a href="#ref-Najafi2019a" role="doc-biblioref"&gt;Najafi, Rigby, and Shang
  2019&lt;/a&gt;; &lt;a href="#ref-Beheshtian2022" role="doc-biblioref"&gt;Beheshtian, Bavand, and Rigby
  2022&lt;/a&gt;; &lt;a href="#ref-keenan2019" role="doc-biblioref"&gt;Keenan 2019&lt;/a&gt;; &lt;a href="#ref-An2021"
  role="doc-biblioref"&gt;An and Yoo 2021&lt;/a&gt;; &lt;a href="#ref-Ocariza2022" role="doc-biblioref"&gt;Ocariza
  2022&lt;/a&gt;)&lt;/span&gt; aims to identify the true "bug introducing commits" (BICs) &lt;span class=
  "citation" data-cites="Sliwerski2005 Rodriguez-Perez2018 Borg2019 Wen2019 An2023"&gt;(&lt;a href=
  "#ref-Sliwerski2005" role="doc-biblioref"&gt;Śliwerski, Zimmermann, and Zeller 2005&lt;/a&gt;; &lt;a href=
  "#ref-Rodriguez-Perez2018" role="doc-biblioref"&gt;Rodríguez-Pérez, Robles, and González-Barahona
  2018&lt;/a&gt;; &lt;a href="#ref-Borg2019" role="doc-biblioref"&gt;Borg et al. 2019&lt;/a&gt;; &lt;a href=
  "#ref-Wen2019" role="doc-biblioref"&gt;Wen et al. 2019&lt;/a&gt;; &lt;a href="#ref-An2023" role=
  "doc-biblioref"&gt;An et al. 2023&lt;/a&gt;)&lt;/span&gt; by rerunning tests over the search space of possible
  versions.&lt;/p&gt;
&lt;p&gt;The extensive amount of work on test selection and prioritization makes it difficult to fully
  summarize outside of a survey &lt;span class="citation" data-cites=
  "Engstrom2010 Singh2012 DeS.CamposJunior2017 DeCastro-Cabrera2020 pan2022"&gt;(&lt;a href=
  "#ref-Engstrom2010" role="doc-biblioref"&gt;Engström, Runeson, and Skoglund 2010&lt;/a&gt;; &lt;a href=
  "#ref-Singh2012" role="doc-biblioref"&gt;Singh et al. 2012&lt;/a&gt;; &lt;a href="#ref-DeS.CamposJunior2017"
  role="doc-biblioref"&gt;de S. Campos Junior et al. 2017&lt;/a&gt;; &lt;a href="#ref-DeCastro-Cabrera2020"
  role="doc-biblioref"&gt;De Castro-Cabrera, García-Dominguez, and Medina-Bulo 2020&lt;/a&gt;; &lt;a href=
  "#ref-pan2022" role="doc-biblioref"&gt;Pan et al. 2022&lt;/a&gt;)&lt;/span&gt;. Here are some highlights. Early
  work in test selection focused on selecting tests while maintaining test suite "adequacy"
  according to some (usually coverage based) criterion. For instance Bates and Horwitz &lt;span class=
  "citation" data-cites="Bates1993"&gt;(&lt;a href="#ref-Bates1993" role="doc-biblioref"&gt;Bates and
  Horwitz 1993&lt;/a&gt;)&lt;/span&gt; define a coverage criterion based on coverage of elements in the Program
  Dependence Graph (PDG) &lt;span class="citation" data-cites="Horwitz1988 Podgurski1989"&gt;(&lt;a href=
  "#ref-Horwitz1988" role="doc-biblioref"&gt;Horwitz, Prins, and Reps 1988&lt;/a&gt;; &lt;a href=
  "#ref-Podgurski1989" role="doc-biblioref"&gt;Podgurski and Clarke 1989&lt;/a&gt;)&lt;/span&gt; and Rothermel and
  Harrold developed a regression selection technique based on the PDG &lt;span class="citation"
  data-cites="Rothermel1997"&gt;(&lt;a href="#ref-Rothermel1997" role="doc-biblioref"&gt;Rothermel and
  Harrold 1997&lt;/a&gt;)&lt;/span&gt;. By 2000 papers such as Elbaum &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"
  data-cites="Elbaum2000"&gt;(&lt;a href="#ref-Elbaum2000" role="doc-biblioref"&gt;Elbaum, Malishevsky, and
  Rothermel 2000&lt;/a&gt;)&lt;/span&gt;'s were looking at prioritizing test cases based statement coverage to
  speed up test suite execution and evaluating performance based on the now standard &lt;em&gt;average
  percentage of faults detected&lt;/em&gt; (APFD) metric. By 2003, the community was starting to look for
  other sources of data to prioritize and filter test cases. For instance, Leon and Podgurski
  &lt;span class="citation" data-cites="Leon2003"&gt;(&lt;a href="#ref-Leon2003" role="doc-biblioref"&gt;Leon
  and Podgurski 2003&lt;/a&gt;)&lt;/span&gt; compared the bug finding efficacy (APFD) of coverage techniques
  versus techniques that prioritized diversity of basic block execution profiles (that included
  execution counts). In 2010 Engstrom &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation" data-cites=
  "Engstrom2010"&gt;(&lt;a href="#ref-Engstrom2010" role="doc-biblioref"&gt;Engström, Runeson, and Skoglund
  2010&lt;/a&gt;)&lt;/span&gt; surveyed the state of the test selection and documented 28 techniques.&lt;/p&gt;
&lt;p&gt;In 2017 the TAP team at Google began publishing on the problem &lt;span class="citation"
  data-cites="Memon2017 Leong2019"&gt;(&lt;a href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al.
  2017&lt;/a&gt;; &lt;a href="#ref-Leong2019" role="doc-biblioref"&gt;Leong et al. 2019&lt;/a&gt;)&lt;/span&gt; and
  separately a partner team had put into production an ML based test filtering system for TAP
  Presubmit. In contrast to this prior work by TAP, our current work is better grounded because of
  the verified culprits dataset we created in the last several years. For Memon &lt;em&gt;et al.&lt;/em&gt;
  &lt;span class="citation" data-cites="Memon2017"&gt;(&lt;a href="#ref-Memon2017" role=
  "doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;)&lt;/span&gt; TAP did not yet have a robust culprit finding
  system to precisely identify the bug introducing changes. Instead, that paper attempted to infer
  what commits might be the culprit. In Leong &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation" data-cites=
  "Leong2019"&gt;(&lt;a href="#ref-Leong2019" role="doc-biblioref"&gt;Leong et al. 2019&lt;/a&gt;)&lt;/span&gt; TAP did
  have a culprit finder but it was not as robust against flakiness and non-determinism as our
  current system. Our current culprit finder uses the Flake Aware algorithm (FACF) that adaptively
  deflakes the tests while performing culprit finding. Additionally, we further &lt;em&gt;verify&lt;/em&gt; all
  culprit commits by doing additional executions in patterns designed to catch flaky behavior
  &lt;span class="citation" data-cites="Henderson2023 Henderson2024"&gt;(&lt;a href="#ref-Henderson2023"
  role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;, &lt;a href="#ref-Henderson2024" role=
  "doc-biblioref"&gt;2024&lt;/a&gt;)&lt;/span&gt;. The FACF paper &lt;span class="citation" data-cites=
  "Henderson2023"&gt;(&lt;a href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al.
  2023&lt;/a&gt;)&lt;/span&gt; performed experiments that show the FACF algorithm is statistically
  significantly more accurate than the "deflaked variant" of the standard "bisect" algorithm used
  in the Leong paper. Specifically the experiments indicate FACF is &lt;span class=
  "math inline"&gt;&amp;gt;&lt;/span&gt;10% more accurate at identifying flaky breakages than Bisect with 8
  additional deflaking runs. By properly deflaking our training and evaluation labels, our model is
  much less likely to prioritize flaky tests and our evaluation more accurately reflects our
  desired productivity outcomes.&lt;/p&gt;
&lt;p&gt;Meanwhile in 2019, Machalica &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation" data-cites=
  "Machalica2019"&gt;(&lt;a href="#ref-Machalica2019" role="doc-biblioref"&gt;Machalica et al.
  2019&lt;/a&gt;)&lt;/span&gt; reported on a similar effort at Facebook in 2019 to develop a test selection
  system for their version of Presubmit testing. Conceptually, the Presubmit selection system at
  Facebook and the Presubmit selection system at Google have similarities. Both use shallow machine
  learning models with sets of features not dissimilar to the features presented in this work. Both
  systems (like this work) take a systematic approach to reducing the effect of flaky failures on
  developer productivity. The primary contribution of this paper versus this past work is the
  application and evaluation of selection techniques to the Postsubmit environment. In Postsubmit,
  the features must be necessarily adjusted as the prediction of whether or not a test will change
  behavior is not against a single commit but against a sequence of commits. Evaluation also
  changes: the concern is not just about recall but also latency of breakage detection and
  time-to-fix.&lt;/p&gt;
&lt;p&gt;Recently the wider field (outside of Google and Facebook) has produced compelling work
  &lt;span class="citation" data-cites=
  "Jin2023 Jin2024 Liu2023 Hong2024 Sun2024 Wang2024 Zeng2024"&gt;(&lt;a href="#ref-Jin2023" role=
  "doc-biblioref"&gt;Jin and Servant 2023&lt;/a&gt;; &lt;a href="#ref-Jin2024" role="doc-biblioref"&gt;Jin et al.
  2024&lt;/a&gt;; &lt;a href="#ref-Liu2023" role="doc-biblioref"&gt;Liu et al. 2023&lt;/a&gt;; &lt;a href=
  "#ref-Hong2024" role="doc-biblioref"&gt;Hong et al. 2024&lt;/a&gt;; &lt;a href="#ref-Sun2024" role=
  "doc-biblioref"&gt;Sun, Habchi, and McIntosh 2024&lt;/a&gt;; &lt;a href="#ref-Wang2024" role=
  "doc-biblioref"&gt;G. Wang et al. 2024&lt;/a&gt;; &lt;a href="#ref-Zeng2024" role="doc-biblioref"&gt;Zeng et al.
  2024&lt;/a&gt;)&lt;/span&gt;. We will note a few items here. Jin and Servant &lt;span class="citation"
  data-cites="Jin2023"&gt;(&lt;a href="#ref-Jin2023" role="doc-biblioref"&gt;Jin and Servant
  2023&lt;/a&gt;)&lt;/span&gt; presented HybridCISave which uses a shallow machine learning based approach to
  do both fine-grained (individual tests) and coarse grained (CI workflow builds) selection. By
  combining both selection types both cost savings and safety are improved. The authors evaluate
  their results using TravisTorrent &lt;span class="citation" data-cites="Beller2022"&gt;(&lt;a href=
  "#ref-Beller2022" role="doc-biblioref"&gt;Beller, Gousios, and Zaidman 2022&lt;/a&gt;)&lt;/span&gt;. This work
  is a good representative of the general approach of using shallow machine learning for selection
  outside of the mega-corp environment. Wang &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation" data-cites=
  "Wang2024"&gt;(&lt;a href="#ref-Wang2024" role="doc-biblioref"&gt;G. Wang et al. 2024&lt;/a&gt;)&lt;/span&gt; address
  the problem of feature selection by using a transformer based language model, GitSense, that
  automates statistic feature extraction. The authors evaluate their results using the dataset from
  Chen &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation" data-cites="Chen2020"&gt;(&lt;a href="#ref-Chen2020" role=
  "doc-biblioref"&gt;Chen et al. 2020&lt;/a&gt;)&lt;/span&gt;. Zeng &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"
  data-cites="Zeng2024"&gt;(&lt;a href="#ref-Zeng2024" role="doc-biblioref"&gt;Zeng et al. 2024&lt;/a&gt;)&lt;/span&gt;
  use mutation testing to better assess the safety of a commercial CI system (YourBase) that skips
  tests based on inferred dependencies. This use of mutation testing highlights one weakness of
  traditional safety evaluation, they only use historical faults and may miss novel bugs. By
  injecting mutation faults a more holistic view of the performance of the model can be obtained.
  Determining how to apply mutation testing to shallow machine learning approaches that use
  metadata features (such as ours) is an open problem.&lt;/p&gt;
&lt;h1 id="sec:conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this paper, we presented a new scheduling system for Google's main test automation platform
  (TAP). The new scheduling system, Speculative Cycles, prioritizes finding novel test breakages
  faster. Speculative Cycles are powered by a new machine learning model, Transition Prediction,
  that predicts when tests are likely to fail. While the results are not perfect, they indicate
  that the new system reduces the breakage detection latency by 65% (70 minutes) on average. This
  reduction in breakage detection time leads to a reduction in developer friction and increased
  productivity. In terms of model design, the most impactful choice made was total amount of prior
  history used to train the model. As the training set size went from 1 week to 3 weeks model
  performance improved. But as the training set size further increased the model performance
  actually began to degrade. Only high level metadata features were used in this paper. In the
  future we look forward to experimenting with more complex models.&lt;/p&gt;

&lt;ul&gt;
  &lt;div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list"&gt;
    &lt;li&gt;
    &lt;div id="ref-Abdalkareem2021" class="csl-entry" role="listitem"&gt;
      Abdalkareem, Rabe, Suhaib Mujahid, and Emad Shihab. 2021. &lt;span&gt;"A &lt;span&gt;Machine Learning
      Approach&lt;/span&gt; to &lt;span&gt;Improve&lt;/span&gt; the &lt;span&gt;Detection&lt;/span&gt; of &lt;span&gt;CI Skip
      Commits&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt; 47 (12): 2740-54.
      &lt;a href=
      "https://doi.org/10.1109/TSE.2020.2967380"&gt;https://doi.org/10.1109/TSE.2020.2967380&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Abdalkareem2021a" class="csl-entry" role="listitem"&gt;
      Abdalkareem, Rabe, Suhaib Mujahid, Emad Shihab, and Juergen Rilling. 2021. &lt;span&gt;"Which
      &lt;span&gt;Commits Can Be CI Skipped&lt;/span&gt;?"&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software
      Engineering&lt;/em&gt; 47 (3): 448-63. &lt;a href=
      "https://doi.org/10.1109/TSE.2019.2897300"&gt;https://doi.org/10.1109/TSE.2019.2897300&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Al-Sabbagh2022" class="csl-entry" role="listitem"&gt;
      Al-Sabbagh, Khaled, Miroslaw Staron, and Regina Hebig. 2022. &lt;span&gt;"Predicting Build Outcomes
      in Continuous Integration Using Textual Analysis of Source Code Commits."&lt;/span&gt; In
      &lt;em&gt;Proceedings of the 18th &lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Predictive
      Models&lt;/span&gt; and &lt;span&gt;Data Analytics&lt;/span&gt; in &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;,
      42-51. Singapore Singapore: ACM. &lt;a href=
      "https://doi.org/10.1145/3558489.3559070"&gt;https://doi.org/10.1145/3558489.3559070&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-An2023" class="csl-entry" role="listitem"&gt;
      An, Gabin, Jingun Hong, Naryeong Kim, and Shin Yoo. 2023. &lt;span&gt;"Fonte: &lt;span&gt;Finding Bug
      Inducing Commits&lt;/span&gt; from &lt;span&gt;Failures&lt;/span&gt;."&lt;/span&gt; arXiv. &lt;a href=
      "http://arxiv.org/abs/2212.06376"&gt;http://arxiv.org/abs/2212.06376&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-An2021" class="csl-entry" role="listitem"&gt;
      An, Gabin, and Shin Yoo. 2021. &lt;span&gt;"Reducing the Search Space of Bug Inducing Commits Using
      Failure Coverage."&lt;/span&gt; In &lt;em&gt;Proceedings of the 29th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on
      &lt;span&gt;European Software Engineering Conference&lt;/span&gt; and &lt;span&gt;Symposium&lt;/span&gt; on the
      &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 1459-62. Athens Greece:
      ACM. &lt;a href=
      "https://doi.org/10.1145/3468264.3473129"&gt;https://doi.org/10.1145/3468264.3473129&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Bates1993" class="csl-entry" role="listitem"&gt;
      Bates, Samuel, and Susan Horwitz. 1993. &lt;span&gt;"Incremental Program Testing Using Program
      Dependence Graphs."&lt;/span&gt; In &lt;em&gt;Proceedings of the 20th &lt;span&gt;ACM SIGPLAN-SIGACT&lt;/span&gt;
      Symposium on &lt;span&gt;Principles&lt;/span&gt; of Programming Languages - &lt;span&gt;POPL&lt;/span&gt; '93&lt;/em&gt;,
      384-96. Charleston, South Carolina, United States: ACM Press. &lt;a href=
      "https://doi.org/10.1145/158511.158694"&gt;https://doi.org/10.1145/158511.158694&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Beheshtian2022" class="csl-entry" role="listitem"&gt;
      Beheshtian, Mohammad Javad, Amir Hossein Bavand, and Peter C. Rigby. 2022. &lt;span&gt;"Software
      &lt;span&gt;Batch Testing&lt;/span&gt; to &lt;span&gt;Save Build Test Resources&lt;/span&gt; and to &lt;span&gt;Reduce
      Feedback Time&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt; 48 (8):
      2784-2801. &lt;a href=
      "https://doi.org/10.1109/TSE.2021.3070269"&gt;https://doi.org/10.1109/TSE.2021.3070269&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Beller2022" class="csl-entry" role="listitem"&gt;
      Beller, Moritz, Georgios Gousios, and Andy Zaidman. 2022.
      &lt;span&gt;"&lt;span&gt;TravisTorrent&lt;/span&gt;."&lt;/span&gt; figshare. &lt;a href=
      "https://doi.org/10.6084/M9.FIGSHARE.19314170.V1"&gt;https://doi.org/10.6084/M9.FIGSHARE.19314170.V1&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Bland2012" class="csl-entry" role="listitem"&gt;
      Bland, Mike. 2012. &lt;span&gt;"The &lt;span&gt;Chris&lt;/span&gt;/&lt;span&gt;Jay Continuous Build&lt;/span&gt;."&lt;/span&gt;
      Personal {{Website}}. &lt;em&gt;Mike Bland's Blog&lt;/em&gt;. &lt;a href=
      "https://mike-bland.com/2012/06/21/chris-jay-continuous-%20build.html"&gt;https://mike-bland.com/2012/06/21/chris-jay-continuous-
      build.html&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Borg2019" class="csl-entry" role="listitem"&gt;
      Borg, Markus, Oscar Svensson, Kristian Berg, and Daniel Hansson. 2019.
      &lt;span&gt;"&lt;span&gt;SZZ&lt;/span&gt; Unleashed: An Open Implementation of the &lt;span&gt;SZZ&lt;/span&gt; Algorithm -
      Featuring Example Usage in a Study of Just-in-Time Bug Prediction for the
      &lt;span&gt;Jenkins&lt;/span&gt; Project."&lt;/span&gt; In &lt;em&gt;Proceedings of the 3rd &lt;span&gt;ACM SIGSOFT
      International Workshop&lt;/span&gt; on &lt;span&gt;Machine Learning Techniques&lt;/span&gt; for &lt;span&gt;Software
      Quality Evaluation&lt;/span&gt; - &lt;span&gt;MaLTeSQuE&lt;/span&gt; 2019&lt;/em&gt;, 7-12. Tallinn, Estonia: ACM
      Press. &lt;a href=
      "https://doi.org/10.1145/3340482.3342742"&gt;https://doi.org/10.1145/3340482.3342742&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Chen2020" class="csl-entry" role="listitem"&gt;
      Chen, Bihuan, Linlin Chen, Chen Zhang, and Xin Peng. 2020. &lt;span&gt;"&lt;span&gt;BUILDFAST&lt;/span&gt;:
      &lt;span class="nocase"&gt;History-aware&lt;/span&gt; Build Outcome Prediction for Fast Feedback and
      Reduced Cost in Continuous Integration."&lt;/span&gt; In &lt;em&gt;2020 35th
      &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; International Conference on Automated Software Engineering
      (&lt;span&gt;ASE&lt;/span&gt;)&lt;/em&gt;, 42-53.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Couder2008" class="csl-entry" role="listitem"&gt;
      Couder, Christian. 2008. &lt;span&gt;"Fighting Regressions with Git Bisect."&lt;/span&gt; &lt;em&gt;The Linux
      Kernel Archives&lt;/em&gt; 4 (5). &lt;a href=
      "https://www.%20kernel.%20org/pub/software/scm/git/doc%20s/git-%20bisect-lk2009.html"&gt;https://www.
      kernel. org/pub/software/scm/git/doc s/git- bisect-lk2009.html&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-DeCastro-Cabrera2020" class="csl-entry" role="listitem"&gt;
      De Castro-Cabrera, M. Del Carmen, Antonio García-Dominguez, and Inmaculada Medina-Bulo. 2020.
      &lt;span&gt;"Trends in Prioritization of Test Cases: 2017-2019."&lt;/span&gt; &lt;em&gt;Proceedings of the ACM
      Symposium on Applied Computing&lt;/em&gt;, 2005-11. &lt;a href=
      "https://doi.org/10.1145/3341105.3374036"&gt;https://doi.org/10.1145/3341105.3374036&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-DeS.CamposJunior2017" class="csl-entry" role="listitem"&gt;
      de S. Campos Junior, Heleno, Marco Antônio P Araújo, José Maria N David, Regina Braga,
      Fernanda Campos, and Victor Ströele. 2017. &lt;span&gt;"Test &lt;span&gt;Case Prioritization&lt;/span&gt;:
      &lt;span&gt;A Systematic Review&lt;/span&gt; and &lt;span&gt;Mapping&lt;/span&gt; of the
      &lt;span&gt;Literature&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Proceedings of the 31st &lt;span&gt;Brazilian
      Symposium&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 34-43. New York, NY, USA: ACM.
      &lt;a href=
      "https://doi.org/10.1145/3131151.3131170"&gt;https://doi.org/10.1145/3131151.3131170&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Elbaum2000" class="csl-entry" role="listitem"&gt;
      Elbaum, Sebastian, Alexey G. Malishevsky, and Gregg Rothermel. 2000. &lt;span&gt;"Prioritizing Test
      Cases for Regression Testing."&lt;/span&gt; In &lt;em&gt;Proceedings of the 2000 &lt;span&gt;ACM SIGSOFT&lt;/span&gt;
      International Symposium on Software Testing and Analysis&lt;/em&gt;, 102-12. Issta '00. New York,
      NY, USA: Association for Computing Machinery. &lt;a href=
      "https://doi.org/10.1145/347324.348910"&gt;https://doi.org/10.1145/347324.348910&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Engstrom2010" class="csl-entry" role="listitem"&gt;
      Engström, Emelie, Per Runeson, and Mats Skoglund. 2010. &lt;span&gt;"A Systematic Review on
      Regression Test Selection Techniques."&lt;/span&gt; &lt;em&gt;Information and Software Technology&lt;/em&gt; 52
      (1): 14-30. &lt;a href=
      "https://doi.org/10.1016/j.infsof.2009.07.001"&gt;https://doi.org/10.1016/j.infsof.2009.07.001&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Fowler2006" class="csl-entry" role="listitem"&gt;
      Fowler, Martin. 2006. &lt;span&gt;"Continuous &lt;span&gt;Integration&lt;/span&gt;."&lt;/span&gt; &lt;a href=
      "https://martinfowler.com/articles/%20continuousIntegration.html"&gt;https://martinfowler.com/articles/
      continuousIntegration.html&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Gligoric2014" class="csl-entry" role="listitem"&gt;
      Gligoric, Milos, Rupak Majumdar, Rohan Sharma, Lamyaa Eloussi, and Darko Marinov. 2014.
      &lt;span&gt;"Regression &lt;span&gt;Test Selection&lt;/span&gt; for &lt;span&gt;Distributed Software
      Histories&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Computer &lt;span&gt;Aided Verification&lt;/span&gt;&lt;/em&gt;, edited by
      David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Alfred Kobsa, Friedemann
      Mattern, John C. Mitchell, et al., 8559:293-309. Cham: Springer International Publishing.
      &lt;a href=
      "https://doi.org/10.1007/978-3-319-08867-9_19"&gt;https://doi.org/10.1007/978-3-319-08867-9_19&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Guillame-Bert2023" class="csl-entry" role="listitem"&gt;
      Guillame-Bert, Mathieu, Sebastian Bruch, Richard Stotz, and Jan Pfeifer. 2023.
      &lt;span&gt;"Yggdrasil &lt;span&gt;Decision Forests&lt;/span&gt;: &lt;span&gt;A Fast&lt;/span&gt; and &lt;span&gt;Extensible
      Decision Forests Library&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Proceedings of the 29th &lt;span&gt;ACM SIGKDD
      Conference&lt;/span&gt; on &lt;span&gt;Knowledge Discovery&lt;/span&gt; and &lt;span&gt;Data Mining&lt;/span&gt;&lt;/em&gt;,
      4068-77. Long Beach CA USA: ACM. &lt;a href=
      "https://doi.org/10.1145/3580305.3599933"&gt;https://doi.org/10.1145/3580305.3599933&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Gupta2011" class="csl-entry" role="listitem"&gt;
      Gupta, Pooja, Mark Ivey, and John Penix. 2011. &lt;span&gt;"Testing at the Speed and Scale of
      &lt;span&gt;Google&lt;/span&gt;."&lt;/span&gt; &lt;a href=
      "https://google-engtools.blogspot.com/2011/06/testing-at-%20speed-and-scale-of-google.html"&gt;https://google-engtools.blogspot.com/2011/06/testing-at-
      speed-and-scale-of-google.html&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Henderson2023" class="csl-entry" role="listitem"&gt;
      Henderson, Tim A. D., Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy. 2023.
      &lt;span&gt;"Flake &lt;span&gt;Aware Culprit Finding&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;2023 &lt;span&gt;IEEE
      Conference&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;, &lt;span&gt;Verification&lt;/span&gt; and
      &lt;span&gt;Validation&lt;/span&gt; (&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;. IEEE. &lt;a href=
      "https://doi.org/10.1109/ICST57152.2023.00041"&gt;https://doi.org/10.1109/ICST57152.2023.00041&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Henderson2024" class="csl-entry" role="listitem"&gt;
      Henderson, Tim A. D., Avi Kondareddy, Sushmita Azad, and Eric Nickell. 2024.
      &lt;span&gt;"&lt;span&gt;SafeRevert&lt;/span&gt;: &lt;span&gt;When Can Breaking Changes&lt;/span&gt; Be &lt;span&gt;Automatically
      Reverted&lt;/span&gt;?"&lt;/span&gt; In &lt;em&gt;2024 &lt;span&gt;IEEE Conference&lt;/span&gt; on &lt;span&gt;Software
      Testing&lt;/span&gt;, &lt;span&gt;Verification&lt;/span&gt; and &lt;span&gt;Validation&lt;/span&gt;
      (&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;, 395-406. Toronto, ON, Canada: IEEE. &lt;a href=
      "https://doi.org/10.1109/ICST60714.2024.00043"&gt;https://doi.org/10.1109/ICST60714.2024.00043&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Hoang2024" class="csl-entry" role="listitem"&gt;
      Hoang, Minh, and Adrian Berding. 2024. &lt;span&gt;"Presubmit &lt;span&gt;Rescue&lt;/span&gt;:
      &lt;span&gt;Automatically Ignoring FlakyTest Executions&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Proceedings of the
      1st &lt;span&gt;International Workshop&lt;/span&gt; on &lt;span&gt;Flaky Tests&lt;/span&gt;&lt;/em&gt;, 1-2. Lisbon
      Portugal: ACM. &lt;a href=
      "https://doi.org/10.1145/3643656.3643896"&gt;https://doi.org/10.1145/3643656.3643896&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Hong2024" class="csl-entry" role="listitem"&gt;
      Hong, Yang, Chakkrit Tantithamthavorn, Jirat Pasuksmit, Patanamon Thongtanunam, Arik
      Friedman, Xing Zhao, and Anton Krasikov. 2024. &lt;span&gt;"Practitioners' &lt;span&gt;Challenges&lt;/span&gt;
      and &lt;span&gt;Perceptions&lt;/span&gt; of &lt;span&gt;CI Build Failure Predictions&lt;/span&gt; at
      &lt;span&gt;Atlassian&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Companion &lt;span&gt;Proceedings&lt;/span&gt; of the 32nd
      &lt;span&gt;ACM International Conference&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
      Engineering&lt;/span&gt;&lt;/em&gt;, 370-81. Porto de Galinhas Brazil: ACM. &lt;a href=
      "https://doi.org/10.1145/3663529.3663856"&gt;https://doi.org/10.1145/3663529.3663856&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Horwitz1988" class="csl-entry" role="listitem"&gt;
      Horwitz, Susan, Jan Prins, and Thomas Reps. 1988. &lt;span&gt;"On the Adequacy of Program
      Dependence Graphs for Representing Programs."&lt;/span&gt; &lt;em&gt;ACM SIGPLAN-SIGACT Symposium on
      Principles of Programming Languages (POPL)&lt;/em&gt;, 146-57. &lt;a href=
      "https://doi.org/10.1145/73560.73573"&gt;https://doi.org/10.1145/73560.73573&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Jin2024" class="csl-entry" role="listitem"&gt;
      Jin, Xianhao, Yifei Feng, Chen Wang, Yang Liu, Yongning Hu, Yufei Gao, Kun Xia, and Luchuan
      Guo. 2024. &lt;span&gt;"&lt;span&gt;PIPELINEASCODE&lt;/span&gt;: &lt;span&gt;A CI&lt;/span&gt;/&lt;span&gt;CD Workflow Management
      System&lt;/span&gt; Through &lt;span&gt;Configuration Files&lt;/span&gt; at &lt;span&gt;ByteDance&lt;/span&gt;."&lt;/span&gt; In
      &lt;em&gt;2024 &lt;span&gt;IEEE International Conference&lt;/span&gt; on &lt;span&gt;Software Analysis&lt;/span&gt;,
      &lt;span&gt;Evolution&lt;/span&gt; and &lt;span&gt;Reengineering&lt;/span&gt; (&lt;span&gt;SANER&lt;/span&gt;)&lt;/em&gt;, 1011-22.
      Rovaniemi, Finland: IEEE. &lt;a href=
      "https://doi.org/10.1109/SANER60148.2024.00109"&gt;https://doi.org/10.1109/SANER60148.2024.00109&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Jin2023" class="csl-entry" role="listitem"&gt;
      Jin, Xianhao, and Francisco Servant. 2023. &lt;span&gt;"&lt;span&gt;HybridCISave&lt;/span&gt;: &lt;span&gt;A Combined
      Build&lt;/span&gt; and &lt;span&gt;Test Selection Approach&lt;/span&gt; in &lt;span&gt;Continuous
      Integration&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;ACM Transactions on Software Engineering and Methodology&lt;/em&gt;
      32 (4): 1-39. &lt;a href="https://doi.org/10.1145/3576038"&gt;https://doi.org/10.1145/3576038&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-keenan2019" class="csl-entry" role="listitem"&gt;
      Keenan, James. 2019. &lt;span&gt;"James &lt;span&gt;E&lt;/span&gt;. &lt;span&gt;Keenan&lt;/span&gt; -
      "&lt;span&gt;Multisection&lt;/span&gt;: &lt;span&gt;When Bisection Isn&lt;/span&gt;'t &lt;span&gt;Enough&lt;/span&gt; to
      &lt;span&gt;Debug&lt;/span&gt; a &lt;span&gt;Problem&lt;/span&gt;"."&lt;/span&gt; &lt;a href=
      "https://www.youtube.com/watch?v=05CwdTRt6AM"&gt;https://www.youtube.com/watch?v=05CwdTRt6AM&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Leon2003" class="csl-entry" role="listitem"&gt;
      Leon, D., and A. Podgurski. 2003. &lt;span&gt;"A Comparison of Coverage-Based and
      Distribution-Based Techniques for Filtering and Prioritizing Test Cases."&lt;/span&gt; In &lt;em&gt;14th
      &lt;span&gt;International Symposium&lt;/span&gt; on &lt;span&gt;Software Reliability Engineering&lt;/span&gt;, 2003.
      &lt;span&gt;ISSRE&lt;/span&gt; 2003.&lt;/em&gt;, 2003-Janua:442-53. IEEE. &lt;a href=
      "https://doi.org/10.1109/ISSRE.2003.1251065"&gt;https://doi.org/10.1109/ISSRE.2003.1251065&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Leong2019" class="csl-entry" role="listitem"&gt;
      Leong, Claire, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John Micco. 2019.
      &lt;span&gt;"Assessing &lt;span&gt;Transition-Based Test Selection Algorithms&lt;/span&gt; at
      &lt;span&gt;Google&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;2019 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st
      &lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
      Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt; (&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 101-10. IEEE.
      &lt;a href=
      "https://doi.org/10.1109/ICSE-SEIP.2019.00019"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00019&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Liu2023" class="csl-entry" role="listitem"&gt;
      Liu, Bohan, He Zhang, Weigang Ma, Gongyuan Li, Shanshan Li, and Haifeng Shen. 2023.
      &lt;span&gt;"The &lt;span&gt;Why&lt;/span&gt;, &lt;span&gt;When&lt;/span&gt;, &lt;span&gt;What&lt;/span&gt;, and &lt;span&gt;How About
      Predictive Continuous Integration&lt;/span&gt;: &lt;span&gt;A Simulation-Based
      Investigation&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt; 49 (12):
      5223-49. &lt;a href=
      "https://doi.org/10.1109/TSE.2023.3330510"&gt;https://doi.org/10.1109/TSE.2023.3330510&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Machalica2019" class="csl-entry" role="listitem"&gt;
      Machalica, Mateusz, Alex Samylkin, Meredith Porth, and Satish Chandra. 2019.
      &lt;span&gt;"Predictive &lt;span&gt;Test Selection&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;2019
      &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International Conference&lt;/span&gt; on
      &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
      (&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 91-100. IEEE. &lt;a href=
      "https://doi.org/10.1109/ICSE-SEIP.2019.00018"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00018&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Memon2017" class="csl-entry" role="listitem"&gt;
      Memon, Atif, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob Siemborski, and John
      Micco. 2017. &lt;span&gt;"Taming &lt;span class="nocase"&gt;Google-scale&lt;/span&gt; Continuous
      Testing."&lt;/span&gt; In &lt;em&gt;2017 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 39th &lt;span&gt;International
      Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software Engineering&lt;/span&gt; in
      &lt;span&gt;Practice Track&lt;/span&gt; (&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 233-42. Piscataway, NJ, USA: IEEE.
      &lt;a href=
      "https://doi.org/10.1109/ICSE-SEIP.2017.16"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.16&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Micco2012" class="csl-entry" role="listitem"&gt;
      Micco, John. 2012. &lt;span&gt;"Tools for &lt;span&gt;Continuous Integration&lt;/span&gt; at &lt;span&gt;Google
      Scale&lt;/span&gt;."&lt;/span&gt; Tech {{Talk}}. Google NYC. &lt;a href=
      "https://youtu.be/KH2_sB1A6lA"&gt;https://youtu.be/KH2_sB1A6lA&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Micco2013" class="csl-entry" role="listitem"&gt;
      ---. 2013. &lt;span&gt;"Continuous &lt;span&gt;Integration&lt;/span&gt; at &lt;span&gt;Google Scale&lt;/span&gt;."&lt;/span&gt;
      Lecture. EclipseCon 2013. &lt;a href=
      "https://web.archive.org/web/20140705215747/https://%20www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-%2003-24%20Continuous%20Integration%20at%20Google%20Scale.pdf"&gt;
      https://web.archive.org/web/20140705215747/https://
      www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
      03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Mondal2015" class="csl-entry" role="listitem"&gt;
      Mondal, Debajyoti, Hadi Hemmati, and Stephane Durocher. 2015. &lt;span&gt;"Exploring Test Suite
      Diversification and Code Coverage in Multi-Objective Test Case Selection."&lt;/span&gt; &lt;em&gt;2015
      IEEE 8th International Conference on Software Testing, Verification and Validation, ICST 2015
      - Proceedings&lt;/em&gt;, 1-10. &lt;a href=
      "https://doi.org/10.1109/ICST.2015.7102588"&gt;https://doi.org/10.1109/ICST.2015.7102588&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Musa2015" class="csl-entry" role="listitem"&gt;
      Musa, Samaila, Abu Bakar Md Sultan, Abdul Azim Bin Abd-Ghani, and Salmi Baharom. 2015.
      &lt;span&gt;"Regression &lt;span&gt;Test Cases&lt;/span&gt; Selection for &lt;span&gt;Object-Oriented Programs&lt;/span&gt;
      Based on &lt;span&gt;Affected Statements&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;International Journal of Software
      Engineering and Its Applications&lt;/em&gt; 9 (10): 91-108. &lt;a href=
      "https://doi.org/10.14257/ijseia.2015.9.10.10"&gt;https://doi.org/10.14257/ijseia.2015.9.10.10&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Najafi2019a" class="csl-entry" role="listitem"&gt;
      Najafi, Armin, Peter C. Rigby, and Weiyi Shang. 2019. &lt;span&gt;"Bisecting Commits and Modeling
      Commit Risk During Testing."&lt;/span&gt; In &lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint
      Meeting&lt;/span&gt; on &lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
      &lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
      Engineering&lt;/span&gt;&lt;/em&gt;, 279-89. New York, NY, USA: ACM. &lt;a href=
      "https://doi.org/10.1145/3338906.3338944"&gt;https://doi.org/10.1145/3338906.3338944&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Najafi2019" class="csl-entry" role="listitem"&gt;
      Najafi, Armin, Weiyi Shang, and Peter C. Rigby. 2019. &lt;span&gt;"Improving &lt;span&gt;Test
      Effectiveness Using Test Executions History&lt;/span&gt;: &lt;span&gt;An Industrial Experience
      Report&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;2019 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st
      &lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
      Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt; (&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 213-22. IEEE.
      &lt;a href=
      "https://doi.org/10.1109/ICSE-SEIP.2019.00031"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00031&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Ocariza2022" class="csl-entry" role="listitem"&gt;
      Ocariza, Frolin S. 2022. &lt;span&gt;"On the &lt;span&gt;Effectiveness&lt;/span&gt; of &lt;span&gt;Bisection&lt;/span&gt;
      in &lt;span&gt;Performance Regression Localization&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;Empirical Software
      Engineering&lt;/em&gt; 27 (4): 95. &lt;a href=
      "https://doi.org/10.1007/s10664-022-10152-3"&gt;https://doi.org/10.1007/s10664-022-10152-3&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-pan2022" class="csl-entry" role="listitem"&gt;
      Pan, Rongqi, Mojtaba Bagherzadeh, Taher A. Ghaleb, and Lionel Briand. 2022. &lt;span&gt;"Test Case
      Selection and Prioritization Using Machine Learning: A Systematic Literature Review."&lt;/span&gt;
      &lt;em&gt;Empirical Software Engineering&lt;/em&gt; 27 (2): 29. &lt;a href=
      "https://doi.org/10.1007/s10664-021-10066-6"&gt;https://doi.org/10.1007/s10664-021-10066-6&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Podgurski1989" class="csl-entry" role="listitem"&gt;
      Podgurski, A, and L Clarke. 1989. &lt;span&gt;"The &lt;span&gt;Implications&lt;/span&gt; of &lt;span&gt;Program
      Dependencies&lt;/span&gt; for &lt;span&gt;Software Testing&lt;/span&gt;, &lt;span&gt;Debugging&lt;/span&gt;, and
      &lt;span&gt;Maintenance&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Proceedings of the &lt;span&gt;ACM SIGSOFT&lt;/span&gt; '89
      &lt;span&gt;Third Symposium&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;, &lt;span&gt;Analysis&lt;/span&gt;, and
      &lt;span&gt;Verification&lt;/span&gt;&lt;/em&gt;, 168-78. New York, NY, USA: ACM. &lt;a href=
      "https://doi.org/10.1145/75308.75328"&gt;https://doi.org/10.1145/75308.75328&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Potvin2016" class="csl-entry" role="listitem"&gt;
      Potvin, Rachel, and Josh Levenberg. 2016. &lt;span&gt;"Why Google Stores Billions of Lines of Code
      in a Single Repository."&lt;/span&gt; &lt;em&gt;Communications of the ACM&lt;/em&gt; 59 (7): 78-87. &lt;a href=
      "https://doi.org/10.1145/2854146"&gt;https://doi.org/10.1145/2854146&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Rodriguez-Perez2018" class="csl-entry" role="listitem"&gt;
      Rodríguez-Pérez, Gema, Gregorio Robles, and Jesús M. González-Barahona. 2018.
      &lt;span&gt;"Reproducibility and Credibility in Empirical Software Engineering: &lt;span&gt;A&lt;/span&gt; Case
      Study Based on a Systematic Literature Review of the Use of the &lt;span&gt;SZZ&lt;/span&gt;
      Algorithm."&lt;/span&gt; &lt;em&gt;Information and Software Technology&lt;/em&gt; 99 (July): 164-76. &lt;a href=
      "https://doi.org/10.1016/j.infsof.2018.03.009"&gt;https://doi.org/10.1016/j.infsof.2018.03.009&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Rothermel1997" class="csl-entry" role="listitem"&gt;
      Rothermel, Gregg, and Mary Jean Harrold. 1997. &lt;span&gt;"A Safe, Efficient Regression Test
      Selection Technique."&lt;/span&gt; &lt;em&gt;ACM Transactions on Software Engineering and
      Methodology&lt;/em&gt; 6 (2): 173-210. &lt;a href=
      "https://doi.org/10.1145/248233.248262"&gt;https://doi.org/10.1145/248233.248262&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Sadowski2018" class="csl-entry" role="listitem"&gt;
      Sadowski, Caitlin, Emma Söderberg, Luke Church, Michal Sipko, and Alberto Bacchelli. 2018.
      &lt;span&gt;"Modern Code Review: A Case Study at Google."&lt;/span&gt; In &lt;em&gt;Proceedings of the 40th
      &lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
      Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;&lt;/em&gt;, 181-90. Gothenburg Sweden: ACM. &lt;a href=
      "https://doi.org/10.1145/3183519.3183525"&gt;https://doi.org/10.1145/3183519.3183525&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Saha2017" class="csl-entry" role="listitem"&gt;
      Saha, Ripon, and Milos Gligoric. 2017. &lt;span&gt;"Selective &lt;span&gt;Bisection
      Debugging&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Fundamental &lt;span&gt;Approaches&lt;/span&gt; to &lt;span&gt;Software
      Engineering&lt;/span&gt;&lt;/em&gt;, edited by Marieke Huisman and Julia Rubin, 10202:60-77. Berlin,
      Heidelberg: Springer Berlin Heidelberg. &lt;a href=
      "https://doi.org/10.1007/978-3-662-54494-5_4"&gt;https://doi.org/10.1007/978-3-662-54494-5_4&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Saidani2020" class="csl-entry" role="listitem"&gt;
      Saidani, Islem, Ali Ouni, Moataz Chouchen, and Mohamed Wiem Mkaouer. 2020. &lt;span&gt;"Predicting
      Continuous Integration Build Failures Using Evolutionary Search."&lt;/span&gt; &lt;em&gt;Information and
      Software Technology&lt;/em&gt; 128 (December): 106392. &lt;a href=
      "https://doi.org/10.1016/j.infsof.2020.106392"&gt;https://doi.org/10.1016/j.infsof.2020.106392&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Saidani2022" class="csl-entry" role="listitem"&gt;
      Saidani, Islem, Ali Ouni, and Mohamed Wiem Mkaouer. 2022a. &lt;span&gt;"Improving the Prediction of
      Continuous Integration Build Failures Using Deep Learning."&lt;/span&gt; &lt;em&gt;Automated Software
      Engineering&lt;/em&gt; 29 (1): 21. &lt;a href=
      "https://doi.org/10.1007/s10515-021-00319-5"&gt;https://doi.org/10.1007/s10515-021-00319-5&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Saidani2022a" class="csl-entry" role="listitem"&gt;
      ---. 2022b. &lt;span&gt;"Detecting &lt;span&gt;Continuous Integration Skip Commits Using Multi-Objective
      Evolutionary Search&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt; 48
      (12): 4873-91. &lt;a href=
      "https://doi.org/10.1109/TSE.2021.3129165"&gt;https://doi.org/10.1109/TSE.2021.3129165&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Singh2012" class="csl-entry" role="listitem"&gt;
      Singh, Yogesh, Arvinder Kaur, Bharti Suri, and Shweta Singhal. 2012. &lt;span&gt;"Systematic
      Literature Review on Regression Test Prioritization Techniques."&lt;/span&gt; &lt;em&gt;Informatica
      (Slovenia)&lt;/em&gt; 36 (4): 379-408. &lt;a href=
      "https://doi.org/10.31449/inf.v36i4.420"&gt;https://doi.org/10.31449/inf.v36i4.420&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Sliwerski2005" class="csl-entry" role="listitem"&gt;
      Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller. 2005. &lt;span&gt;"When Do Changes Induce
      Fixes?"&lt;/span&gt; &lt;em&gt;ACM SIGSOFT Software Engineering Notes&lt;/em&gt; 30 (4): 1. &lt;a href=
      "https://doi.org/10.1145/1082983.1083147"&gt;https://doi.org/10.1145/1082983.1083147&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Sun2024" class="csl-entry" role="listitem"&gt;
      Sun, Gengyi, Sarra Habchi, and Shane McIntosh. 2024. &lt;span&gt;"&lt;span&gt;RavenBuild&lt;/span&gt;:
      &lt;span&gt;Context&lt;/span&gt;, &lt;span&gt;Relevance&lt;/span&gt;, and &lt;span&gt;Dependency Aware Build Outcome
      Prediction&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;Proceedings of the ACM on Software Engineering&lt;/em&gt; 1 (FSE):
      996-1018. &lt;a href="https://doi.org/10.1145/3643771"&gt;https://doi.org/10.1145/3643771&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Wang2024" class="csl-entry" role="listitem"&gt;
      Wang, Guoqing, Zeyu Sun, Yizhou Chen, Yifan Zhao, Qingyuan Liang, and Dan Hao. 2024.
      &lt;span&gt;"Commit &lt;span&gt;Artifact Preserving Build Prediction&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Proceedings
      of the 33rd &lt;span&gt;ACM SIGSOFT International Symposium&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;
      and &lt;span&gt;Analysis&lt;/span&gt;&lt;/em&gt;, 1236-48. Vienna Austria: ACM. &lt;a href=
      "https://doi.org/10.1145/3650212.3680356"&gt;https://doi.org/10.1145/3650212.3680356&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Wang2021" class="csl-entry" role="listitem"&gt;
      Wang, Kaiyuan, Daniel Rall, Greg Tener, Vijay Gullapalli, Xin Huang, and Ahmed Gad. 2021.
      &lt;span&gt;"Smart &lt;span&gt;Build Targets Batching Service&lt;/span&gt; at &lt;span&gt;Google&lt;/span&gt;."&lt;/span&gt; In
      &lt;em&gt;2021 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 43rd &lt;span&gt;International Conference&lt;/span&gt; on
      &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
      (&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 160-69. Madrid, ES: IEEE. &lt;a href=
      "https://doi.org/10.1109/ICSE-SEIP52600.2021.00025"&gt;https://doi.org/10.1109/ICSE-SEIP52600.2021.00025&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Wang2020" class="csl-entry" role="listitem"&gt;
      Wang, Kaiyuan, Greg Tener, Vijay Gullapalli, Xin Huang, Ahmed Gad, and Daniel Rall. 2020.
      &lt;span&gt;"Scalable Build Service System with Smart Scheduling Service."&lt;/span&gt; In
      &lt;em&gt;Proceedings of the 29th &lt;span&gt;ACM SIGSOFT International Symposium&lt;/span&gt; on
      &lt;span&gt;Software Testing&lt;/span&gt; and &lt;span&gt;Analysis&lt;/span&gt;&lt;/em&gt;, 452-62. Virtual Event USA: ACM.
      &lt;a href=
      "https://doi.org/10.1145/3395363.3397371"&gt;https://doi.org/10.1145/3395363.3397371&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Wen2019" class="csl-entry" role="listitem"&gt;
      Wen, Ming, Rongxin Wu, Yepang Liu, Yongqiang Tian, Xuan Xie, Shing-Chi Cheung, and Zhendong
      Su. 2019. &lt;span&gt;"Exploring and Exploiting the Correlations Between Bug-Inducing and
      Bug-Fixing Commits."&lt;/span&gt; In &lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint
      Meeting&lt;/span&gt; on &lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
      &lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
      Engineering&lt;/span&gt;&lt;/em&gt;, 326-37. Tallinn Estonia: ACM. &lt;a href=
      "https://doi.org/10.1145/3338906.3338962"&gt;https://doi.org/10.1145/3338906.3338962&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Zeng2024" class="csl-entry" role="listitem"&gt;
      Zeng, Zhili, Tao Xiao, Maxime Lamothe, Hideaki Hata, and Shane Mcintosh. 2024. &lt;span&gt;"A
      &lt;span&gt;Mutation-Guided Assessment&lt;/span&gt; of &lt;span&gt;Acceleration Approaches&lt;/span&gt; for
      &lt;span&gt;Continuous Integration&lt;/span&gt;: &lt;span&gt;An Empirical Study&lt;/span&gt; of
      &lt;span&gt;YourBase&lt;/span&gt;."&lt;/span&gt; In &lt;em&gt;Proceedings of the 21st &lt;span&gt;International
      Conference&lt;/span&gt; on &lt;span&gt;Mining Software Repositories&lt;/span&gt;&lt;/em&gt;, 556-68. Lisbon Portugal:
      ACM. &lt;a href=
      "https://doi.org/10.1145/3643991.3644914"&gt;https://doi.org/10.1145/3643991.3644914&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Zhou2010" class="csl-entry" role="listitem"&gt;
      Zhou, Zhi Quan. 2010. &lt;span&gt;"Using Coverage Information to Guide Test Case Selection in
      &lt;span&gt;Adaptive Random Testing&lt;/span&gt;."&lt;/span&gt; &lt;em&gt;Proceedings - International Computer
      Software and Applications Conference&lt;/em&gt;, 208-13. &lt;a href=
      "https://doi.org/10.1109/COMPSACW.2010.43"&gt;https://doi.org/10.1109/COMPSACW.2010.43&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Ziftci2013a" class="csl-entry" role="listitem"&gt;
      Ziftci, Celal, and Vivek Ramavajjala. 2013. &lt;span&gt;"Finding &lt;span&gt;Culprits
      Automatically&lt;/span&gt; in &lt;span&gt;Failing Builds&lt;/span&gt; - i.e. &lt;span&gt;Who Broke&lt;/span&gt; the
      &lt;span&gt;Build&lt;/span&gt;?"&lt;/span&gt; &lt;a href=
      "https://www.youtube.com/watch?v=SZLuBYlq3OM"&gt;https://www.youtube.com/watch?v=SZLuBYlq3OM&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
    &lt;div id="ref-Ziftci2017" class="csl-entry" role="listitem"&gt;
      Ziftci, Celal, and Jim Reardon. 2017. &lt;span&gt;"Who Broke the Build? &lt;span&gt;Automatically&lt;/span&gt;
      Identifying Changes That Induce Test Failures in Continuous Integration at Google
      Scale."&lt;/span&gt; &lt;em&gt;Proceedings - 2017 IEEE/ACM 39th International Conference on Software
      Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017&lt;/em&gt;, 113-22. &lt;a href=
      "https://doi.org/10.1109/ICSE-SEIP.2017.13"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.13&lt;/a&gt;.
    &lt;/div&gt;
    &lt;/li&gt;
  &lt;/div&gt;
  &lt;/ul&gt;
&lt;section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"&gt;
    &lt;hr /&gt;
    &lt;ol&gt;
      &lt;li id="fn1"&gt;
        &lt;p&gt;&lt;a href="https://bazel.build/" class="uri"&gt;https://bazel.build/&lt;/a&gt;&lt;a href="#fnref1"
        class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li id="fn2"&gt;
        &lt;p&gt;When a target breaks we perform culprit finding. The conclusion of the culprit finding
        is the culprit commit. We colloquially say the target is blaming the culprit.&lt;a href=
        "#fnref2" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/section&gt;</content><category term="Paper"></category></entry><entry><title>SafeRevert: When Can Breaking Changes be Automatically Reverted?</title><link href="https://hackthology.com/saferevert-when-can-breaking-changes-be-automatically-reverted.html" rel="alternate"></link><published>2024-05-27T00:00:00-04:00</published><updated>2024-05-27T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;,  Avi Kondareddy, Sushmita Azad, Eric Nickell</name></author><id>tag:hackthology.com,2024-05-27:/saferevert-when-can-breaking-changes-be-automatically-reverted.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Avi Kondareddy, Sushmita Azad, and Eric Nickell.
&lt;em&gt;SafeRevert: When Can Breaking Changes be Automatically Reverted?&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icst-2024/icst-2024-industry/7/SafeRevert-When-Can-Breaking-Changes-be-Automatically-Reverted-"&gt;ICST Industry Track 2024&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/ICST60714.2024.00043"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2024.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/saferevert-when-can-breaking-changes-be-automatically-reverted.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2024.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract …&lt;/h4&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Avi Kondareddy, Sushmita Azad, and Eric Nickell.
&lt;em&gt;SafeRevert: When Can Breaking Changes be Automatically Reverted?&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icst-2024/icst-2024-industry/7/SafeRevert-When-Can-Breaking-Changes-be-Automatically-Reverted-"&gt;ICST Industry Track 2024&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/ICST60714.2024.00043"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2024.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/saferevert-when-can-breaking-changes-be-automatically-reverted.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2024.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;When bugs or defects are introduced into a large scale software repository,
they reduce productivity. Programmers working on related areas of the code
will encounter test failures, compile breakages, or other anomalous behavior.
On encountering these issues, they will need to troubleshoot and determine
that their changes were not the cause of the error and that another change is
at fault. They must then find that change and &lt;em&gt;revert it&lt;/em&gt; to return the
repository to a healthy state. In the past, our group has identified ways to
identify the root cause (or culprit) change that introduced a test failure
even when the test is flaky. This paper focuses on a related issue: at what
point does the Continuous Integration system have enough evidence to support
&lt;em&gt;automatically reverting a change&lt;/em&gt;? We will motivate the problem, provide
several methods to address it, and empirically evaluate our solution on a
large set (25,137) of real-world breaking changes that occurred at
Google. SafeRevert improved recall (number of changes recommend for reversion)
by 2x over the baseline method while meeting our safety criterion.&lt;/p&gt;
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;h1 id="sec:introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Large scale software development is enabled by automatically
executing tests in a continuous integration environment. Continuous
integration (CI) &lt;span class="citation" data-cites="Fowler2006"&gt;(&lt;a
href="#ref-Fowler2006" role="doc-biblioref"&gt;Fowler 2006&lt;/a&gt;)&lt;/span&gt; is
the industrial practice of using automated systems to automatically
integrate changes into the source of truth for the software system or
repository. This improves collaboration by helping software developers
avoid breaking compilation, tests, or structure of the system that
others are relying on.&lt;/p&gt;
&lt;p&gt;Corporations and teams may engage in CI using adhoc tools across many
independent software repositories. For instance Github (github.com)
supports CI “Actions" which can be integrated into any repository.
However, many organizations are recognizing the value of using a
“mono-repository" (monorepo) development model where many or all teams
in the organization use a single shared software repository &lt;span
class="citation" data-cites="Potvin2016"&gt;(&lt;a href="#ref-Potvin2016"
role="doc-biblioref"&gt;Potvin and Levenberg 2016&lt;/a&gt;)&lt;/span&gt;. At the
largest organizations such as Google &lt;span class="citation"
data-cites="Potvin2016 Memon2017 Henderson2023"&gt;(&lt;a
href="#ref-Potvin2016" role="doc-biblioref"&gt;Potvin and Levenberg
2016&lt;/a&gt;; &lt;a href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al.
2017&lt;/a&gt;; &lt;a href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et
al. 2023&lt;/a&gt;)&lt;/span&gt;, Microsoft &lt;span class="citation"
data-cites="Herzig2015a"&gt;(&lt;a href="#ref-Herzig2015a"
role="doc-biblioref"&gt;Herzig and Nagappan 2015&lt;/a&gt;)&lt;/span&gt;, and Facebook
&lt;span class="citation" data-cites="Machalica2019"&gt;(&lt;a
href="#ref-Machalica2019" role="doc-biblioref"&gt;Machalica et al.
2019&lt;/a&gt;)&lt;/span&gt; large repositories are complimented by advanced
centralized CI systems.&lt;/p&gt;
&lt;p&gt;In these large, modern CI systems, the integration goes beyond just
ensuring the code textually merges. Compilations are invoked, tests are
executed, and additional static and dynamic verification steps are
performed. The demand for machine resources can exceed capacity for
build, test, and verification tasks desired by a large-scale CI system.
To combat this problem, test case selection or prioritization is used
&lt;span class="citation"
data-cites="Gupta2011 Micco2013 Machalica2019"&gt;(&lt;a href="#ref-Gupta2011"
role="doc-biblioref"&gt;Gupta, Ivey, and Penix 2011&lt;/a&gt;; &lt;a
href="#ref-Micco2013" role="doc-biblioref"&gt;Micco 2013&lt;/a&gt;; &lt;a
href="#ref-Machalica2019" role="doc-biblioref"&gt;Machalica et al.
2019&lt;/a&gt;)&lt;/span&gt; to select fewer tests to run.&lt;/p&gt;
&lt;p&gt;Additionally, CI steps are often invoked at multiple points in the
Software Development Life Cycle. In the past, it may have been assumed
that the tests were executed by the CI system once, at the time a commit
was created in the version control. Today, CI may execute tests multiple
times during development: when a change is sent for code review,
immediately prior to submission or integration into the main development
branch, after one or more changes has been integrated into the
development branch, and when a new release is created. &lt;em&gt;This paper is
primary concerned with testing that occurs after the code has been
integrated into the main development branch.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="why-is-testing-necessary-after-code-integration"&gt;Why is Testing
Necessary After Code Integration?&lt;/h2&gt;
&lt;p&gt;When deploying CI for the first time, many organizations primarily
focus on conducting testing at the time a change is merged into the main
branch. For instance, they may test when a Pull Request (PR) on Github
is going to be merged into the main branch. The PR is merged if the
tests pass and no changes have been made to the main branch since the
testing started. This strategy works well until the rate of PR
submission exceeds the average amount of time it takes to run all the
tests.&lt;/p&gt;
&lt;p&gt;At this point, organizations may do a stop gap fix such as adding
more machines to run tests in parallel or reducing the size of the test
suite. However, at some point, these measures will prove ineffective and
the rate that testing can be conducted will become a impediment to an
organization’s engineering velocity. To address this, one common
solution is to introduce a “submission queue" which batches changes
together for testing and merges them all if the tests pass &lt;span
class="citation" data-cites="Ananthanarayanan2019"&gt;(&lt;a
href="#ref-Ananthanarayanan2019" role="doc-biblioref"&gt;Ananthanarayanan
et al. 2019&lt;/a&gt;)&lt;/span&gt;. If the tests fail, the offending change must be
identified and the remaining changes in the batch must be retested (at
least doubling the total testing time) &lt;span class="citation"
data-cites="Najafi2019"&gt;(&lt;a href="#ref-Najafi2019"
role="doc-biblioref"&gt;Najafi, Shang, and Rigby 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As the submission rate continues to increase, the organization may
add conservative test selection based on file-level dependence analysis
&lt;span class="citation" data-cites="Ananthanarayanan2019"&gt;(&lt;a
href="#ref-Ananthanarayanan2019" role="doc-biblioref"&gt;Ananthanarayanan
et al. 2019&lt;/a&gt;)&lt;/span&gt;. But at this point, the organization will be
reaching the limit of what can be done to completely prevent any
breaking changes from being integrated into the main development branch
beyond just buying more and more machines to further parallelize the
testing.&lt;/p&gt;
&lt;p&gt;And what about just buying more machines? Won’t this effectively
solve the problem? It would until the submission rate exceeds the time
it takes to run the slowest test. At this point, the number of machines
purchased can only partially control impact to developer productivity.
Even with a very large budget for testing, the number of changes per
batch will continue to grow as increases in the number of machines will
not reduce the batch size but only keep the testing time per batch
relatively constant. The larger the batch size, the more likely that a
developer will experience conflicts with another change when attempting
to submit. These conflicts (either at the syntactical or semantic level)
will make it difficult for developers to reliably submit their changes
and require them to constantly monitor the submission process. This
monitoring will hurt overall developer productivity.&lt;/p&gt;
&lt;p&gt;At the highest submission rates seen in industry today, using
submission queues that guarantee zero breaking changes becomes
infeasible both from a machine cost perspective and from a developer
productivity perspective. Therefore, many organizations relax the
requirement that an integrated change will never break a test. Even
while using submit queues to gate contributions for a single team,
Google has relied on postsubmit testing for mission-critical software
since the early 2000s as documented by Mike Bland &lt;span class="citation"
data-cites="Bland2012"&gt;(&lt;a href="#ref-Bland2012"
role="doc-biblioref"&gt;Bland 2012&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="post-submission-testing"&gt;Post Submission Testing&lt;/h2&gt;
&lt;p&gt;The purpose of testing after a change has been integrated (called
&lt;em&gt;Post-Submission&lt;/em&gt; Testing or &lt;em&gt;Postsubmit&lt;/em&gt; Testing
hereafter) is to identify defects that slipped through testing that
occurred prior to integration. Typically, organizations are unable to
run all tests at every integrated change. Instead, testing is conducted
periodically. In this paper we will refer to this periodic testing as
&lt;em&gt;Testing Cycles&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Figure &lt;a href="#fig:ci-timeline" data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt; is a simplified
view of the postsubmit testing strategy used at Google. The test
scheduler waits until our Build System &lt;span class="citation"
data-cites="Wang2020"&gt;(&lt;a href="#ref-Wang2020" role="doc-biblioref"&gt;Wang
et al. 2020&lt;/a&gt;)&lt;/span&gt; has capacity to start a test cycle. It then
schedules tests and then waits until the system has capacity again. When
scheduling tests, certain tests may be temporarily skipped or throttled
to conserve resources. As shown in Figure &lt;a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt;, this leads to
the system usually detecting new failures some time after the version
that introduced them had been integrated. This leads to the problem of
“Culprit Finding."&lt;/p&gt;
&lt;h2 id="culprit-finding"&gt;Culprit Finding&lt;/h2&gt;
&lt;p&gt;Culprit finding is conceptually simple: given a list of versions that
may have introduced a fault, locate the offending change. One may solve
this problem with a variety of techniques: Zeller’s Delta Debugging
&lt;span class="citation" data-cites="Zeller1999"&gt;(&lt;a
href="#ref-Zeller1999" role="doc-biblioref"&gt;Zeller 1999&lt;/a&gt;)&lt;/span&gt;,
Git’s bisection algorithm &lt;span class="citation"
data-cites="Couder2008"&gt;(&lt;a href="#ref-Couder2008"
role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;, or Google’s Flake Aware
Culprit Finding (FACF) algorithm &lt;span class="citation"
data-cites="Henderson2023"&gt;(&lt;a href="#ref-Henderson2023"
role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt;. Every technique
used to identify the offending change will have some error rate in
industrial practice.&lt;/p&gt;
&lt;p&gt;But wait! How can a binary search or delta debugging have an “error
rate"? The answer is that old nemesis of industrial practice: flaky or
non-deterministic test behavior &lt;span class="citation"
data-cites="parry2022"&gt;(&lt;a href="#ref-parry2022"
role="doc-biblioref"&gt;Parry et al. 2022&lt;/a&gt;)&lt;/span&gt;. Flaky tests can be
caused both by problems in the production code (ex: race conditions
causing rare errors), in the test code (ex: use of “sleep") and by
infrastructure problems (ex: unreliable machine with bad ram or network
congestion). All of these problems behave in &lt;em&gt;version
non-hermetic&lt;/em&gt; ways, where failures may not be strictly linked to the
version of code executed. Furthermore, even when the test is fully
&lt;em&gt;version hermetic&lt;/em&gt; the flakiness may not obey a Bernoulli
distribution as subsequent executions may not be fully independent of
the prior ones.&lt;/p&gt;
&lt;p&gt;The above issues mean that even culprit finding algorithms such as
FACF &lt;span class="citation" data-cites="Henderson2023"&gt;(&lt;a
href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al.
2023&lt;/a&gt;)&lt;/span&gt; that have been purpose-built to mitigate flakiness will
have some error rate. While that error rate will be much less than a
naive algorithm, it may still be high enough to cause problems when
deployed in certain environments.&lt;/p&gt;
&lt;h2 id="intro:auto-revert"&gt;Automatically Reverting Changes&lt;/h2&gt;
&lt;p&gt;Once an organization has a reliable and high performance culprit
finding system, it is natural to use it to automatically revert (undo,
roll back) changes that introduce defects into the main development
branch. By automatically undoing these changes, the system decreases the
amount of time developers working with impacted tests will experience
breakages that are unrelated to the changes they are working on. It will
also increase the number of versions that are viable to be used to make
software releases. Reducing development friction and increasing the
number of release candidates improves developer productivity by reducing
the amount of time developers spend troubleshooting tests that were
broken by someone else.&lt;/p&gt;
&lt;p&gt;Unfortunately, naively integrating FACF directly into a system that
immediately reverts all changes it identifies will lead to unhappy
developers. This is because even though Google’s FACF has a measured
&lt;em&gt;per-test accuracy&lt;/em&gt; of 99.54%, when aggregated (grouped) by
blamed change, the accuracy &lt;em&gt;per-change&lt;/em&gt; is only 77.37% in the
last 28 day window as of this writing.&lt;a href="#fn1"
class="footnote-ref" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;
This would translate into &lt;em&gt;incorrectly reverting&lt;/em&gt; approximately
20-130 changes per day out of an approximately of 300-500 total changes
per day reverted (see Figure &lt;a href="#fig:baseline"
data-reference-type="ref"
data-reference="fig:baseline"&gt;[fig:baseline]&lt;/a&gt;). At Google, we find
the cost of incorrectly reverting a change is extremely high in terms of
developer toil and frustration. Therefore, we need to reduce the rate of
bad reversions as much as possible. In this work, we aim to incorrectly
revert fewer than one change per day on average.&lt;/p&gt;
&lt;h2 id="our-contributions-and-findings"&gt;Our Contributions and
Findings&lt;/h2&gt;
&lt;p&gt;In this paper we propose a shallow machine-learning based method for
using the output of a culprit finding algorithm to automatically revert
(undo, rollback) a change that controls for potential errors from the
culprit finding algorithm. Our method is generic and can be used with
any culprit finding algorithm.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;SafeRevert: a method for using the output of any culprit finding
algorithm to automatically revert (undo) a change. SafeRevert controls
for the error rate while increasing total number of reversions over
baseline methods.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;An ablation study on the models, designs and features used by the
machine learning system used in SafeRevert to identify the most
impactful features and best models.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A large scale study on SafeRevert’s efficacy using &lt;span
class="math inline"&gt;\(\thicksim\)&lt;/span&gt;25,137 potential culprit changes
identified by Google’s production culprit finder over a &lt;span
class="math inline"&gt;\(\sim\)&lt;/span&gt;3 month window yields a recall of
55.7%, &lt;span class="math inline"&gt;\(\sim2.1 \times\)&lt;/span&gt; higher than
the baseline method.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="background"&gt;Background&lt;/h1&gt;

&lt;p&gt;&lt;span id="fig:ci-timeline" label="fig:ci-timeline"&gt;
&lt;a href="images/icst-2024/fig1.png"&gt;&lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;&lt;/em&gt;
&lt;img alt="Figure 1" src="images/icst-2024/fig1.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Test Automation Platform (TAP) is a proprietary Continuous
Integration (CI) system at Google. It runs tests that execute on a
single machine without the use of the network or external resources.&lt;a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; All tests on TAP are required to run
in under 15 minutes and exceeding that limit is considered a test
failure. TAP executes tests both before a user submits their changes and
after the user has submitted their change. This paper is only concerned
with the testing that has occurred after a user has submitted a change.
At Google, this is referred to as “Postsubmit Testing". The part of TAP
that does Postsubmit Testing is called TAP Postsubmit.&lt;/p&gt;
&lt;p&gt;TAP has appeared in the literature several times &lt;span
class="citation"
data-cites="Gupta2011 Micco2012 Micco2013 Memon2017 Leong2019 Wang2020 Henderson2023"&gt;(&lt;a
href="#ref-Gupta2011" role="doc-biblioref"&gt;Gupta, Ivey, and Penix
2011&lt;/a&gt;; &lt;a href="#ref-Micco2012" role="doc-biblioref"&gt;Micco 2012&lt;/a&gt;,
&lt;a href="#ref-Micco2013" role="doc-biblioref"&gt;2013&lt;/a&gt;; &lt;a
href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;; &lt;a
href="#ref-Leong2019" role="doc-biblioref"&gt;Leong et al. 2019&lt;/a&gt;; &lt;a
href="#ref-Wang2020" role="doc-biblioref"&gt;Wang et al. 2020&lt;/a&gt;; &lt;a
href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al.
2023&lt;/a&gt;)&lt;/span&gt; and there has been gradual evolution its testing
strategy over the years. However, in general TAP uses a combination of
static and dynamic Test Selection, execution throttling, and
just-in-time scheduling to control testing load. A simplified diagram of
TAP Postsubmit testing is visualized in Figure &lt;a
href="#fig:ci-timeline" data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt;. Tests are
periodically run in Testing Cycles.&lt;a href="#fn3" class="footnote-ref"
id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; During a cycle, Projects
that are eligible to run their test in the cycle are selected. Tests
from those projects are included if some file was modified since that
test’s last execution can influence their behavior via inspection of a
dependence graph at the Build Target and File level granularity &lt;span
class="citation" data-cites="Gupta2011 Micco2012"&gt;(&lt;a
href="#ref-Gupta2011" role="doc-biblioref"&gt;Gupta, Ivey, and Penix
2011&lt;/a&gt;; &lt;a href="#ref-Micco2012" role="doc-biblioref"&gt;Micco
2012&lt;/a&gt;)&lt;/span&gt;. When breakages inevitably occur culprit finding is
conducted using the FACF algorithm to locate the offending change &lt;span
class="citation" data-cites="Henderson2023"&gt;(&lt;a
href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al.
2023&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;FACF operates on a single test target at a time. For simplicity, we
use the term “test” both for a target which executes test code or for a
“build targets” which verifies that a binary can compile. Conceptually,
FACF performs a “Noisy Binary Search” &lt;span class="citation"
data-cites="Rivest1978"&gt;(&lt;a href="#ref-Rivest1978"
role="doc-biblioref"&gt;Rivest, Meyer, and Kleitman 1978&lt;/a&gt;)&lt;/span&gt; (also
called a Rényi-Ulam game &lt;span class="citation"
data-cites="Pelc2002"&gt;(&lt;a href="#ref-Pelc2002" role="doc-biblioref"&gt;Pelc
2002&lt;/a&gt;)&lt;/span&gt;) which FACF models under the Bayesian Search Framework
&lt;span class="citation" data-cites="Ben-Or2008"&gt;(&lt;a
href="#ref-Ben-Or2008" role="doc-biblioref"&gt;Ben-Or and Hassidim
2008&lt;/a&gt;)&lt;/span&gt;. The input to FACF includes the suspect changes
(“suspects”) that may have broken the test and an estimate collected by
an independent system of how likely it is to fail non-deterministically
without the presence of a deterministic bug, it’s “flakiness”. Much like
a normal binary search, it then divides the search space, executes
tests, and updates a probability distribution based on the outcomes.
Eventually, the system will determine that one of the suspects is above
a set probability threshold (.9999) and is the source of the test
failure, or that none of the suspects is at fault and the original
failure was spurious, due to a flake (non-deterministic failure).&lt;/p&gt;
&lt;h2 id="culprit-finding-accuracy"&gt;Culprit Finding Accuracy&lt;/h2&gt;
&lt;p&gt;Now some caveats: The math behind FACF &lt;span class="citation"
data-cites="Henderson2023"&gt;(&lt;a href="#ref-Henderson2023"
role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt; assumes that
individual test executions of the same test at either the same version
or different versions are independent statistical events. That is, a
prior or concurrent execution of a test cannot influence a subsequent
execution. Unfortunately, while this assumption is theoretically sound,
in practice this property does not always hold. External factors outside
a program’s code can also influence a test’s execution behavior. For
example, the time of day, day of year, load on the machine, etc., can
all influence the outcome of certain tests. Thus, while we have
configured FACF to have an accuracy of 99.99%, in practice we do not
observe this level of accuracy. As noted in the Introduction, our
observed accuracy was 99.54% in the last 28 day window as of this
writing. This level of accuracy is consistent with the empirical study
conducted in the 2023 paper &lt;span class="citation"
data-cites="Henderson2023"&gt;(&lt;a href="#ref-Henderson2023"
role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How was accuracy “observed”? What was the ground truth used? By what
measurement technique? At Google, we continuously randomly sample a
subset of culprit finding execution results as they are completed. The
selected sample is then cross checked by performing extensive reruns
that demand a higher level of accuracy and assume a worse flakiness rate
for the test than used for culprit finding. Additionally, more reruns
are scheduled for a later time to control for time-based effects.
Finally, execution ordering effects are controlled for by ensuring that
executions at versions that should fail can fail both before and after
versions that should pass are executed. Full details on our methods for
verification can be found in the FACF paper &lt;span class="citation"
data-cites="Henderson2023"&gt;(&lt;a href="#ref-Henderson2023"
role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt;. Even with
extensive reruns there remains a small probability of error. While our
method of verification is imperfect, it allows for continuously
computing a statistical estimate on the accuracy of the culprit
finder.&lt;/p&gt;
&lt;p&gt;Measurements of FACF accuracy reported in the 2023 paper were
performed per test breakage. That is, culprit finding is performed on a
test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; when it has an observed
failure at some version &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; when
it was previously passing at a prior version &lt;span
class="math inline"&gt;\(\alpha\)&lt;/span&gt;. The range &lt;span
class="math inline"&gt;\((\alpha, \beta]\)&lt;/span&gt; is referred to as the
&lt;em&gt;search-range&lt;/em&gt; and the key &lt;span class="math inline"&gt;\(\{t \times
(\alpha, \beta]\}\)&lt;/span&gt; is the &lt;em&gt;search-key&lt;/em&gt;. For a given
search-key, the culprit finder first filters the search-range to changes
on which &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; has a transitive build
dependence, which we call the &lt;em&gt;suspect-set&lt;/em&gt;. A culprit &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt; is identified somewhere between
&lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\alpha\)&lt;/span&gt; (e.g. &lt;span
class="math inline"&gt;\(\alpha \sqsubset \kappa \sqsubseteq \beta\)&lt;/span&gt;
where &lt;span class="math inline"&gt;\(a
\sqsubset b\)&lt;/span&gt; indicates version &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt; is before version &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;). It is possible that there are
multiple &lt;span class="math inline"&gt;\(\kappa_t\)&lt;/span&gt; for different
&lt;span class="math inline"&gt;\(t\)&lt;/span&gt; w.r.t. the same search range,
which we can call the &lt;em&gt;culprit-set&lt;/em&gt; of the search range. What is
measured is whether or not &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt;
is correct (true or false) for a given &lt;span
class="math inline"&gt;\(t\)&lt;/span&gt; and search range &lt;span
class="math inline"&gt;\(\alpha \sqsubset \beta\)&lt;/span&gt;. Accuracy is the
total correct measurements over the total number of measurements
taken.&lt;/p&gt;
&lt;h2 id="applying-culprit-finding-results-to-change-reversion"&gt;Applying
Culprit Finding Results to Change Reversion&lt;/h2&gt;
&lt;p&gt;When breaking changes get merged into the main development branch
they impede development productivity. At Google we term impediments to
productivity as “friction.” Breaking changes impact three types of
friction tracked at Google: Release Friction, Presubmit Friction and
Development Friction. Release Friction occurs when breaking changes that
merge into the main development branch impede automated releases and
require manual intervention by the primary team’s on-duty engineer.
Presubmit Friction occurs when pre-submission (“presubmit”) testing in
TAP fails due to a broken test in the main development branch.
Development Friction occurs when a developer manually triggers the
execution of a test broken on the main branch during active code
development. All three types of friction can steal time from developers
and reduce productivity.&lt;/p&gt;
&lt;p&gt;To reduce friction, changes that break tests can be automatically
reverted (“rolled back”). However, auto-revert (“auto-rollback”) only
improves developer productivity if the changes reverted actually broke
tests. Let’s consider for a moment what happens when a change is
incorrectly reverted. The developer who authored the change now needs to
go on a wholly different troubleshooting journey to determine why their
change didn’t stay submitted, and they are faced with the same
damning-but-incorrect evidence used to revert their change. The
developer is frustrated and must now debug tests that their team doesn’t
own and which their changes should not affect.&lt;/p&gt;
&lt;p&gt;Some changes are particularly difficult to submit with a buggy
auto-revert system: those that change core libraries. These changes have
the potential to break a large number of tests and impact a large number
of other developers. But, at the same time they are also more likely to
be incorrectly blamed (as TAP tracks what tests are affected by each
change and uses this as an input to culprit finding). Past auto-revert
systems that have been too inaccurate have caused core library authors
to opt their changes out of auto reversion as the productivity cost to
the core library teams has been too great to bear.&lt;/p&gt;
&lt;h2 id="accuracy-of-auto-revert-versus-culprit-finding"&gt;Accuracy of
Auto-Revert versus Culprit Finding&lt;/h2&gt;
&lt;p&gt;A change &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt; that breaks a
test may have broken more than one. If so, then multiple independent
executions of the culprit finding algorithm (one for each broken test)
will blame the version &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt; as
the culprit. As explored above, the algorithms have an error rate. A
change &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt; that breaks &lt;span
class="math inline"&gt;\(n\)&lt;/span&gt; tests will have &lt;span
class="math inline"&gt;\(m \le n\)&lt;/span&gt; culprit finding conclusions that
correctly identify &lt;span class="math inline"&gt;\(\kappa\)&lt;/span&gt; as the
culprit.&lt;/p&gt;
&lt;p&gt;Our concern with auto-revert isn’t the accuracy of culprit finding
per search-key &lt;span class="math inline"&gt;\(\{t \times (\alpha,
\beta]\}\)&lt;/span&gt; but rather per culprit change &lt;span
class="math inline"&gt;\(\kappa\)&lt;/span&gt;. Let &lt;span
class="math inline"&gt;\(K\)&lt;/span&gt; be the set of all changes in the
repository blamed as culprits by the culprit finding system. Let &lt;span
class="math inline"&gt;\(c \subseteq K\)&lt;/span&gt; be the subset of the
culprits that are correct and &lt;span class="math inline"&gt;\(\overline{c}
\subseteq K\)&lt;/span&gt; be the subset of culprits that are incorrect. Then
if every culprit in &lt;span class="math inline"&gt;\(K\)&lt;/span&gt; was
automatically reverted, the accuracy of the auto-revert system would be
&lt;span class="math inline"&gt;\({\left|{c}\right|} /
{\left|{K}\right|}\)&lt;/span&gt;. In the introduction, we noted that the
accuracy &lt;em&gt;per change&lt;/em&gt; was 77.37% which equals &lt;span
class="math inline"&gt;\({\left|{c}\right|} /
{\left|{K}\right|}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Why is the auto-revert accuracy (77.37%) lower than the culprit
finding accuracy (99.54%)? Because there are many more tests than
culprits and because correctly identified culprit changes may have
broken more than one test. The chance of finding that culprit is higher
because there are more chances to find it. Additionally, all of the
culprit finding conclusions that identify one of those changes that
break many tests will be correct. The set of incorrectly identified
culprit &lt;span class="math inline"&gt;\(\overline{c}\)&lt;/span&gt; tends to
contain mostly changes that are blamed by a fewer number of tests than
those in set &lt;span class="math inline"&gt;\(c\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="problem-statement"&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;Identify a method for selecting as many as possible changes from the
set of culprits &lt;span class="math inline"&gt;\(K\)&lt;/span&gt; that can be
safely reverted. A change is safe to revert if it is a true culprit.
Safety can be defined in terms of probability as well: A change is safe
to revert if the chance it is a true culprit is &lt;span
class="math inline"&gt;\(&amp;gt;99\%\)&lt;/span&gt;. The number of changes selected
should be maximized (while maintaining safety) to ensure the methods
performs reversions rather than safely doing nothing at all.&lt;/p&gt;
&lt;h1 id="automatically-reverting-changes"&gt;Automatically Reverting
Changes&lt;/h1&gt;

&lt;p&gt;&lt;span id="fig:baseline" label="fig:baseline"&gt;
&lt;a href="images/icst-2024/fig2.png"&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;&lt;/em&gt;
&lt;img alt="Figure 2" src="images/icst-2024/fig2.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will describe several methods for selecting changes to revert
while maintaining the intended safety property. The first method is a
simple “baseline” method that the more complex methods will be evaluated
against. The other methods use shallow machine learning systems to take
advantage of metadata available at revert time.&lt;/p&gt;
&lt;p&gt;&lt;span id="sec:baseline" label="sec:baseline"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our baseline method is based on a single observation: a change &lt;span
class="math inline"&gt;\(\kappa_0\)&lt;/span&gt; that is identified as the
culprit by culprit finding on many tests is more likely to be a culprit
than a change &lt;span class="math inline"&gt;\(\kappa_1\)&lt;/span&gt; that is only
blamed by a few tests. Therefore, the simplest heuristic approach to
selecting changes to revert is to threshold on a minimum number of tests
identifying the change. BASELINE(N) will refer to this method with
selecting changes with at least N blaming targets. Figure &lt;a
href="#fig:baseline" data-reference-type="ref"
data-reference="fig:baseline"&gt;[fig:baseline]&lt;/a&gt; shows the performance
of this method at various minimum number of targets. The most
conservative, BASELINE(50) can avoid most false positives while
reverting only a &lt;span class="math inline"&gt;\(\sim\)&lt;/span&gt;13% of true
culprits. We choose BASELINE(10) for our final evaluation as this is the
heuristic Google has historically used. Additionally, 10 is the lowest
configuration of BASELINE that generally (although not always) meets the
desired safety criteria.&lt;/p&gt;
&lt;h2 id="subsec:ml-models"&gt;Predicting Auto-Revert Correctness using
Machine Learning&lt;/h2&gt;
&lt;p&gt;Instead of choosing just one suggestive feature – the number of
blaming tests – to decide on a reversion, our proposed method uses
multiple features and shallow machine learning models to improve
performance relative to the baseline method. A selection of easily
obtainable coarse-grained metadata on the changes, tests, and culprit
finding process are used as features to the models. As these features
are mainly simple numerical, categorical, and textual features about the
code, we use simple model architectures to control for over-fitting.
Architectures examined are: Random Forests (RF) and AdaBoost (ADA) (both
provided by scikit-learn&lt;a href="#fn4" class="footnote-ref" id="fnref4"
role="doc-noteref"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; &lt;span class="citation"
data-cites="scikit-learn"&gt;(&lt;a href="#ref-scikit-learn"
role="doc-biblioref"&gt;Pedregosa et al. 2011&lt;/a&gt;)&lt;/span&gt;), and XGBoost
(XGB)&lt;a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; &lt;span class="citation"
data-cites="Chen2016a"&gt;(&lt;a href="#ref-Chen2016a"
role="doc-biblioref"&gt;Chen and Guestrin 2016&lt;/a&gt;)&lt;/span&gt;. These
tree-based model architectures are easy to use, have low computational
costs, and are robust to the feature representation versus alternatives
that require more preprocessing.&lt;/p&gt;
&lt;h2 id="features-used"&gt;Features Used&lt;/h2&gt;
&lt;p&gt;A suspect change is one that has been identified by at least one
culprit-finder in the process of culprit finding a test. We can consider
this the test &lt;em&gt;blaming&lt;/em&gt; the change. For any suspect, we have one
or more blaming tests. Then, we have a few sources of information that
may be valuable for predicting a true breakage: characteristics of the
change itself and characteristics of the blaming tests or culprit
finding results. Table &lt;a href="#table:features-table"
data-reference-type="ref" data-reference="table:features-table"&gt;1&lt;/a&gt;
contains the grouping of the features against their logical feature
category, arrived at using the Pearson correlation between numerical
features and lift analysis for non-numeric features.&lt;/p&gt;
&lt;h2 id="feature-representation"&gt;Feature Representation&lt;/h2&gt;
&lt;h3 id="categorical-features"&gt;Categorical Features&lt;/h3&gt;
&lt;p&gt;Given a set of categories, we can create a fixed length
representation by encoding a choice of category as a one-hot vector.
When we have a variable number of categories per instance, such as
language per test, this trivially becomes a multi-hot representation by
summing the vectors.&lt;/p&gt;
&lt;h3 id="numerical-features"&gt;Numerical Features&lt;/h3&gt;
&lt;p&gt;Singular numerical fields are integrated without preprocessing or
normalization, as we expect decision trees to be robust to
value-scaling. Variable-length numerical fields are turned into
variable-length categorical fields by generating bins representing
quantiles of the training set distribution of that value. Then,
bin-mapped values can be condensed into a fixed-length multi-hot
encoding as above.&lt;/p&gt;
&lt;h3 id="token-set-features"&gt;Token Set Features&lt;/h3&gt;
&lt;p&gt;Token Sets correspond to variable-length categorical features where
the list of categories is not known in advance. A good example of such a
feature is compiler flags which should be dealt with adaptively by the
model. We handle this issue by building a vocabulary per token-set
feature on the training set parameterized by a minimum and maximum
across-instance frequency. The change description, while potentially
better dealt with through doc2vec or a more sophisticated NLP embedding
approach, is treated like a token set and the resulting multi-hot
encoding, having each field normalized by document frequency
(TF-IDF).&lt;/p&gt;
&lt;p&gt; &lt;span id="table:features-table"
label="table:features-table"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class="center"&gt;
&lt;div id="table:features-table"&gt;
&lt;table&gt;
&lt;caption&gt; &lt;span class="smallcaps"&gt;Features for Machine Learning
Model.&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th style="text-align: left;"&gt;Category&lt;/th&gt;
&lt;th style="text-align: left;"&gt;Features&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;BASE&lt;/td&gt;
&lt;td style="text-align: left;"&gt;# of Blaming Tests&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td rowspan="3" style="text-align: left;"&gt;CHANGE_CONTENT&lt;/td&gt;
&lt;td style="text-align: left;"&gt;LOC (changed, added, deleted, total)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;# of Files Changed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td style="text-align: left;"&gt;File Extensions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td rowspan="3" style="text-align: left;"&gt;CHANGE_METADATA&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Reviewer/Approver Count&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td style="text-align: left;"&gt;Issue/Bug Count&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Robot Author&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td rowspan="2" style="text-align: left;"&gt;CHANGE_TOKENS&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Directory Tokens&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Description&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td rowspan="2" style="text-align: left;"&gt;CULPRIT_FINDER&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;span
class="math inline"&gt;\(|\)&lt;/span&gt;Suspect-Set&lt;span
class="math inline"&gt;\(|\)&lt;/span&gt; of Search-Key&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;&lt;span
class="math inline"&gt;\(|\)&lt;/span&gt;Culprit-Set&lt;span
class="math inline"&gt;\(|\)&lt;/span&gt; of Search-Range&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td rowspan="2" style="text-align: left;"&gt;FLAKINESS&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Historical Flaky Identification Count
(Build/Test)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Historical Execution Flake Rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td rowspan="6" style="text-align: left;"&gt;TEST_ATTRIBUTES&lt;/td&gt;
&lt;td style="text-align: left;"&gt;Type of Test (Test vs Build)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Bazel Rule (Ex: “java_test”)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td style="text-align: left;"&gt;Machine Types (Ex: gpu)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Test Language (Ex: java, c++, etc)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td style="text-align: left;"&gt;Compiler Flags (ex: sanitizers)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Average Machine Cost&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h2 id="feature-grouping"&gt;Feature Grouping&lt;/h2&gt;
&lt;h3 id="logical-sub-grouping-for-feature-lift-analysis"&gt;Logical
Sub-Grouping for Feature Lift Analysis&lt;/h3&gt;
&lt;p&gt;In order to avoid crowding out the evaluation and analysis with too
many combinations of features, we’ve grouped them in Table &lt;a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table"&gt;1&lt;/a&gt; into logical groups of
features based off of conceptual categories and to reflect cross-feature
correlations for numerical features, computed via pearsons correlation
coefficient. As our “Token Set" features represent a distinct class of
features both in terms of encoding method and granularity (highly
specific to individual changes), we separate them from the coarse-grain
change features. These logical groups will allow us to clearly convey
the individual signal being provided by each class of feature during our
feature analysis.&lt;/p&gt;
&lt;h3 id="subsec:avail-group-features"&gt;Availability Sub-Grouping for Model
Comparison Analysis&lt;/h3&gt;
&lt;p&gt;Separately, we define the feature sets F = {&lt;em&gt;BASE&lt;/em&gt;,
&lt;em&gt;AHEAD-OF-TIME&lt;/em&gt;, &lt;em&gt;REAL-TIME&lt;/em&gt;, &lt;em&gt;ALL&lt;/em&gt;} to distinguish
between features available at any potential inference time&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;BASE&lt;/em&gt; is just the number of blaming tests. This
corresponds to the BASELINE heuristic method.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;AHEAD-OF-TIME&lt;/em&gt; features are properties of the change
under suspicion, and are therefore available immediately. Features under
CHANGE_CONTENT, CHANGE_METADATA, and CHANGE_TOKENS from Table &lt;a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table"&gt;1&lt;/a&gt; belong here.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;REAL-TIME&lt;/em&gt; features are available only as a result of
culprit finding processes at play continuously during the decision
window, and can become available at different times based on different
sources of generation, often after a change has already been suspected
by a culprit finder – all other features from Table &lt;a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table"&gt;1&lt;/a&gt; fall in this set.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;ALL&lt;/em&gt; features is the union of &lt;em&gt;AHEAD-OF-TIME&lt;/em&gt; and
&lt;em&gt;REAL-TIME&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This distinction is important: the usefulness of automatically
reverting changes is critically dependent on its latency from the point
of discovering that a breakage exists. The longer we take to
automatically revert the change, the more friction experienced by
developers, and the more likely a developer would have had to manually
intervene.&lt;/p&gt;
&lt;h3 id="subsubsec:feature-min"&gt;Feature Minimization&lt;/h3&gt;
&lt;p&gt;Given the demonstrated feature lift, we’re also interested in
minimizing the features needed to achieve similar performance levels
without over-fitting on redundant features. We’ll elaborate on the exact
minimized feature sets in the evaluation section for feature analysis,
where we use representative features from each logical sub-groups that
provide significant lift individually. Thus we have 3 further feature
sets, MIN(f) for &lt;span class="math inline"&gt;\(f \in F\)&lt;/span&gt;.
Representative features are determined over the test set based on
individual feature comparison, left out for brevity in the analysis,
rather than the logical category evaluation presented here. We evaluate
RF(f), ADA(f), XGB(f), RF(MIN(f)), ADA(MIN(f)), and XGB(MIN(f)) for each
feature set &lt;span class="math inline"&gt;\(f \in F\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;h2
id="thresholding-to-use-model-prediction-score-for-selection"&gt;Thresholding
to use Model Prediction Score for Selection&lt;/h2&gt;&lt;/p&gt;
&lt;p&gt;Models produce an output probability score from 0 to 1. To discretize
these scores into actual change selections. We dynamically pick a
threshold using the TEST dataset to select the threshold that minimizes
the bad revert rate while maximizing a positive, non-zero recall.&lt;/p&gt;
&lt;h2 id="hyperparams-for-ml-models"&gt;Hyperparams for ML Models&lt;/h2&gt;
&lt;p&gt;Using scikit-learn, we define RF as the RandomForestClassifier
parameterized with a depth of 16 and ADA as the AdaBoostClassifier with
default parameters, each of which provide discretized predictions with
the above threshold selection procedure. XGBoost is configured with:
objective=‘binary:logistic’, eta=0.05, and max_delta_step = 1.&lt;/p&gt;
&lt;h1 id="sec:evaluation"&gt;Empirical Evaluation&lt;/h1&gt;
&lt;p&gt;We empirically evaluated SafeRevert (the ML based method) against the
BASELINE heuristic method. We evaluate: the overall performance of the
different ML models considered, the importance features used, and
compare the BASELINE method to a selected ML model.&lt;/p&gt;
&lt;h2 id="research-questions"&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;What is the safety and performance of studied methods in the
context of the developer workflow?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the marginal benefit provided by each feature?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Did the chosen method significantly improve performance over the
baseline method while maintaining required safety levels?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="sec:safety"&gt;Measuring Safety&lt;/h2&gt;
&lt;p&gt;As mentioned in section &lt;a href="#intro:auto-revert"
data-reference-type="ref" data-reference="intro:auto-revert"&gt;1.4&lt;/a&gt;, an
incorrectly reverted change causes unacceptably high developer toil.
Safety is the likelihood that a method will produce a false positive
result by incorrectly categorizing a change as being a culprit and
reverting it. This is a referred to as a &lt;em&gt;Bad Revert&lt;/em&gt; while
correctly reverting a change that introduced a bug is a &lt;em&gt;Good
Revert&lt;/em&gt;. The total number of reverts is &lt;em&gt;Bad Reverts&lt;/em&gt; +
&lt;em&gt;Good Reverts&lt;/em&gt;. A change that should have been reverted but
wasn’t is a &lt;em&gt;Missed Good Revert&lt;/em&gt; and a change that was correctly
not reverted is a &lt;em&gt;Avoided Bad Revert&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In terms of classic terminology for evaluating binary
classifiers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Good Revert&lt;/em&gt; = True Positive (TP)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Bad Revert&lt;/em&gt; = False Positive (FP)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Avoided Bad Revert&lt;/em&gt; = True Negative (TN)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Missed Good Revert&lt;/em&gt; = False Negative (FN)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The safety properties we are most interested are (a) the total number
of bad reverts, (b) total number of bad reverts per day, and (c) the bad
revert rate. The bad revert rate (BRR) is the percentage of bad reverts
out of the total number of reverts &lt;span class="math inline"&gt;\(=
\frac{FP}{TP + FP}\)&lt;/span&gt;. This is otherwise known as the False
Discovery Rate (FDR). Note, that &lt;span class="math inline"&gt;\(FDR = 1 -
Precision = 1 - \frac{TP}{TP + FP}\)&lt;/span&gt;. Thus, safety can either be
stated in terms of precision (ex. precision must be above 99%) or in
terms of bad revert rate (ex. bad revert rate must be below 1%).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In this evaluation we will consider a method safe if its bad
revert rate is below 1%, there are fewer than 14 bad reverts in the
final validation set, and there are no more than 5 bad reverts per
day.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Why these numbers?&lt;/em&gt; Our end goal is a production SafeRevert.
Our small team supports a large number of services and users of TAP. We
need to minimize toil for the 2 engineers per week who are “oncall” for
them. The numbers above were selected to be manageable for us in terms
of the overhead required for communicating with our users and
investigating the root cause of bad reverts. While these numbers are
subjective and dependent on our context, they are meaningful to our
team. We expect other teams supporting central CI systems would make
similar choices.&lt;/p&gt;
&lt;h2 id="sec:performance"&gt;Measuring Performance&lt;/h2&gt;
&lt;p&gt;If a method is deemed safe (meets above criteria) then it is an
eligible method to be used to pick changes to revert. To determine
whether one safe reversion method is better than another we look at the
how many &lt;em&gt;Good Reverts&lt;/em&gt; a method is able to achieve out of the
total possible good reverts. This corresponds to the metric known as
recall &lt;span class="math inline"&gt;\(= \frac{TP}{TP + FN}\)&lt;/span&gt;. The
higher a safe method’s recall the better it performs. As with safety, we
prefer methods that are consistent and have low variability in their
reversion recall per day. We then define performance as the number of
breaking changes successfully reverted both in total and per-day.&lt;/p&gt;
&lt;h2 id="sec:dataset"&gt;Evaluation Dataset&lt;/h2&gt;
&lt;p&gt;Our evaluation dataset consists of roughly 3.5 months of data split
between a training, test, and validation set and contains &lt;span
class="math inline"&gt;\(\thicksim\)&lt;/span&gt;25,137 unique changes identified
by the production culprit finder as culprits. Each row has a boolean
indicating whether followup verification confirmed the change was indeed
a culprit. This verification continues to be done in the manner
described in our 2023 paper &lt;span class="citation"
data-cites="Henderson2023"&gt;(&lt;a href="#ref-Henderson2023"
role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For training and evaluation we produce a time-based split: TRAIN
consists of the first 2.5 months, TEST the next &lt;span
class="math inline"&gt;\(\sim\)&lt;/span&gt;2 weeks, and VALIDATION the final
&lt;span class="math inline"&gt;\(\sim\)&lt;/span&gt;2 weeks. Time based splits
avoid cross-contaminating our training with diffuse information about
types of breakages that may be based on time-dependent attributes of the
codebase. It is important that the training data is uncontaminated with
any data from the time period where the evaluation occurs. If it is, the
evaluation will not reflect the performance observed in production.&lt;/p&gt;
&lt;p&gt;All comparative model and feature evaluation was performed against
the TEST set in order to determine our optimal ML model configuration,
OPTIMAL. We then evaluate OPTIMAL and BASELINE against the VALIDATION
set.&lt;/p&gt;

&lt;h1 id="sec:results"&gt;Results&lt;/h1&gt;

&lt;p&gt;&lt;span id="table:feature-ablation" label="table:feature-ablation"&gt;
&lt;span id="table:model-comparison" label="table:model-comparison"&gt;
&lt;span id="table:featureMinimization" label="table:featureMinimization"&gt;
&lt;span id="table:validation-evaluation" label="table:validation-evaluation"&gt;
&lt;a href="images/icst-2024/tables2-5.png"&gt;&lt;em&gt;&lt;strong&gt;Tables 2-5&lt;/strong&gt;&lt;/em&gt;
&lt;img alt="Tables 2-5" src="images/icst-2024/tables2-5.png"&gt;&lt;/a&gt;
&lt;/span&gt;
&lt;/span&gt;
&lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id="fig:model-comparison" label="fig:model-comparison"&gt;
&lt;span id="fig:feature-group-contrib" label="fig:feature-group-contrib"&gt;
&lt;a href="images/icst-2024/fig3-4.png"&gt;&lt;em&gt;&lt;strong&gt;Figures 3-4&lt;/strong&gt;&lt;/em&gt;
&lt;img alt="Figures 3-4" src="images/icst-2024/fig3-4.png"&gt;&lt;/a&gt;
&lt;/span&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;RQ1&lt;/strong&gt;: What is the safety and performance of
studied methods in the context of the developer workflow?&lt;/em&gt; &lt;span
id="result:rq1" label="result:rq1"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class="tcolorbox"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/em&gt;: The XGB(ALL) was best overall,
with a recall of 61.2%, and a bad revert rate of .3%.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="tcolorbox"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: This experiment was conducted on the
TEST data set as the VALIDATION set was reserved for RQ3.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Table &lt;a href="#table:model-comparison" data-reference-type="ref"
data-reference="table:model-comparison"&gt;[table:model-comparison]&lt;/a&gt;
summarizes the critical metrics for safety (bad revert rate) and
performance (recall) for the three model types considered: RandomForest
[RF], AdaBoost [ADA], and XGBoost [XGB]. Figure &lt;a
href="#fig:model-comparison" data-reference-type="ref"
data-reference="fig:model-comparison"&gt;[fig:model-comparison]&lt;/a&gt; shows
the Receiver Operator Curves (ROC) for all 3 model types and their
associated area under the curve (AUC) values. The ROC curve better
summarizes model performance over a wider range of target objectives
than the table which is focused on the safest configuration (minimizing
Bad Revert Rate). For instance if a different application had a higher
tolerance for bad reversion/false positives the ROC curves show that you
could achieve a 90% true positive rate with a 20% false positive
rate.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Safety&lt;/strong&gt;: In general, XGB and ADA both outperformed
the RF in terms of safety. Under none of the test configurations did the
RF have a safe configuration. XGB and ADA are safe (under our criteria,
see Section &lt;a href="#sec:safety" data-reference-type="ref"
data-reference="sec:safety"&gt;4.2&lt;/a&gt;) using all the features or the
REAL-TIME subset. The safest configuration was XGB(ALL) which had only 5
bad reverts and a bad revert rate of 0.003. XGB(REAL-TIME) and ADA(ALL)
has the same or smaller number of bad reverts but a worse rate due to
their lower recall.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Performance&lt;/strong&gt;: The highest performing configuration
was XGB(ALL) which had 1980 Good Reverts in the Test set and a recall of
63.2%. The recall rate of XGB(REAL-TIME) was 52.2% indicating that most
of the model performance is gained from the REAL-TIME features, not from
the AHEAD-OF-TIME features which performed much worse while being
unsafe. ADA(ALL) was safe but its recall rate was only 42.2% which lower
than both safe XGB configurations.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;
&lt;p&gt; &lt;em&gt;&lt;strong&gt;RQ2&lt;/strong&gt;: What is the marginal
benefit provided by each feature group&lt;/em&gt;? &lt;span id="result:rq2"
label="result:rq2"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class="tcolorbox"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/em&gt;: Historical flakiness data was the
most valuable feature set, boosting model recall by 17%, while change
metadata was the least important.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="tcolorbox"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: This experiment was conducted on the
TEST data set as the VALIDATION set was reserved for RQ3.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To determine marginal benefit we perform an ablation study to look at
the marginal benefit provided by each feature (as grouped in Table &lt;a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table"&gt;1&lt;/a&gt;). The ablation study was
performed using the XGBoost model which was chosen over AdaBoost due to
its higher recall as seen in Table &lt;a href="#table:model-comparison"
data-reference-type="ref"
data-reference="table:model-comparison"&gt;[table:model-comparison]&lt;/a&gt;.
Three marginal benefit experiments were performed:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Measure the performance of the model trained against
&lt;strong&gt;only&lt;/strong&gt; the features in a single feature set. We visualize
the results in Figure &lt;a href="#fig:feature-group-contrib"
data-reference-type="ref"
data-reference="fig:feature-group-contrib"&gt;[fig:feature-group-contrib]&lt;/a&gt;
as an ROC curve. Table &lt;a href="#table:feature-ablation"
data-reference-type="ref"
data-reference="table:feature-ablation"&gt;[table:feature-ablation]&lt;/a&gt;
contains the critical safety and performance metrics of Bad Reverts
(safety) and Recall (performance) as well as the AUC which summarizes
overall model predictive performance – generally a model with a higher
AUC will be more performant than one with a lower AUC.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Measure the &lt;strong&gt;Positive(+) &lt;em&gt;Lift&lt;/em&gt;&lt;/strong&gt; provided
by adding a single feature group to our BASE feature (the number of
blaming tests). For instance, we would add the features in
CHANGE_CONTENT to create a XGB(CHANGE_CONTENT + BASE) model. The
&lt;strong&gt;+Lift&lt;/strong&gt; for Recall is computed as
Recall[XGB(CHANGE_CONTENT + BASE)] - Recall[XGB(BASE)] and similar for
AUC of the ROC curve. The results of this analysis are shown in Table &lt;a
href="#table:feature-ablation" data-reference-type="ref"
data-reference="table:feature-ablation"&gt;[table:feature-ablation]&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Measure the &lt;strong&gt;Negative(–) &lt;em&gt;Lift&lt;/em&gt;&lt;/strong&gt; provided
by removing a single feature group from out ALL feature set. For
instance we would remove the features in CHANGE_CONTENT to create a
XGB(ALL - CHANGE_CONTENT) model. The &lt;strong&gt;–Lift&lt;/strong&gt; for Recall
is computed as Recall[XGB(ALL)] - Recall[XGB(ALL - CHANGE_CONTENT)] and
similar for AUC of the ROC curve. The results of this analysis are shown
in Table &lt;a href="#table:feature-ablation" data-reference-type="ref"
data-reference="table:feature-ablation"&gt;[table:feature-ablation]&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As discussed in section &lt;a href="#subsubsec:feature-min"
data-reference-type="ref"
data-reference="subsubsec:feature-min"&gt;3.4.3&lt;/a&gt;, we attempted to
minimize the feature subsets while providing comparable performance to
the full feature sets. Table &lt;a href="#table:featureMinimization"
data-reference-type="ref"
data-reference="table:featureMinimization"&gt;[table:featureMinimization]&lt;/a&gt;
contains a breakdown of the performance for these minimal subsets.&lt;/p&gt;
&lt;p&gt; &lt;em&gt;&lt;strong&gt;RQ3&lt;/strong&gt;:Did the chosen method
improve performance over the baseline method while maintaining required
safety levels?&lt;/em&gt; &lt;span id="result:rq3" label="result:rq3"&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class="tcolorbox"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/em&gt;: XGB(ALL) improved overall recall
while meeting our safety requirements with a Bad Revert Rate under 1%
and an average of 0.6 Bad Reverts per Day. BASELINE(10) had a recall of
26.3% while XGB(ALL) had a recall of 55.7% – an improvement of &lt;span
class="math inline"&gt;\(\sim 2.1\times\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="tcolorbox"&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/em&gt;: We used the reserved VALIDATION data
set for this research question.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Table &lt;a href="#table:validation-evaluation"
data-reference-type="ref"
data-reference="table:validation-evaluation"&gt;[table:validation-evaluation]&lt;/a&gt;
shows per day metrics comparing XGB(ALL) against the heuristic
BASELINE(10) method. XGB(ALL) was selected as our “OPTIMAL” model to
compare against the BASELINE method given its performance in the TEST
set as evaluated in RQ1. As a reminder the BASELINE method thresholds
the number of blaming tests to a fixed number. BASELINE(10)’s average
bad reverts per day is 0.86 and its total Bad Revert Rate (BRR) is 1.5%.
XGB(ALL) achieves the overall safety limit with a BRR of 0.5% and has an
average bad reverts per day of 0.6 which meets our safety threshold.&lt;/p&gt;
&lt;p&gt;XGB(ALL)’s bad revert rate outperforms BASELINE(10), with a recall
 2.1 times higher than BASELINE(10). XGB(ALL) made 1,823 total good
reverts in the validation set while BASELINE(10) performed 860 good
reverts. Based on the data in Table &lt;a
href="#table:validation-evaluation" data-reference-type="ref"
data-reference="table:validation-evaluation"&gt;[table:validation-evaluation]&lt;/a&gt;
XGB(ALL) is both safer and more performant than BASELINE(10) on the
validation data set.&lt;/p&gt;
&lt;h2 id="discussion"&gt;Discussion&lt;/h2&gt;
&lt;p&gt;The proposed method, SafeRevert, is generic and can be used with any
culprit finding algorithm. By grouping individual features from
different sources of data into logical feature sets (Table &lt;a
href="#table:features-table" data-reference-type="ref"
data-reference="table:features-table"&gt;1&lt;/a&gt;), performing a detailed
feature ablation study (Fig &lt;a href="#fig:feature-group-contrib"
data-reference-type="ref"
data-reference="fig:feature-group-contrib"&gt;[fig:feature-group-contrib]&lt;/a&gt;),
and running the model on a minimal set of features (Table &lt;a
href="#table:featureMinimization" data-reference-type="ref"
data-reference="table:featureMinimization"&gt;[table:featureMinimization]&lt;/a&gt;),
we hope to provide a template via which teams in other contexts build on
when adopting the approach we outline. In particular, while some
features used may be Google specific, our feature ablation study can be
replicated on different features in other software development
organizations. While we don’t expect a team implementing SafeRevert to
achieve the exact Recall and Bad Revert Rates we report we do expect
this method to out perform the BASELINE method once an appropriate set
of features is identified.&lt;/p&gt;
&lt;h2 id="threats-to-validity"&gt;Threats to Validity&lt;/h2&gt;
&lt;p&gt;Our dataset may misrepresent information available at inference time
in the forthcoming service as it may include information not available
to us at our decision time. This is due to using offline data in the
dataset as the production system based on this paper is currently under
construction. We adjust this threat by evaluating performance restricted
to features available independent of any culprit finding event and
restricting our real-time data to a time bound relative to change
submission time.&lt;/p&gt;
&lt;p&gt;The data presented in this final manuscript differs slightly than the
reviewed manuscript. At the time of review approximated 15% of the
dataset was lacking verification results (collected using the method
described by Henderson &lt;span class="citation"
data-cites="Henderson2023"&gt;(&lt;a href="#ref-Henderson2023"
role="doc-biblioref"&gt;Henderson et al. 2023&lt;/a&gt;)&lt;/span&gt;). This was
disclosed in this section to the reviewers and we made two conservative
assumptions: 1) any culprit change lacking verification results was
considered a false positive and 2) we assumed the BASELINE method was
correct for those changes (inflating the BASELINE methods performance
versus the studied ML methods for SafeRevert). Since the peer review was
completed, a bug in the verification system was identified and fixed.
The bug caused a proportion of incorrect culprit changes to “get stuck”
in a queue waiting for an additional test execution due to a typo in a
comparison (using &lt;code&gt;&amp;gt;&lt;/code&gt; instead of &lt;code&gt;&amp;gt;=&lt;/code&gt;). Once
the bug was fixed, the research team was able to rerun the study with
the additional label data.&lt;/p&gt;
&lt;p&gt;Post-rerun we observed: 1) the BASELINE method performed worse and 2)
the studied methods were robust to the change in labeling data. In the
reviewed manuscript, XGB(ALL) had the following results in RQ3: 13 Total
Bad Reverts, 1817 Good Reverts, 0.7% Bad Revert Rate, and 55.8% Recall;
BASELINE(5) was compared against and had 8 Total Bad Reverts, 1286 Good
Reverts, 0.6% Bad Revert Rate, and 39.4% Recall. Compare against Table
&lt;a href="#table:validation-evaluation" data-reference-type="ref"
data-reference="table:validation-evaluation"&gt;[table:validation-evaluation]&lt;/a&gt;
and Figure &lt;a href="#fig:baseline" data-reference-type="ref"
data-reference="fig:baseline"&gt;[fig:baseline]&lt;/a&gt;. We switched to using
BASELINE(10) for this final manuscript as it is the production method
currently used at Google.&lt;/p&gt;
&lt;h1 id="related-work"&gt;Related Work&lt;/h1&gt;
&lt;p&gt;In this paper, we are presenting what we believe to be a novel
problem to the wider software engineering community: how to safely
choose changes to revert with incomplete but suggestive evidence. This
problem relies on identifying these problematic changes. In our case we
identify the problematic changes to revert via automated culprit finding
&lt;span class="citation"
data-cites="Couder2008 Ziftci2013a Ziftci2017 Saha2017 Najafi2019a Beheshtian2022 keenan2019 An2021 Ocariza2022 Henderson2023"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;; &lt;a
href="#ref-Ziftci2013a" role="doc-biblioref"&gt;Ziftci and Ramavajjala
2013&lt;/a&gt;; &lt;a href="#ref-Ziftci2017" role="doc-biblioref"&gt;Ziftci and
Reardon 2017&lt;/a&gt;; &lt;a href="#ref-Saha2017" role="doc-biblioref"&gt;Saha and
Gligoric 2017&lt;/a&gt;; &lt;a href="#ref-Najafi2019a"
role="doc-biblioref"&gt;Najafi, Rigby, and Shang 2019&lt;/a&gt;; &lt;a
href="#ref-Beheshtian2022" role="doc-biblioref"&gt;Beheshtian, Bavand, and
Rigby 2022&lt;/a&gt;; &lt;a href="#ref-keenan2019" role="doc-biblioref"&gt;Keenan
2019&lt;/a&gt;; &lt;a href="#ref-An2021" role="doc-biblioref"&gt;An and Yoo
2021&lt;/a&gt;; &lt;a href="#ref-Ocariza2022" role="doc-biblioref"&gt;Ocariza
2022&lt;/a&gt;; &lt;a href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et
al. 2023&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Culprit finding’s development has often occurred outside of the
academic literature. The first reference to the process of using binary
search to identify a bug was in Yourdon’s 1975 book on Program Design:
Page 286, Figure 8.3 titled “A binary search for bugs” &lt;span
class="citation" data-cites="Yourdon1975"&gt;(&lt;a href="#ref-Yourdon1975"
role="doc-biblioref"&gt;Yourdon 1975&lt;/a&gt;)&lt;/span&gt;. The process is explained
in detail in the &lt;code&gt;BUG-HUNTING&lt;/code&gt; file in the Linux source tree
in 1996 by Larry McVoy &lt;span class="citation"
data-cites="McVoy1996 Cox2023"&gt;(&lt;a href="#ref-McVoy1996"
role="doc-biblioref"&gt;McVoy 1996&lt;/a&gt;; &lt;a href="#ref-Cox2023"
role="doc-biblioref"&gt;Cox, Ness, and McVoy 2023&lt;/a&gt;)&lt;/span&gt;. By 1997,
Brian Ness had created a binary search based system for use at Cray with
their “Unicos Source Manager” version control system &lt;span
class="citation" data-cites="Ness1997 Cox2023"&gt;(&lt;a href="#ref-Ness1997"
role="doc-biblioref"&gt;Ness and Ngo 1997&lt;/a&gt;; &lt;a href="#ref-Cox2023"
role="doc-biblioref"&gt;Cox, Ness, and McVoy 2023&lt;/a&gt;)&lt;/span&gt;. Previously
in &lt;span class="citation" data-cites="Henderson2023"&gt;(&lt;a
href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al.
2023&lt;/a&gt;)&lt;/span&gt; we had credited Linus Torvalds for the invention based
on lack of findable antecedent for his work on the
&lt;code&gt;git bisect&lt;/code&gt; command &lt;span class="citation"
data-cites="Couder2008"&gt;(&lt;a href="#ref-Couder2008"
role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;. We regret the error and
recognize the above individuals for their important contributions.&lt;/p&gt;
&lt;p&gt;Other methods for identifying buggy code or breaking changes have
been studied (some widely) in the literature. Fault Localization looks
to identify the buggy code that is causing either operational failure or
test failures &lt;span class="citation"
data-cites="Agarwal2014 Wong2016"&gt;(&lt;a href="#ref-Agarwal2014"
role="doc-biblioref"&gt;Agarwal and Agrawal 2014&lt;/a&gt;; &lt;a
href="#ref-Wong2016" role="doc-biblioref"&gt;Wong et al. 2016&lt;/a&gt;)&lt;/span&gt;.
Methods for fault localization include: delta debugging &lt;span
class="citation" data-cites="Zeller1999"&gt;(&lt;a href="#ref-Zeller1999"
role="doc-biblioref"&gt;Zeller 1999&lt;/a&gt;)&lt;/span&gt;, statistical coverage based
fault localization &lt;span class="citation"
data-cites="Jones2002 Jones2005 Lucia2014 Henderson2018 Henderson2019 Kucuk2021"&gt;(&lt;a
href="#ref-Jones2002" role="doc-biblioref"&gt;J. a. Jones, Harrold, and
Stasko 2002&lt;/a&gt;; &lt;a href="#ref-Jones2005" role="doc-biblioref"&gt;J. A.
Jones and Harrold 2005&lt;/a&gt;; &lt;a href="#ref-Lucia2014"
role="doc-biblioref"&gt;Lucia et al. 2014&lt;/a&gt;; &lt;a href="#ref-Henderson2018"
role="doc-biblioref"&gt;Henderson and Podgurski 2018&lt;/a&gt;; &lt;a
href="#ref-Henderson2019" role="doc-biblioref"&gt;Henderson, Podgurski, and
Kucuk 2019&lt;/a&gt;; &lt;a href="#ref-Kucuk2021" role="doc-biblioref"&gt;Kucuk,
Henderson, and Podgurski 2021&lt;/a&gt;)&lt;/span&gt;, information retrieval (which
may include historical information) &lt;span class="citation"
data-cites="Zhou2012 Youm2015 Ciborowska2022"&gt;(&lt;a href="#ref-Zhou2012"
role="doc-biblioref"&gt;J. Zhou, Zhang, and Lo 2012&lt;/a&gt;; &lt;a
href="#ref-Youm2015" role="doc-biblioref"&gt;Youm et al. 2015&lt;/a&gt;; &lt;a
href="#ref-Ciborowska2022" role="doc-biblioref"&gt;Ciborowska and Damevski
2022&lt;/a&gt;)&lt;/span&gt;, and program slicing &lt;span class="citation"
data-cites="Podgurski1990 Horwitz1992 Ren2004"&gt;(&lt;a
href="#ref-Podgurski1990" role="doc-biblioref"&gt;Podgurski and Clarke
1990&lt;/a&gt;; &lt;a href="#ref-Horwitz1992" role="doc-biblioref"&gt;Horwitz and
Reps 1992&lt;/a&gt;; &lt;a href="#ref-Ren2004" role="doc-biblioref"&gt;Ren et al.
2004&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Bug Prediction attempts to predict if a change, method, class, or
file is likely to contain a bug &lt;span class="citation"
data-cites="Lewis2013 Punitha2013 Osman2017"&gt;(&lt;a href="#ref-Lewis2013"
role="doc-biblioref"&gt;Lewis et al. 2013&lt;/a&gt;; &lt;a href="#ref-Punitha2013"
role="doc-biblioref"&gt;Punitha and Chitra 2013&lt;/a&gt;; &lt;a
href="#ref-Osman2017" role="doc-biblioref"&gt;Osman et al.
2017&lt;/a&gt;)&lt;/span&gt;. This idea has conceptual similarities to the work we
are doing in this paper where we are using similar metrics to predict
whether or not a change which has been implicated by culprit finding is
in fact the change that introduced the bug. Our methods could be
potentially improved by incorporating the additional features used in
the bug prediction work such as the change complexity, code complexity,
and object-oriented complexity metrics.&lt;/p&gt;
&lt;p&gt;Test Case Selection &lt;span class="citation"
data-cites="Leon2003 Engstrom2010 Zhou2010 Mondal2015 Musa2015 pan2022"&gt;(&lt;a
href="#ref-Leon2003" role="doc-biblioref"&gt;Leon and Podgurski 2003&lt;/a&gt;;
&lt;a href="#ref-Engstrom2010" role="doc-biblioref"&gt;Engström, Runeson, and
Skoglund 2010&lt;/a&gt;; &lt;a href="#ref-Zhou2010" role="doc-biblioref"&gt;Z. Q.
Zhou 2010&lt;/a&gt;; &lt;a href="#ref-Mondal2015" role="doc-biblioref"&gt;Mondal,
Hemmati, and Durocher 2015&lt;/a&gt;; &lt;a href="#ref-Musa2015"
role="doc-biblioref"&gt;Musa et al. 2015&lt;/a&gt;; &lt;a href="#ref-pan2022"
role="doc-biblioref"&gt;Pan et al. 2022&lt;/a&gt;)&lt;/span&gt; and Test Case
Prioritization &lt;span class="citation"
data-cites="Singh2012 DeS.CamposJunior2017 DeCastro-Cabrera2020 pan2022"&gt;(&lt;a
href="#ref-Singh2012" role="doc-biblioref"&gt;Singh et al. 2012&lt;/a&gt;; &lt;a
href="#ref-DeS.CamposJunior2017" role="doc-biblioref"&gt;de S. Campos
Junior et al. 2017&lt;/a&gt;; &lt;a href="#ref-DeCastro-Cabrera2020"
role="doc-biblioref"&gt;De Castro-Cabrera, García-Dominguez, and
Medina-Bulo 2020&lt;/a&gt;; &lt;a href="#ref-pan2022" role="doc-biblioref"&gt;Pan et
al. 2022&lt;/a&gt;)&lt;/span&gt; are related problems to the change reversion
problem we study. Instead of predicting whether or not a change caused a
known test failure in Test Case Selection/Prioritization often the
change is used to predict whether a given test will fail before it is
run. There is a large body work in that uses dynamic information from
past test executions (such as code coverage) to inform the selection
process. We believe this hints that such information could be highly
informative for the change reversion problem as well.&lt;/p&gt;
&lt;p&gt;Finally, there are a family of methods for finding bug inducing
commits for the purpose of supporting studies that data mine software
repositories &lt;span class="citation"
data-cites="Sliwerski2005 Rodriguez-Perez2018 Borg2019 Wen2019 An2023"&gt;(&lt;a
href="#ref-Sliwerski2005" role="doc-biblioref"&gt;Śliwerski, Zimmermann,
and Zeller 2005&lt;/a&gt;; &lt;a href="#ref-Rodriguez-Perez2018"
role="doc-biblioref"&gt;Rodríguez-Pérez, Robles, and González-Barahona
2018&lt;/a&gt;; &lt;a href="#ref-Borg2019" role="doc-biblioref"&gt;Borg et al.
2019&lt;/a&gt;; &lt;a href="#ref-Wen2019" role="doc-biblioref"&gt;Wen et al.
2019&lt;/a&gt;; &lt;a href="#ref-An2023" role="doc-biblioref"&gt;An et al.
2023&lt;/a&gt;)&lt;/span&gt;. These methods typically used to conduct a historical
analysis of a repository rather than as an online detection as in
culprit finding.&lt;/p&gt;
&lt;h1 id="sec:conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;We presented: SafeRevert a method for improving systems that
automatically revert changes that break tests. SafeRevert was developed
as a way to improve the number bad changes automatically reverted while
maintaining safety (rarely reverting good changes). To evaluate
SafeRevert, we performed an empirical evaluation comparing the
performance of SafeRevert against the baseline method that is currently
utilized in production which utilizes a simple heuristic to determine if
a change is safe to revert: the number of tests which “blame” the
culprit change. When evaluating RQ3 in Section &lt;a href="#sec:results"
data-reference-type="ref" data-reference="sec:results"&gt;5&lt;/a&gt;, it was
observed that the XGB(ALL) configuration of SafeRevert doubled the
number changes reverted while reducing the number of bad reverts
performed (see Table &lt;a href="#table:validation-evaluation"
data-reference-type="ref"
data-reference="table:validation-evaluation"&gt;[table:validation-evaluation]&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;While it is unlikely that a replication study in a different
development environment would reproduce our exact results we do expect
based on the robust difference observed between SafeRevert and BASELINE
that SafeRevert (or similar ML based method) will be able to improve the
number of changes eligible for automatic reversion. We hope that by
introducing this problem to the larger software engineering community
that new and innovative approaches to solving it will be developed.&lt;/p&gt;

&lt;h1 id="sec:References"&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography"&gt;
&lt;ol&gt;
&lt;li&gt;&lt;div id="ref-Agarwal2014" class="csl-entry" role="doc-biblioentry"&gt;
Agarwal, Pragya, and Arun Prakash Agrawal. 2014.
&lt;span&gt;“Fault-Localization &lt;span&gt;Techniques&lt;/span&gt; for &lt;span&gt;Software
Systems&lt;/span&gt;: &lt;span&gt;A Literature Review&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;SIGSOFT
Softw. Eng. Notes&lt;/em&gt; 39 (5): 1–8. &lt;a
href="https://doi.org/10.1145/2659118.2659125"&gt;https://doi.org/10.1145/2659118.2659125&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-An2023" class="csl-entry" role="doc-biblioentry"&gt;
An, Gabin, Jingun Hong, Naryeong Kim, and Shin Yoo. 2023. &lt;span&gt;“Fonte:
&lt;span&gt;Finding Bug Inducing Commits&lt;/span&gt; from
&lt;span&gt;Failures&lt;/span&gt;.”&lt;/span&gt; &lt;span&gt;arXiv&lt;/span&gt;. &lt;a
href="http://arxiv.org/abs/2212.06376"&gt;http://arxiv.org/abs/2212.06376&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-An2021" class="csl-entry" role="doc-biblioentry"&gt;
An, Gabin, and Shin Yoo. 2021. &lt;span&gt;“Reducing the Search Space of Bug
Inducing Commits Using Failure Coverage.”&lt;/span&gt; In &lt;em&gt;Proceedings of
the 29th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on &lt;span&gt;European Software
Engineering Conference&lt;/span&gt; and &lt;span&gt;Symposium&lt;/span&gt; on the
&lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;,
1459–62. &lt;span&gt;Athens Greece&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3468264.3473129"&gt;https://doi.org/10.1145/3468264.3473129&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ananthanarayanan2019" class="csl-entry"
role="doc-biblioentry"&gt;
Ananthanarayanan, Sundaram, Masoud Saeida Ardekani, Denis Haenikel,
Balaji Varadarajan, Simon Soriano, Dhaval Patel, and Ali Reza
Adl-Tabatabai. 2019. &lt;span&gt;“Keeping Master Green at Scale.”&lt;/span&gt;
&lt;em&gt;Proceedings of the 14th EuroSys Conference 2019&lt;/em&gt;. &lt;a
href="https://doi.org/10.1145/3302424.3303970"&gt;https://doi.org/10.1145/3302424.3303970&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Beheshtian2022" class="csl-entry" role="doc-biblioentry"&gt;
Beheshtian, Mohammad Javad, Amir Hossein Bavand, and Peter C. Rigby.
2022. &lt;span&gt;“Software &lt;span&gt;Batch Testing&lt;/span&gt; to &lt;span&gt;Save Build
Test Resources&lt;/span&gt; and to &lt;span&gt;Reduce Feedback Time&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt; 48 (8): 2784–2801. &lt;a
href="https://doi.org/10.1109/TSE.2021.3070269"&gt;https://doi.org/10.1109/TSE.2021.3070269&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ben-Or2008" class="csl-entry" role="doc-biblioentry"&gt;
Ben-Or, Michael, and Avinatan Hassidim. 2008. &lt;span&gt;“The &lt;span&gt;Bayesian
Learner&lt;/span&gt; Is &lt;span&gt;Optimal&lt;/span&gt; for &lt;span&gt;Noisy Binary
Search&lt;/span&gt; (and &lt;span&gt;Pretty Good&lt;/span&gt; for &lt;span&gt;Quantum&lt;/span&gt; as
&lt;span&gt;Well&lt;/span&gt;).”&lt;/span&gt; In &lt;em&gt;2008 49th &lt;span&gt;Annual IEEE
Symposium&lt;/span&gt; on &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Computer
Science&lt;/span&gt;&lt;/em&gt;, 221–30. &lt;span&gt;Philadelphia, PA, USA&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/FOCS.2008.58"&gt;https://doi.org/10.1109/FOCS.2008.58&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Bland2012" class="csl-entry" role="doc-biblioentry"&gt;
Bland, Mike. 2012. &lt;span&gt;“The &lt;span&gt;Chris&lt;/span&gt;/&lt;span&gt;Jay Continuous
Build&lt;/span&gt;.”&lt;/span&gt; Personal {{Website}}. &lt;em&gt;Mike Bland’s Blog&lt;/em&gt;.
&lt;a href="https://mike-bland.com/2012/06/21/chris-jay-continuous-
                  build.html"&gt;https://mike-bland.com/2012/06/21/chris-jay-continuous-
build.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Borg2019" class="csl-entry" role="doc-biblioentry"&gt;
Borg, Markus, Oscar Svensson, Kristian Berg, and Daniel Hansson. 2019.
&lt;span&gt;“&lt;span&gt;SZZ&lt;/span&gt; Unleashed: An Open Implementation of the
&lt;span&gt;SZZ&lt;/span&gt; Algorithm - Featuring Example Usage in a Study of
Just-in-Time Bug Prediction for the &lt;span&gt;Jenkins&lt;/span&gt;
Project.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 3rd &lt;span&gt;ACM SIGSOFT
International Workshop&lt;/span&gt; on &lt;span&gt;Machine Learning
Techniques&lt;/span&gt; for &lt;span&gt;Software Quality Evaluation&lt;/span&gt; -
&lt;span&gt;MaLTeSQuE&lt;/span&gt; 2019&lt;/em&gt;, 7–12. &lt;span&gt;Tallinn, Estonia&lt;/span&gt;:
&lt;span&gt;ACM Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3340482.3342742"&gt;https://doi.org/10.1145/3340482.3342742&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Chen2016a" class="csl-entry" role="doc-biblioentry"&gt;
Chen, Tianqi, and Carlos Guestrin. 2016. &lt;span&gt;“&lt;span&gt;XGBoost&lt;/span&gt;:
&lt;span&gt;A Scalable Tree Boosting System&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings
of the 22nd &lt;span&gt;ACM SIGKDD International Conference&lt;/span&gt; on
&lt;span&gt;Knowledge Discovery&lt;/span&gt; and &lt;span&gt;Data Mining&lt;/span&gt;&lt;/em&gt;,
785–94. &lt;span&gt;San Francisco California USA&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/2939672.2939785"&gt;https://doi.org/10.1145/2939672.2939785&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ciborowska2022" class="csl-entry" role="doc-biblioentry"&gt;
Ciborowska, Agnieszka, and Kostadin Damevski. 2022. &lt;span&gt;“Fast
Changeset-Based Bug Localization with &lt;span&gt;BERT&lt;/span&gt;.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 44th &lt;span&gt;International Conference&lt;/span&gt; on
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 946–57. &lt;span&gt;Pittsburgh
Pennsylvania&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3510003.3510042"&gt;https://doi.org/10.1145/3510003.3510042&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Couder2008" class="csl-entry" role="doc-biblioentry"&gt;
Couder, Christian. 2008. &lt;span&gt;“Fighting Regressions with Git
Bisect.”&lt;/span&gt; &lt;em&gt;The Linux Kernel Archives&lt;/em&gt; 4 (5). &lt;a
href="https://www. kernel. org/pub/software/scm/git/doc s/git-
                  bisect-lk2009.html"&gt;https://www. kernel.
org/pub/software/scm/git/doc s/git- bisect-lk2009.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Cox2023" class="csl-entry" role="doc-biblioentry"&gt;
Cox, Russ, Brian Ness, and Larry McVoy. 2023. &lt;span&gt;“Comp.lang.compilers
"Binary Search Debugging of Compilers".”&lt;/span&gt; &lt;a
href="https://groups.google.com/g/comp.compilers/c/vGh4s3HBQ-s/m/
                  Chvpu7vTAgAJ"&gt;https://groups.google.com/g/comp.compilers/c/vGh4s3HBQ-s/m/
Chvpu7vTAgAJ&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-DeCastro-Cabrera2020" class="csl-entry"
role="doc-biblioentry"&gt;
De Castro-Cabrera, M. Del Carmen, Antonio García-Dominguez, and
Inmaculada Medina-Bulo. 2020. &lt;span&gt;“Trends in Prioritization of Test
Cases: 2017-2019.”&lt;/span&gt; &lt;em&gt;Proceedings of the ACM Symposium on
Applied Computing&lt;/em&gt;, 2005–11. &lt;a
href="https://doi.org/10.1145/3341105.3374036"&gt;https://doi.org/10.1145/3341105.3374036&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-DeS.CamposJunior2017" class="csl-entry"
role="doc-biblioentry"&gt;
de S. Campos Junior, Heleno, Marco Antônio P Araújo, José Maria N David,
Regina Braga, Fernanda Campos, and Victor Ströele. 2017. &lt;span&gt;“Test
&lt;span&gt;Case Prioritization&lt;/span&gt;: &lt;span&gt;A Systematic Review&lt;/span&gt; and
&lt;span&gt;Mapping&lt;/span&gt; of the &lt;span&gt;Literature&lt;/span&gt;.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 31st &lt;span&gt;Brazilian Symposium&lt;/span&gt; on
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 34–43. &lt;span&gt;New York, NY,
USA&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3131151.3131170"&gt;https://doi.org/10.1145/3131151.3131170&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Engstrom2010" class="csl-entry" role="doc-biblioentry"&gt;
Engström, Emelie, Per Runeson, and Mats Skoglund. 2010. &lt;span&gt;“A
Systematic Review on Regression Test Selection Techniques.”&lt;/span&gt;
&lt;em&gt;Information and Software Technology&lt;/em&gt; 52 (1): 14–30. &lt;a
href="https://doi.org/10.1016/j.infsof.2009.07.001"&gt;https://doi.org/10.1016/j.infsof.2009.07.001&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Fowler2006" class="csl-entry" role="doc-biblioentry"&gt;
Fowler, Martin. 2006. &lt;span&gt;“Continuous
&lt;span&gt;Integration&lt;/span&gt;.”&lt;/span&gt; &lt;a
href="https://martinfowler.com/articles/
                  continuousIntegration.html"&gt;https://martinfowler.com/articles/
continuousIntegration.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Gupta2011" class="csl-entry" role="doc-biblioentry"&gt;
Gupta, Pooja, Mark Ivey, and John Penix. 2011. &lt;span&gt;“Testing at the
Speed and Scale of &lt;span&gt;Google&lt;/span&gt;.”&lt;/span&gt; &lt;a
href="http://google-engtools.blogspot.com/2011/06/testing-at-
                  speed-and-scale-of-google.html"&gt;http://google-engtools.blogspot.com/2011/06/testing-at-
speed-and-scale-of-google.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Henderson2023" class="csl-entry" role="doc-biblioentry"&gt;
Henderson, Tim A. D., Bobby Dorward, Eric Nickell, Collin Johnston, and
Avi Kondareddy. 2023. &lt;span&gt;“Flake &lt;span&gt;Aware Culprit
Finding&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2023 &lt;span&gt;IEEE Conference&lt;/span&gt; on
&lt;span&gt;Software Testing&lt;/span&gt;, &lt;span&gt;Verification&lt;/span&gt; and
&lt;span&gt;Validation&lt;/span&gt; (&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;. &lt;span&gt;IEEE&lt;/span&gt;. 
&lt;a href="https://hackthology.com/flake-aware-culprit-finding.html"&gt;https://hackthology.com/flake-aware-culprit-finding.html&lt;/a&gt;.
&lt;a href="https://doi.org/10.1109/ICST57152.2023.00041"&gt;https://doi.org/10.1109/ICST57152.2023.00041&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Henderson2018" class="csl-entry" role="doc-biblioentry"&gt;
Henderson, Tim A. D., and Andy Podgurski. 2018. &lt;span&gt;“Behavioral
&lt;span&gt;Fault Localization&lt;/span&gt; by &lt;span&gt;Sampling Suspicious Dynamic
Control Flow Subgraphs&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2018 &lt;span&gt;IEEE&lt;/span&gt;
11th &lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Software
Testing&lt;/span&gt;, &lt;span&gt;Verification&lt;/span&gt; and &lt;span&gt;Validation&lt;/span&gt;
(&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;, 93–104. &lt;span&gt;Vasteras&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. 
&lt;a href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html"&gt;https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html&lt;/a&gt;.
&lt;a href="https://doi.org/10.1109/ICST.2018.00019"&gt;https://doi.org/10.1109/ICST.2018.00019&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Henderson2019" class="csl-entry" role="doc-biblioentry"&gt;
Henderson, Tim A. D., Andy Podgurski, and Yigit Kucuk. 2019.
&lt;span&gt;“Evaluating &lt;span&gt;Automatic Fault Localization Using Markov
Processes&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019 19th &lt;span&gt;International Working
Conference&lt;/span&gt; on &lt;span&gt;Source Code Analysis&lt;/span&gt; and
&lt;span&gt;Manipulation&lt;/span&gt; (&lt;span&gt;SCAM&lt;/span&gt;)&lt;/em&gt;, 115–26.
&lt;span&gt;Cleveland, OH, USA&lt;/span&gt;: &lt;span&gt;IEEE&lt;/span&gt;.
&lt;a href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html"&gt;https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html&lt;/a&gt;
&lt;a href="https://doi.org/10.1109/SCAM.2019.00021"&gt;https://doi.org/10.1109/SCAM.2019.00021&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Herzig2015a" class="csl-entry" role="doc-biblioentry"&gt;
Herzig, Kim, and Nachiappan Nagappan. 2015. &lt;span&gt;“Empirically
&lt;span&gt;Detecting False Test Alarms Using Association
Rules&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt;
37th &lt;span&gt;IEEE International Conference&lt;/span&gt; on &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 39–48. &lt;span&gt;Florence, Italy&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE.2015.133"&gt;https://doi.org/10.1109/ICSE.2015.133&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Horwitz1992" class="csl-entry" role="doc-biblioentry"&gt;
Horwitz, S, and T Reps. 1992. &lt;span&gt;“The Use of Program Dependence
Graphs in Software Engineering.”&lt;/span&gt; In &lt;em&gt;International
&lt;span&gt;Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;,
9:349. &lt;span&gt;Springer&lt;/span&gt;. &lt;a
href="http://portal.acm.org/citation.cfm?id=24041&amp;amp;amp;dl="&gt;http://portal.acm.org/citation.cfm?id=24041&amp;amp;amp;dl=&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Jones2002" class="csl-entry" role="doc-biblioentry"&gt;
Jones, J.a., M. J. Harrold, and J. Stasko. 2002. &lt;span&gt;“Visualization of
Test Information to Assist Fault Localization.”&lt;/span&gt; &lt;em&gt;Proceedings
of the 24th International Conference on Software Engineering. ICSE
2002&lt;/em&gt;. &lt;a
href="https://doi.org/10.1145/581339.581397"&gt;https://doi.org/10.1145/581339.581397&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Jones2005" class="csl-entry" role="doc-biblioentry"&gt;
Jones, James A., and Mary Jean Harrold. 2005. &lt;span&gt;“Empirical
&lt;span&gt;Evaluation&lt;/span&gt; of the &lt;span class="nocase"&gt;Tarantula Automatic
Fault-localization Technique&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the
20th &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM International Conference&lt;/span&gt; on
&lt;span&gt;Automated Software Engineering&lt;/span&gt;&lt;/em&gt;, 273–82. &lt;span&gt;New
York, NY, USA&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/1101908.1101949"&gt;https://doi.org/10.1145/1101908.1101949&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-keenan2019" class="csl-entry" role="doc-biblioentry"&gt;
Keenan, James. 2019. &lt;span&gt;“James &lt;span&gt;E&lt;/span&gt;. &lt;span&gt;Keenan&lt;/span&gt; -
"&lt;span&gt;Multisection&lt;/span&gt;: &lt;span&gt;When Bisection Isn&lt;/span&gt;’t
&lt;span&gt;Enough&lt;/span&gt; to &lt;span&gt;Debug&lt;/span&gt; a
&lt;span&gt;Problem&lt;/span&gt;".”&lt;/span&gt; &lt;a
href="https://www.youtube.com/watch?v=05CwdTRt6AM"&gt;https://www.youtube.com/watch?v=05CwdTRt6AM&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Kucuk2021" class="csl-entry" role="doc-biblioentry"&gt;
Kucuk, Yigit, Tim A. D. Henderson, and Andy Podgurski. 2021.
&lt;span&gt;“Improving &lt;span&gt;Fault Localization&lt;/span&gt; by &lt;span&gt;Integrating
Value&lt;/span&gt; and &lt;span&gt;Predicate Based Causal Inference
Techniques&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2021
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 43rd &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;
(&lt;span&gt;ICSE&lt;/span&gt;)&lt;/em&gt;, 649–60. &lt;span&gt;Madrid, ES&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE43902.2021.00066"&gt;https://doi.org/10.1109/ICSE43902.2021.00066&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Leon2003" class="csl-entry" role="doc-biblioentry"&gt;
Leon, D., and A. Podgurski. 2003. &lt;span&gt;“A Comparison of Coverage-Based
and Distribution-Based Techniques for Filtering and Prioritizing Test
Cases.”&lt;/span&gt; In &lt;em&gt;14th &lt;span&gt;International Symposium&lt;/span&gt; on
&lt;span&gt;Software Reliability Engineering&lt;/span&gt;, 2003. &lt;span&gt;ISSRE&lt;/span&gt;
2003.&lt;/em&gt;, 2003-Janua:442–53. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ISSRE.2003.1251065"&gt;https://doi.org/10.1109/ISSRE.2003.1251065&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Leong2019" class="csl-entry" role="doc-biblioentry"&gt;
Leong, Claire, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John
Micco. 2019. &lt;span&gt;“Assessing &lt;span&gt;Transition-Based Test Selection
Algorithms&lt;/span&gt; at &lt;span&gt;Google&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 101–10. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00019"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00019&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Lewis2013" class="csl-entry" role="doc-biblioentry"&gt;
Lewis, Chris, Zhongpeng Lin, Caitlin Sadowski, and Xiaoyan Zhu. 2013.
&lt;span&gt;“Does Bug Prediction Support Human Developers? Findings from a
Google Case Study.”&lt;/span&gt; In &lt;em&gt;Proceedings of the …&lt;/em&gt;, 372–81. &lt;a
href="http://dl.acm.org/citation.cfm?id=2486838"&gt;http://dl.acm.org/citation.cfm?id=2486838&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Lucia2014" class="csl-entry" role="doc-biblioentry"&gt;
Lucia, David Lo, Lingxiao Jiang, Ferdian Thung, and Aditya Budi. 2014.
&lt;span&gt;“Extended Comprehensive Study of Association Measures for Fault
Localization.”&lt;/span&gt; &lt;em&gt;Journal of Software: Evolution and
Process&lt;/em&gt; 26 (2): 172–219. &lt;a
href="https://doi.org/10.1002/smr.1616"&gt;https://doi.org/10.1002/smr.1616&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Machalica2019" class="csl-entry" role="doc-biblioentry"&gt;
Machalica, Mateusz, Alex Samylkin, Meredith Porth, and Satish Chandra.
2019. &lt;span&gt;“Predictive &lt;span&gt;Test Selection&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 91–100. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00018"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00018&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-McVoy1996" class="csl-entry" role="doc-biblioentry"&gt;
McVoy, Larry. 1996. &lt;span&gt;“&lt;span&gt;BUG-HUNTING&lt;/span&gt;.”&lt;/span&gt;
&lt;span&gt;linux/1.3.73&lt;/span&gt;. &lt;a
href="https://elixir.bootlin.com/linux/1.3.73/source/
                  Documentation/BUG-HUNTING"&gt;https://elixir.bootlin.com/linux/1.3.73/source/
Documentation/BUG-HUNTING&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Memon2017" class="csl-entry" role="doc-biblioentry"&gt;
Memon, Atif, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob
Siemborski, and John Micco. 2017. &lt;span&gt;“Taming &lt;span
class="nocase"&gt;Google-scale&lt;/span&gt; Continuous Testing.”&lt;/span&gt; In
&lt;em&gt;2017 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 39th &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice Track&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 233–42. &lt;span&gt;Piscataway, NJ, USA&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2017.16"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.16&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Micco2012" class="csl-entry" role="doc-biblioentry"&gt;
Micco, John. 2012. &lt;span&gt;“Tools for &lt;span&gt;Continuous Integration&lt;/span&gt;
at &lt;span&gt;Google Scale&lt;/span&gt;.”&lt;/span&gt; Tech {{Talk}}. &lt;span&gt;Google
NYC&lt;/span&gt;. &lt;a
href="https://youtu.be/KH2_sB1A6lA"&gt;https://youtu.be/KH2_sB1A6lA&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Micco2013" class="csl-entry" role="doc-biblioentry"&gt;
———. 2013. &lt;span&gt;“Continuous &lt;span&gt;Integration&lt;/span&gt; at &lt;span&gt;Google
Scale&lt;/span&gt;.”&lt;/span&gt; Lecture. &lt;span&gt;EclipseCon 2013&lt;/span&gt;. &lt;a
href="https://web.archive.org/web/20140705215747/https://
                  www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
                  03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf"&gt;https://web.archive.org/web/20140705215747/https://
www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Mondal2015" class="csl-entry" role="doc-biblioentry"&gt;
Mondal, Debajyoti, Hadi Hemmati, and Stephane Durocher. 2015.
&lt;span&gt;“Exploring Test Suite Diversification and Code Coverage in
Multi-Objective Test Case Selection.”&lt;/span&gt; &lt;em&gt;2015 IEEE 8th
International Conference on Software Testing, Verification and
Validation, ICST 2015 - Proceedings&lt;/em&gt;, 1–10. &lt;a
href="https://doi.org/10.1109/ICST.2015.7102588"&gt;https://doi.org/10.1109/ICST.2015.7102588&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Musa2015" class="csl-entry" role="doc-biblioentry"&gt;
Musa, Samaila, Abu Bakar Md Sultan, Abdul Azim Bin Abd-Ghani, and Salmi
Baharom. 2015. &lt;span&gt;“Regression &lt;span&gt;Test Cases&lt;/span&gt; Selection for
&lt;span&gt;Object-Oriented Programs&lt;/span&gt; Based on &lt;span&gt;Affected
Statements&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;International Journal of Software
Engineering and Its Applications&lt;/em&gt; 9 (10): 91–108. &lt;a
href="https://doi.org/10.14257/ijseia.2015.9.10.10"&gt;https://doi.org/10.14257/ijseia.2015.9.10.10&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Najafi2019a" class="csl-entry" role="doc-biblioentry"&gt;
Najafi, Armin, Peter C. Rigby, and Weiyi Shang. 2019. &lt;span&gt;“Bisecting
Commits and Modeling Commit Risk During Testing.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on
&lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
&lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 279–89. &lt;span&gt;New York, NY, USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3338906.3338944"&gt;https://doi.org/10.1145/3338906.3338944&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Najafi2019" class="csl-entry" role="doc-biblioentry"&gt;
Najafi, Armin, Weiyi Shang, and Peter C. Rigby. 2019. &lt;span&gt;“Improving
&lt;span&gt;Test Effectiveness Using Test Executions History&lt;/span&gt;: &lt;span&gt;An
Industrial Experience Report&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 213–22. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00031"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00031&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ness1997" class="csl-entry" role="doc-biblioentry"&gt;
Ness, B., and V. Ngo. 1997. &lt;span&gt;“Regression Containment Through Source
Change Isolation.”&lt;/span&gt; In &lt;em&gt;Proceedings &lt;span&gt;Twenty-First Annual
International Computer Software&lt;/span&gt; and &lt;span&gt;Applications
Conference&lt;/span&gt; (&lt;span&gt;COMPSAC&lt;/span&gt;’97)&lt;/em&gt;, 616–21.
&lt;span&gt;Washington, DC, USA&lt;/span&gt;: &lt;span&gt;IEEE Comput. Soc&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/CMPSAC.1997.625082"&gt;https://doi.org/10.1109/CMPSAC.1997.625082&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ocariza2022" class="csl-entry" role="doc-biblioentry"&gt;
Ocariza, Frolin S. 2022. &lt;span&gt;“On the &lt;span&gt;Effectiveness&lt;/span&gt; of
&lt;span&gt;Bisection&lt;/span&gt; in &lt;span&gt;Performance Regression
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Empirical Software Engineering&lt;/em&gt; 27
(4): 95. &lt;a
href="https://doi.org/10.1007/s10664-022-10152-3"&gt;https://doi.org/10.1007/s10664-022-10152-3&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Osman2017" class="csl-entry" role="doc-biblioentry"&gt;
Osman, Haidar, Mohammad Ghafari, Oscar Nierstrasz, and Mircea Lungu.
2017. &lt;span&gt;“An &lt;span&gt;Extensive Analysis&lt;/span&gt; of &lt;span&gt;Efficient Bug
Prediction Configurations&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 13th
&lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Predictive Models&lt;/span&gt;
and &lt;span&gt;Data Analytics&lt;/span&gt; in &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 107–16. &lt;span&gt;Toronto Canada&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3127005.3127017"&gt;https://doi.org/10.1145/3127005.3127017&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-pan2022" class="csl-entry" role="doc-biblioentry"&gt;
Pan, Rongqi, Mojtaba Bagherzadeh, Taher A. Ghaleb, and Lionel Briand.
2022. &lt;span&gt;“Test Case Selection and Prioritization Using Machine
Learning: A Systematic Literature Review.”&lt;/span&gt; &lt;em&gt;Empirical Software
Engineering&lt;/em&gt; 27 (2): 29. &lt;a
href="https://doi.org/10.1007/s10664-021-10066-6"&gt;https://doi.org/10.1007/s10664-021-10066-6&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-parry2022" class="csl-entry" role="doc-biblioentry"&gt;
Parry, Owain, Gregory M. Kapfhammer, Michael Hilton, and Phil McMinn.
2022. &lt;span&gt;“A &lt;span&gt;Survey&lt;/span&gt; of &lt;span&gt;Flaky Tests&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;ACM Transactions on Software Engineering and Methodology&lt;/em&gt; 31
(1): 1–74. &lt;a
href="https://doi.org/10.1145/3476105"&gt;https://doi.org/10.1145/3476105&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-scikit-learn" class="csl-entry" role="doc-biblioentry"&gt;
Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.
Grisel, M. Blondel, et al. 2011. &lt;span&gt;“Scikit-Learn:
&lt;span&gt;Machine&lt;/span&gt; Learning in &lt;span&gt;Python&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;Journal of Machine Learning Research&lt;/em&gt; 12: 2825–30.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Pelc2002" class="csl-entry" role="doc-biblioentry"&gt;
Pelc, Andrzej. 2002. &lt;span&gt;“Searching Games with Errorsfifty Years of
Coping with Liars.”&lt;/span&gt; &lt;em&gt;Theoretical Computer Science&lt;/em&gt; 270
(1-2): 71–109. &lt;a
href="https://doi.org/10.1016/S0304-3975(01)00303-6"&gt;https://doi.org/10.1016/S0304-3975(01)00303-6&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Podgurski1990" class="csl-entry" role="doc-biblioentry"&gt;
Podgurski, A, and L A Clarke. 1990. &lt;span&gt;“A &lt;span&gt;Formal Model&lt;/span&gt;
of &lt;span&gt;Program Dependences&lt;/span&gt; and &lt;span&gt;Its Implications&lt;/span&gt;
for &lt;span&gt;Software Testing&lt;/span&gt;, &lt;span&gt;Debugging&lt;/span&gt;, and
&lt;span&gt;Maintenance&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions of Software
Engineering&lt;/em&gt; 16 (9): 965–79. &lt;a
href="https://doi.org/10.1109/32.58784"&gt;https://doi.org/10.1109/32.58784&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Potvin2016" class="csl-entry" role="doc-biblioentry"&gt;
Potvin, Rachel, and Josh Levenberg. 2016. &lt;span&gt;“Why Google Stores
Billions of Lines of Code in a Single Repository.”&lt;/span&gt;
&lt;em&gt;Communications of the ACM&lt;/em&gt; 59 (7): 78–87. &lt;a
href="https://doi.org/10.1145/2854146"&gt;https://doi.org/10.1145/2854146&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Punitha2013" class="csl-entry" role="doc-biblioentry"&gt;
Punitha, K., and S. Chitra. 2013. &lt;span&gt;“Software Defect Prediction
Using Software Metrics - &lt;span&gt;A&lt;/span&gt; Survey.”&lt;/span&gt; In &lt;em&gt;2013
&lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Information
Communication&lt;/span&gt; and &lt;span&gt;Embedded Systems&lt;/span&gt;
(&lt;span&gt;ICICES&lt;/span&gt;)&lt;/em&gt;, 555–58. &lt;span&gt;Chennai&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICICES.2013.6508369"&gt;https://doi.org/10.1109/ICICES.2013.6508369&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ren2004" class="csl-entry" role="doc-biblioentry"&gt;
Ren, Xiaoxia, Fenil Shah, Frank Tip, Barbara G. Ryder, and Ophelia
Chesley. 2004. &lt;span&gt;“Chianti: A Tool for Change Impact Analysis of Java
Programs.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 19th Annual &lt;span&gt;ACM
SIGPLAN&lt;/span&gt; Conference on &lt;span class="nocase"&gt;Object-oriented&lt;/span&gt;
Programming, Systems, Languages, and Applications&lt;/em&gt;, 432–48.
&lt;span&gt;Vancouver BC Canada&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/1028976.1029012"&gt;https://doi.org/10.1145/1028976.1029012&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Rivest1978" class="csl-entry" role="doc-biblioentry"&gt;
Rivest, R. L., A. R. Meyer, and D. J. Kleitman. 1978. &lt;span&gt;“Coping with
Errors in Binary Search Procedures (&lt;span&gt;Preliminary
Report&lt;/span&gt;).”&lt;/span&gt; In &lt;em&gt;Proceedings of the Tenth Annual
&lt;span&gt;ACM&lt;/span&gt; Symposium on &lt;span&gt;Theory&lt;/span&gt; of Computing -
&lt;span&gt;STOC&lt;/span&gt; ’78&lt;/em&gt;, 227–32. &lt;span&gt;San Diego, California, United
States&lt;/span&gt;: &lt;span&gt;ACM Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/800133.804351"&gt;https://doi.org/10.1145/800133.804351&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Rodriguez-Perez2018" class="csl-entry"
role="doc-biblioentry"&gt;
Rodríguez-Pérez, Gema, Gregorio Robles, and Jesús M. González-Barahona.
2018. &lt;span&gt;“Reproducibility and Credibility in Empirical Software
Engineering: &lt;span&gt;A&lt;/span&gt; Case Study Based on a Systematic Literature
Review of the Use of the &lt;span&gt;SZZ&lt;/span&gt; Algorithm.”&lt;/span&gt;
&lt;em&gt;Information and Software Technology&lt;/em&gt; 99 (July): 164–76. &lt;a
href="https://doi.org/10.1016/j.infsof.2018.03.009"&gt;https://doi.org/10.1016/j.infsof.2018.03.009&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Saha2017" class="csl-entry" role="doc-biblioentry"&gt;
Saha, Ripon, and Milos Gligoric. 2017. &lt;span&gt;“Selective &lt;span&gt;Bisection
Debugging&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Fundamental &lt;span&gt;Approaches&lt;/span&gt; to
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, edited by Marieke Huisman and
Julia Rubin, 10202:60–77. &lt;span&gt;Berlin, Heidelberg&lt;/span&gt;:
&lt;span&gt;Springer Berlin Heidelberg&lt;/span&gt;. &lt;a
href="https://doi.org/10.1007/978-3-662-54494-5_4"&gt;https://doi.org/10.1007/978-3-662-54494-5_4&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Singh2012" class="csl-entry" role="doc-biblioentry"&gt;
Singh, Yogesh, Arvinder Kaur, Bharti Suri, and Shweta Singhal. 2012.
&lt;span&gt;“Systematic Literature Review on Regression Test Prioritization
Techniques.”&lt;/span&gt; &lt;em&gt;Informatica (Slovenia)&lt;/em&gt; 36 (4): 379–408. &lt;a
href="https://doi.org/10.31449/inf.v36i4.420"&gt;https://doi.org/10.31449/inf.v36i4.420&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Sliwerski2005" class="csl-entry" role="doc-biblioentry"&gt;
Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller. 2005.
&lt;span&gt;“When Do Changes Induce Fixes?”&lt;/span&gt; &lt;em&gt;ACM SIGSOFT Software
Engineering Notes&lt;/em&gt; 30 (4): 1. &lt;a
href="https://doi.org/10.1145/1082983.1083147"&gt;https://doi.org/10.1145/1082983.1083147&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wang2020" class="csl-entry" role="doc-biblioentry"&gt;
Wang, Kaiyuan, Greg Tener, Vijay Gullapalli, Xin Huang, Ahmed Gad, and
Daniel Rall. 2020. &lt;span&gt;“Scalable Build Service System with Smart
Scheduling Service.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 29th &lt;span&gt;ACM
SIGSOFT International Symposium&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;
and &lt;span&gt;Analysis&lt;/span&gt;&lt;/em&gt;, 452–62. &lt;span&gt;Virtual Event USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3395363.3397371"&gt;https://doi.org/10.1145/3395363.3397371&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wen2019" class="csl-entry" role="doc-biblioentry"&gt;
Wen, Ming, Rongxin Wu, Yepang Liu, Yongqiang Tian, Xuan Xie, Shing-Chi
Cheung, and Zhendong Su. 2019. &lt;span&gt;“Exploring and Exploiting the
Correlations Between Bug-Inducing and Bug-Fixing Commits.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on
&lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
&lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 326–37. &lt;span&gt;Tallinn Estonia&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3338906.3338962"&gt;https://doi.org/10.1145/3338906.3338962&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wong2016" class="csl-entry" role="doc-biblioentry"&gt;
Wong, W. Eric, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016.
&lt;span&gt;“A &lt;span&gt;Survey&lt;/span&gt; on &lt;span&gt;Software Fault
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software
Engineering&lt;/em&gt; 42 (8): 707–40. &lt;a
href="https://doi.org/10.1109/TSE.2016.2521368"&gt;https://doi.org/10.1109/TSE.2016.2521368&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Youm2015" class="csl-entry" role="doc-biblioentry"&gt;
Youm, Klaus Changsun, June Ahn, Jeongho Kim, and Eunseok Lee. 2015.
&lt;span&gt;“Bug &lt;span&gt;Localization Based&lt;/span&gt; on &lt;span&gt;Code Change
Histories&lt;/span&gt; and &lt;span&gt;Bug Reports&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015
&lt;span&gt;Asia-Pacific Software Engineering Conference&lt;/span&gt;
(&lt;span&gt;APSEC&lt;/span&gt;)&lt;/em&gt;, 190–97. &lt;span&gt;New Delhi&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/APSEC.2015.23"&gt;https://doi.org/10.1109/APSEC.2015.23&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Yourdon1975" class="csl-entry" role="doc-biblioentry"&gt;
Yourdon, E. 1975. &lt;em&gt;Techniques of &lt;span&gt;Program Design&lt;/span&gt;&lt;/em&gt;.
&lt;span&gt;New Jersey&lt;/span&gt;: &lt;span&gt;Prentice-Hall&lt;/span&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Zeller1999" class="csl-entry" role="doc-biblioentry"&gt;
Zeller, Andreas. 1999. &lt;span&gt;“Yesterday, &lt;span&gt;My Program Worked&lt;/span&gt;.
&lt;span&gt;Today&lt;/span&gt;, &lt;span&gt;It Does Not&lt;/span&gt;. &lt;span&gt;Why&lt;/span&gt;?”&lt;/span&gt;
&lt;em&gt;SIGSOFT Softw. Eng. Notes&lt;/em&gt; 24 (6): 253–67. &lt;a
href="https://doi.org/10.1145/318774.318946"&gt;https://doi.org/10.1145/318774.318946&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Zhou2012" class="csl-entry" role="doc-biblioentry"&gt;
Zhou, Jian, Hongyu Zhang, and David Lo. 2012. &lt;span&gt;“Where Should the
Bugs Be Fixed? &lt;span&gt;More&lt;/span&gt; Accurate Information Retrieval-Based
Bug Localization Based on Bug Reports.”&lt;/span&gt; &lt;em&gt;Proceedings -
International Conference on Software Engineering&lt;/em&gt;, 14–24. &lt;a
href="https://doi.org/10.1109/ICSE.2012.6227210"&gt;https://doi.org/10.1109/ICSE.2012.6227210&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Zhou2010" class="csl-entry" role="doc-biblioentry"&gt;
Zhou, Zhi Quan. 2010. &lt;span&gt;“Using Coverage Information to Guide Test
Case Selection in &lt;span&gt;Adaptive Random Testing&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;Proceedings - International Computer Software and Applications
Conference&lt;/em&gt;, 208–13. &lt;a
href="https://doi.org/10.1109/COMPSACW.2010.43"&gt;https://doi.org/10.1109/COMPSACW.2010.43&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ziftci2013a" class="csl-entry" role="doc-biblioentry"&gt;
Ziftci, Celal, and Vivek Ramavajjala. 2013. &lt;span&gt;“Finding
&lt;span&gt;Culprits Automatically&lt;/span&gt; in &lt;span&gt;Failing Builds&lt;/span&gt; -
i.e. &lt;span&gt;Who Broke&lt;/span&gt; the &lt;span&gt;Build&lt;/span&gt;?”&lt;/span&gt; &lt;a
href="https://www.youtube.com/watch?v=SZLuBYlq3OM"&gt;https://www.youtube.com/watch?v=SZLuBYlq3OM&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ziftci2017" class="csl-entry" role="doc-biblioentry"&gt;
Ziftci, Celal, and Jim Reardon. 2017. &lt;span&gt;“Who Broke the Build?
&lt;span&gt;Automatically&lt;/span&gt; Identifying Changes That Induce Test Failures
in Continuous Integration at Google Scale.”&lt;/span&gt; &lt;em&gt;Proceedings -
2017 IEEE/ACM 39th International Conference on Software Engineering:
Software Engineering in Practice Track, ICSE-SEIP 2017&lt;/em&gt;, 113–22. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2017.13"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.13&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;/div&gt;
&lt;section class="footnotes footnotes-end-of-document"
role="doc-endnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1" role="doc-endnote"&gt;&lt;p&gt;Both the &lt;em&gt;per-test&lt;/em&gt; error rate
and the &lt;em&gt;per-change&lt;/em&gt; error rates quoted above are drawn from the
same 28 day window. The Postsubmit result which triggers culprit finding
may itself have been a flake, so there were spurious suspect ranges in
the same period. These rates may differ slightly from the numbers
reported in &lt;span class="citation" data-cites="Henderson2023"&gt;(&lt;a
href="#ref-Henderson2023" role="doc-biblioref"&gt;Henderson et al.
2023&lt;/a&gt;)&lt;/span&gt; as they reflect our most recent data as of
2023-11-12.&lt;a href="#fnref1" class="footnote-back"
role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2" role="doc-endnote"&gt;&lt;p&gt;There are some legacy tests which are
allowed to use the network but there is an on-going effort to clean up
their usage.&lt;a href="#fnref2" class="footnote-back"
role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3" role="doc-endnote"&gt;&lt;p&gt;Previously, we referred to these
cycles as “Milestones” in most of the previous literature but we are
gradually changing our internal nomenclature as we evolve TAP
Postsubmit’s testing model.&lt;a href="#fnref3" class="footnote-back"
role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4" role="doc-endnote"&gt;&lt;p&gt;https://scikit-learn.org/&lt;a
href="#fnref4" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://xgboost.readthedocs.io/en/stable/index.html"
class="uri"&gt;https://xgboost.readthedocs.io/en/stable/index.html&lt;/a&gt;&lt;a
href="#fnref5" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="Paper"></category></entry><entry><title>Flake Aware Culprit Finding</title><link href="https://hackthology.com/flake-aware-culprit-finding.html" rel="alternate"></link><published>2023-04-17T00:00:00-04:00</published><updated>2023-04-17T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, Bobby Dorward, Eric Nickell, Collin Johnston, Avi Kondareddy</name></author><id>tag:hackthology.com,2023-04-17:/flake-aware-culprit-finding.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy.
&lt;em&gt;Flake Aware Culprit Finding&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icst-2023/icst-2023-industry/46/Flake-Aware-Culprit-Finding"&gt;ICST Industry Track 2023&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.dx.org/10.1109/ICST57152.2023.00041"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/flake-aware-culprit-finding.html"&gt;WEB&lt;/a&gt;.
&lt;a href="https://research.google/pubs/pub52048/"&gt;Google Research Preprint&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;pdf version …&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy.
&lt;em&gt;Flake Aware Culprit Finding&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icst-2023/icst-2023-industry/46/Flake-Aware-Culprit-Finding"&gt;ICST Industry Track 2023&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.dx.org/10.1109/ICST57152.2023.00041"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/flake-aware-culprit-finding.html"&gt;WEB&lt;/a&gt;.
&lt;a href="https://research.google/pubs/pub52048/"&gt;Google Research Preprint&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;When a change introduces a bug into a large software repository, there is
often a delay between when the change is committed and when bug is detected.
This is true even when the bug causes an existing test to fail! These delays
are caused by resource constraints which prevent the organization from running
all of the tests on every change. Due to the delay, a Continuous Integration
system needs to locate buggy commits. Locating them is complicated by flaky
tests that pass and fail non-deterministically. The flaky tests introduce
noise into the CI system requiring costly reruns to determine if a failure was
caused by a bad code change or caused by non-deterministic test behavior. This
paper presents an algorithm, &lt;em&gt;Flake Aware Culprit Finding&lt;/em&gt;, that locates
buggy commits more accurately than a traditional bisection search. The
algorithm is based on Bayesian inference and noisy binary search, utilizing
prior information about which changes are most likely to contain the bug. A
large scale empirical study was conducted at Google on 13,000+ test breakages.
The study evaluates the accuracy and cost of the new algorithm versus a
traditional deflaked bisection search.&lt;/p&gt;
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;h1 id="sec:introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Fast, collaborative, large scale development is enabled by the use of
Continuous Integration in a &lt;em&gt;mono-repository&lt;/em&gt; (monorepo)
environment where all of the source code is stored in a single shared
repository &lt;span class="citation" data-cites="Potvin2016"&gt;(&lt;a
href="#ref-Potvin2016" role="doc-biblioref"&gt;Potvin and Levenberg
2016&lt;/a&gt;)&lt;/span&gt;. In a monorepo, all developers share a single source of
truth for the state of the code and most builds are made from “head”
(the most recent commit to the repository) using the source code for all
libraries and dependencies rather than versioned pre-compiled archives.
This enables (among other things) unified versioning, atomic changes,
large scale refactorings, code re-use, cross team collaboration, and
flexible code ownership boundaries.&lt;/p&gt;
&lt;p&gt;Very large scale monorepos (such as those at Google &lt;span
class="citation" data-cites="Potvin2016 Memon2017"&gt;(&lt;a
href="#ref-Potvin2016" role="doc-biblioref"&gt;Potvin and Levenberg
2016&lt;/a&gt;; &lt;a href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al.
2017&lt;/a&gt;)&lt;/span&gt;, Microsoft &lt;span class="citation"
data-cites="Herzig2015a"&gt;(&lt;a href="#ref-Herzig2015a"
role="doc-biblioref"&gt;Herzig and Nagappan 2015&lt;/a&gt;)&lt;/span&gt;, and Facebook
&lt;span class="citation" data-cites="Machalica2019"&gt;(&lt;a
href="#ref-Machalica2019" role="doc-biblioref"&gt;Machalica et al.
2019&lt;/a&gt;)&lt;/span&gt;) require advanced systems for ensuring changes
submitted to the repository are properly tested and validated.
&lt;em&gt;Continuous Integration&lt;/em&gt; (CI) &lt;span class="citation"
data-cites="Fowler2006"&gt;(&lt;a href="#ref-Fowler2006"
role="doc-biblioref"&gt;Fowler 2006&lt;/a&gt;)&lt;/span&gt; is a system and practice of
automatically integrating changes into the code repository that serves
as the source of truth for the organization. In modern environments,
integration involves not only performing the textual merge required to
add the change but also verification tasks such as ensuring the software
compiles and the automated tests pass both on the new change before it
is integrated &lt;em&gt;and after&lt;/em&gt; it has been incorporated into the main
development branch.&lt;/p&gt;

&lt;p&gt;&lt;span id="fig:ci-timeline" label="fig:ci-timeline"&gt;
&lt;a href="images/icst-2023/fig1.png"&gt;&lt;img alt="Figure 1" src="images/icst-2023/fig1.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Due to resource constraints, tests are only run at specific versions
(called &lt;em&gt;milestones&lt;/em&gt; at Google) and many tests may be skipped
because they were not affected by the intervening changes (as determined
by static build dependencies) &lt;span class="citation"
data-cites="Gupta2011"&gt;(&lt;a href="#ref-Gupta2011"
role="doc-biblioref"&gt;Gupta, Ivey, and Penix 2011&lt;/a&gt;)&lt;/span&gt;. This
process is illustrated in Figure &lt;a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt;. A test is
considered passing at a version &lt;span class="math inline"&gt;\(b\)&lt;/span&gt;
if and only if it has a passing result at the most recent preceding
affecting version. Formally, if there exists a version &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt; where the test is passing and &lt;span
class="math inline"&gt;\(a \le b\)&lt;/span&gt; and there does not exist an
intervening version &lt;span class="math inline"&gt;\(c\)&lt;/span&gt;, &lt;span
class="math inline"&gt;\(a &amp;lt; c \le b\)&lt;/span&gt;, which affects the test
then the test is passing at version &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt;. A project’s status is considered
passing at a version &lt;span class="math inline"&gt;\(b\)&lt;/span&gt; if all tests
are passing (using the above definition) at version &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If a test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; is failing
(consistently) at a milestone version &lt;span
class="math inline"&gt;\(m\)&lt;/span&gt;, it may not necessarily mean that
version &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; introduced a change which
broke test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;. As shown in Figure &lt;a
href="#fig:ci-timeline" data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt; one or more
changes between &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; and the previous
milestone at which test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; was run
may have introduced a breaking change. Figuring out which version
introduced the &lt;em&gt;breakage&lt;/em&gt; is called &lt;em&gt;culprit finding&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Many companies now employ automated culprit finding systems. These
systems typically implement a (possibly n-way) “bisect search” similar
to the one built into the Git version control system &lt;span
class="citation" data-cites="Couder2008"&gt;(&lt;a href="#ref-Couder2008"
role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;. Git’s bisect command runs
a binary search on the versions between the first known breaking version
and the last known passing version to identify the version which broke
the test.&lt;/p&gt;
&lt;p&gt;In the Google environment, there are two problems with a traditional
bisection algorithm &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;: (1)
it does not account for flaky (non-deterministic) tests, and (2) when
using &lt;span class="math inline"&gt;\(k\)&lt;/span&gt;-reruns to deflake the
search accuracy plateaus while build cost continues to increase
linearly. &lt;strong&gt;This paper proposes&lt;/strong&gt; a new method for culprit
finding which is both robust to non-deterministic (flaky) test behavior
and can identify the culprit with only logarithmic builds in the best
case by using prior information to accelerate the search.&lt;/p&gt;
&lt;h2 id="contributions"&gt;Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A Flake Aware Culprit Finding (FACF) algorithm.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A mathematical model for culprit finding adjusting for
non-deterministic test behavior.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A large scale empirical study of FACF on Google’s mono repository
comparing its performance against the traditional bisect
algorithm.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The study also evaluates the effectiveness: (a) optimizations for
FACF, and (b) adding deflaking runs to Bisect.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="background"&gt;Background&lt;/h1&gt;
&lt;p&gt;At Google, the Test Automation Platform (TAP) runs most tests that
can run on a single machine (and do not use the network except for the
loopback interface)&lt;a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and can run in under 15 minutes.&lt;a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Figure &lt;a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt; illustrates at a
high level of how TAP works. First, (not illustrated) tests are grouped
into logical projects by the teams that own them. Then, (for accounting
and quota allocation purposes) those projects are grouped into very high
level groupings called Quota Buckets. Each Quota Bucket is scheduled
independently when build resources are available. This means that TAP
does not run every test at every version. Instead it only runs tests
periodically at select versions called “Milestones” &lt;span
class="citation" data-cites="Memon2017 Micco2013 Leong2019"&gt;(&lt;a
href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;; &lt;a
href="#ref-Micco2013" role="doc-biblioref"&gt;Micco 2013&lt;/a&gt;; &lt;a
href="#ref-Leong2019" role="doc-biblioref"&gt;Leong et al.
2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;When scheduling a Milestone, all the tests that are built and run
with the same command line flags to the build system&lt;a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; are
grouped into a set. That set is filtered to remove tests that haven’t
been affected by a change to themselves or a build level dependency
&lt;span class="citation" data-cites="Gupta2011 Micco2013 Micco2012"&gt;(&lt;a
href="#ref-Gupta2011" role="doc-biblioref"&gt;Gupta, Ivey, and Penix
2011&lt;/a&gt;; &lt;a href="#ref-Micco2013" role="doc-biblioref"&gt;Micco 2013&lt;/a&gt;,
&lt;a href="#ref-Micco2012" role="doc-biblioref"&gt;2012&lt;/a&gt;)&lt;/span&gt;. This is
a very coarse grained form of static dependence-based test selection
&lt;span class="citation" data-cites="Binkley1992 Horwitz1992 Tip1995"&gt;(&lt;a
href="#ref-Binkley1992" role="doc-biblioref"&gt;Binkley 1992&lt;/a&gt;; &lt;a
href="#ref-Horwitz1992" role="doc-biblioref"&gt;Horwitz and Reps 1992&lt;/a&gt;;
&lt;a href="#ref-Tip1995" role="doc-biblioref"&gt;Tip 1995&lt;/a&gt;)&lt;/span&gt;.
Google’s use of build dependence test selection has been influential and
it is now also used at a number of other corporations who have adopted
Bazel or similar tools &lt;span class="citation"
data-cites="Ananthanarayanan2019 Machalica2019"&gt;(&lt;a
href="#ref-Ananthanarayanan2019" role="doc-biblioref"&gt;Ananthanarayanan
et al. 2019&lt;/a&gt;; &lt;a href="#ref-Machalica2019"
role="doc-biblioref"&gt;Machalica et al. 2019&lt;/a&gt;)&lt;/span&gt;. Finally, that
(potentially very large) set of “affected” tests is sent to the Build
System &lt;span class="citation" data-cites="Wang2020"&gt;(&lt;a
href="#ref-Wang2020" role="doc-biblioref"&gt;Wang et al. 2020&lt;/a&gt;)&lt;/span&gt;
which batches them into efficiently sized groups and then builds and
runs them when there is capacity.&lt;/p&gt;
&lt;p&gt;Since tests are only run periodically even with accurate build level
dependence analysis to drive test selection, there are usually multiple
changes which may have introduced a fault in the software or a bug in
the test. Finding these “bug inducing changes” – which we call
&lt;strong&gt;culprits&lt;/strong&gt; – is the subject of this paper. The process of
finding these changes is aggravated by any non-deterministic behavior of
the tests and the build system running them.&lt;/p&gt;
&lt;h2 id="flaky-tests"&gt;Flaky Tests&lt;/h2&gt;
&lt;p&gt;When a test behaves non-deterministically, we refer to it, using the
standard term, as a &lt;em&gt;flaky test&lt;/em&gt;. A flaky test is one that passes
or fails non-deterministically with no changes to the code of either the
test or the system under test. At Google, we consider all tests to be at
least &lt;em&gt;slightly&lt;/em&gt; flaky as the machines that tests are running on
could fail in ways that get classified as a test failure (such as
resource exhaustion due to a noisy neighbor resulting in a test
exceeding its time limit). We encourage everyone to adopt this
conservative view for the purposes of identifying culprits.&lt;/p&gt;
&lt;p&gt;An important input to the culprit finding algorithm we are presenting
is an estimate of how flaky a test might be, as represented by an
estimated probability of the test failing due to a flaky failure. We
will denote the estimated probability of failure due to a flake for a
test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; as &lt;span
class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt;. The actual probability of
failure due to a flake will be denoted &lt;span
class="math inline"&gt;\(f_t\)&lt;/span&gt; (it is impossible in practice to know
&lt;span class="math inline"&gt;\(f_t\)&lt;/span&gt;). At Google, we estimate &lt;span
class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt; based on the historical failure
rate of the test. Apple and Facebook recently published on their work in
on estimating flake rates &lt;span class="citation"
data-cites="Kowalczyk2020 Machalica2020"&gt;(&lt;a href="#ref-Kowalczyk2020"
role="doc-biblioref"&gt;Kowalczyk et al. 2020&lt;/a&gt;; &lt;a
href="#ref-Machalica2020" role="doc-biblioref"&gt;Machalica et al.
2020&lt;/a&gt;)&lt;/span&gt;. One challenge with flake rate estimation is that a
single change to the code can completely change the flakiness
characteristics of a test in unpredictable ways. Making the flake rate
estimation sensitive to recent changes is an ongoing challenge and one
we partially solve in this work by doing live re-estimation of the flake
rate during the culprit finding process itself.&lt;/p&gt;
&lt;h2 id="sec:background:cf"&gt;Culprit Finding&lt;/h2&gt;
&lt;p&gt;Informally, Culprit Finding is the problem of identifying a version
&lt;span class="math inline"&gt;\(c\)&lt;/span&gt; between two versions &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt; such that &lt;span class="math inline"&gt;\(a
&amp;lt; c \le b\)&lt;/span&gt; where a given test &lt;span
class="math inline"&gt;\(t\)&lt;/span&gt; was passing at &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt;, failing at &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;, and &lt;span
class="math inline"&gt;\(c\)&lt;/span&gt; is the version which introduced the
fault. At Google we consider the passing status of a test to be 100%
reliable and therefore version &lt;span class="math inline"&gt;\(a\)&lt;/span&gt;
cannot be a culprit but version &lt;span class="math inline"&gt;\(b\)&lt;/span&gt;
could be the culprit. For simplicity, this paper only considers
searching for the culprit for a single test &lt;span
class="math inline"&gt;\(t\)&lt;/span&gt; (which failed at &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;) but in principle it can be extended to
a set of tests &lt;span class="math inline"&gt;\(T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Google’s version control system, Piper &lt;span class="citation"
data-cites="Potvin2016"&gt;(&lt;a href="#ref-Potvin2016"
role="doc-biblioref"&gt;Potvin and Levenberg 2016&lt;/a&gt;)&lt;/span&gt;, stores the
mainline history as a linear list of versions. Each version gets a
monotonically, &lt;em&gt;but not sequentially&lt;/em&gt;, increasing number as its
version number called the &lt;em&gt;Changelist Number&lt;/em&gt; or CL number for
short. Thus, our implementation and experiments do not consider branches
and merges as would be necessary for a Distributed Version Control
System (DVCS) such as git. However, only trivial changes to the
mathematics are needed to apply our approach to such an environment
&lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus, the versions larger than &lt;span class="math inline"&gt;\(a\)&lt;/span&gt;
and smaller or equal to &lt;span class="math inline"&gt;\(b\)&lt;/span&gt; are an
ordered list of items we will call the &lt;em&gt;search range&lt;/em&gt; and denote
as &lt;span class="math inline"&gt;\(S =
\left\{s_1, ..., s_{n-1}, s_n\right\}\)&lt;/span&gt; where &lt;span
class="math inline"&gt;\(s_n = b\)&lt;/span&gt;. During the search, test
executions will be run (and may be run in parallel) at various versions.
The unordered set of executions at version &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt; will be denoted &lt;span
class="math inline"&gt;\(X_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For the purposes of culprit finding, based on the status of failure
detected, the build/test execution statuses are mapped to one of 2
possible states: &lt;code&gt;PASS&lt;/code&gt; or &lt;code&gt;FAIL&lt;/code&gt;. We denote (for
version &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt;): the number of
statuses that map to &lt;code&gt;PASS&lt;/code&gt; as &lt;span
class="math inline"&gt;\(\texttt{\small PASSES}(X_i)\)&lt;/span&gt; and the
number of statuses that map to &lt;code&gt;FAIL&lt;/code&gt; as &lt;span
class="math inline"&gt;\(\texttt{\small FAILS}(X_i)\)&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id="bisection"&gt;Bisection&lt;/h2&gt;

&lt;p&gt;&lt;span id="alg:bisect" label="alg:bisect"&gt;
&lt;a href="images/icst-2023/alg1.png"&gt;&lt;img alt="Algorithm Bisect" src="images/icst-2023/alg1.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Bisect algorithm is a specialized form of binary search. It runs
tests (or more generally any operation on the code that can return PASS
or FAIL) to attempt to identify which commit introduced a bug. Algorithm
&lt;a href="#alg:bisect" data-reference-type="ref"
data-reference="alg:bisect"&gt;[alg:bisect]&lt;/a&gt; gives the psuedo-code for
this procedure. Bisect is now commonly implemented in most version
control systems (see &lt;code&gt;git bisect&lt;/code&gt; and &lt;code&gt;hg bisect&lt;/code&gt;
as common examples). In the case of Distributed Version Control Systems
(DVCS) systems such as ‘git‘ and ‘hg‘ the implementation of bisect is
somewhat more complex as they model version history as a directed
acyclic graph &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The algorithm presented in Alg &lt;a href="#alg:bisect"
data-reference-type="ref" data-reference="alg:bisect"&gt;[alg:bisect]&lt;/a&gt;
has two important changes from the standard binary search. First, it
takes into account that a test can fail and pass at the same version and
the &lt;code&gt;firstFail&lt;/code&gt; function specifically looks for a failure
after the &lt;code&gt;lastPass&lt;/code&gt;. Second, it takes a parameter &lt;span
class="math inline"&gt;\(k\)&lt;/span&gt; which allows the user to specify that
each test execution be run &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; times.
This allows the user to “deflake” the test executions by rerunning
failures. Our empirical evaluation (Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;)
examines both the cost and effectiveness of this simple modification.
While this simple modification to bisect may be reasonably effective for
moderately flaky tests, it does not explicitly model the effect of
non-deterministic failures on the likelihood of a suspect to be the
culprit.&lt;/p&gt;
&lt;h2 id="guessing-games-and-noisy-searches"&gt;Guessing Games and Noisy
Searches&lt;/h2&gt;
&lt;div class="algorithm"&gt;

&lt;/div&gt;
&lt;p&gt;One can view the culprit finding scenario as a kind of guessing game.
The questioner guesses the test always fails at a given suspect version.
A test execution at that suspect acts as a kind of “responder” or
“oracle” which answers. The test execution is an unreliable oracle which
may answer FAIL when it should have answered PASS. But, the oracle never
answers PASS when it should have answered FAIL – note the asymmetry.&lt;/p&gt;
&lt;p&gt;This kind of guessing game is a Rényi-Ulam game, which has been well
characterized by the community &lt;span class="citation"
data-cites="Pelc2002"&gt;(&lt;a href="#ref-Pelc2002" role="doc-biblioref"&gt;Pelc
2002&lt;/a&gt;)&lt;/span&gt;. The main differences from the “classical” games
presented by Rényi and separately by Ulam is (a) the format of the
questions and (b) the limitations placed on the oracle responding
incorrectly. In a common formulation of the Rényi-Ulam game, the
questioner is trying to “guess the number” between (for instance) 1 and
1,000,000. Each question is allowed to be any subset of numbers in the
search range. The responder or oracle is allowed to lie at most a fixed
number of times (for instance once or twice). This formulation has
allowed the community to find analytic solutions to determine the number
of questions in the optimal strategy. We refer the reader to the survey
from Pelc for an overview of the field and its connection with error
correcting codes &lt;span class="citation" data-cites="Pelc2002"&gt;(&lt;a
href="#ref-Pelc2002" role="doc-biblioref"&gt;Pelc 2002&lt;/a&gt;)&lt;/span&gt;. The
culprit finding scenario is “noisy binary search” scenario where the
oracle only lies for one type of response. Rivest &lt;em&gt;et al.&lt;/em&gt; &lt;span
class="citation" data-cites="Rivest1978"&gt;(&lt;a href="#ref-Rivest1978"
role="doc-biblioref"&gt;Rivest, Meyer, and Kleitman 1978&lt;/a&gt;)&lt;/span&gt;
provide a theoretical treatment of this “half-lie” variant (as well as
the usual variation).&lt;/p&gt;
&lt;h2 id="bayesian-binary-search"&gt;Bayesian Binary Search&lt;/h2&gt;
&lt;p&gt;Ben Or and Hassidim &lt;span class="citation"
data-cites="Ben-Or2008"&gt;(&lt;a href="#ref-Ben-Or2008"
role="doc-biblioref"&gt;Ben-Or and Hassidim 2008&lt;/a&gt;)&lt;/span&gt; noted the
connection between these “Noisy Binary Search” problems and Bayesian
inference. Our work builds off of their formulation and applies it to
the culprit finding setting. In the Bayesian setting, consider the
probability each suspect is the culprit and the probability there is no
culprit (i.e. the original failure was caused by a flake). These
probabilities together form a distribution: &lt;span
class="math display"&gt;\[(\textrm{Pr}\left[{\text{there is no
culprit}}\right]) +
  \sum_{i=1}^{n} \textrm{Pr}\left[{s_i \text{ is the culprit}}\right] =
1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We represent this distribution with &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right]\)&lt;/span&gt; with &lt;span
class="math inline"&gt;\(i=1..(n+1)\)&lt;/span&gt; where &lt;span
class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{C_i}\right] &amp;amp;= \textrm{Pr}\left[{s_i \text{ is
the culprit}}\right], \; \forall \; i \in [1, n]
   \\
  \textrm{Pr}\left[{C_{n+1}}\right] &amp;amp;= \textrm{Pr}\left[{\text{there
is no culprit}}\right] \\
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Initially when the original failure is first detected at &lt;span
class="math inline"&gt;\(s_n\)&lt;/span&gt; and the search range is created, the
probability distribution is uniform: &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right] =
\frac{1}{(n+1)}\)&lt;/span&gt; for all &lt;span class="math inline"&gt;\(i \in [1,
n+1]\)&lt;/span&gt;. Now consider new evidence &lt;span
class="math inline"&gt;\(E\)&lt;/span&gt; arriving. In the Bayesian model, this
would &lt;em&gt;update&lt;/em&gt; our probability given this new evidence: &lt;span
class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | E}\right] = \frac{\textrm{Pr}\left[{E |
C_i}\right] \cdot
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{E}\right]}
  \label{eq:bayes}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above equation, the &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right]\)&lt;/span&gt; is the
&lt;em&gt;prior&lt;/em&gt; probability and the probability &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i | E}\right]\)&lt;/span&gt; is the
&lt;em&gt;posterior&lt;/em&gt; probability. In the culprit finding context, the new
evidence is typically newly observed test executions. However, there is
no reason we cannot take into account other information as well.&lt;/p&gt;
&lt;p&gt;In the Bayesian framework, new evidence is applied iteratively. In
each iteration, the prior probability is the posterior probability of
the previous iteration. Since multiplication is commutative, multiple
pieces of evidence can be applied in any order to arrive at the same
posterior probability: &lt;span class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{C_i | E_1, E_2}\right] &amp;amp;=
\frac{\textrm{Pr}\left[{E_2 |
C_i}\right]}{\textrm{Pr}\left[{E_2}\right]}
                         \frac{\textrm{Pr}\left[{E_1 |
C_i}\right]}{\textrm{Pr}\left[{E_1}\right]} \textrm{Pr}\left[{Ci}\right]
\\
                      &amp;amp;= \frac{\textrm{Pr}\left[{E_1 |
C_i}\right]}{\textrm{Pr}\left[{E_1}\right]}
                         \frac{\textrm{Pr}\left[{E_2 |
C_i}\right]}{\textrm{Pr}\left[{E_2}\right]} \textrm{Pr}\left[{Ci}\right]
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After the current posterior probability has been calculated, it is
used to select the next suspect to execute a test at. In the traditional
Bayes approach, the suspect to examine is the one with the maximum
posterior probability. However, in the context of a noisy binary search,
the suspects are ordered so a PASS at suspect &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt; indicates a PASS at all suspects
prior to &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt;. Following Ben Or and
Hassidim &lt;span class="citation" data-cites="Ben-Or2008"&gt;(&lt;a
href="#ref-Ben-Or2008" role="doc-biblioref"&gt;Ben-Or and Hassidim
2008&lt;/a&gt;)&lt;/span&gt;, we convert the posterior distribution to a cumulative
probability distribution and select the first suspect with a cumulative
probability &lt;span class="math inline"&gt;\(\geq 0.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id="flake-aware-culprit-finding"&gt;Flake Aware Culprit Finding&lt;/h1&gt;
&lt;p&gt;Now we introduce our algorithm, Flake Aware Culprit Finding (FACF),
in the Bayesian framework of Ben Or and Hassidim &lt;span class="citation"
data-cites="Ben-Or2008"&gt;(&lt;a href="#ref-Ben-Or2008"
role="doc-biblioref"&gt;Ben-Or and Hassidim 2008&lt;/a&gt;)&lt;/span&gt;. As above, let
&lt;span class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right]\)&lt;/span&gt; be
the probability that suspect &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; is
the culprit and &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_{n+1}}\right]\)&lt;/span&gt; be the
probability there is no culprit. These probabilities are initialized to
some suitable initial distribution such as the uniform distribution.&lt;/p&gt;
&lt;p&gt;To update the distribution with new evidence &lt;span
class="math inline"&gt;\(E\)&lt;/span&gt; in the form of a pass or fail for some
suspect &lt;span class="math inline"&gt;\(k\)&lt;/span&gt;, apply Bayes rule
(Equation &lt;a href="#eq:bayes" data-reference-type="ref"
data-reference="eq:bayes"&gt;[eq:bayes]&lt;/a&gt;). Note that &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{E}\right]\)&lt;/span&gt; can be
rewritten by Bayes rule in terms of a sum by marginalizing over the
likelihood that each suspect &lt;span class="math inline"&gt;\(j\)&lt;/span&gt;
could be the culprit: &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{E}\right] = \sum_{j=1}^{n} \textrm{Pr}\left[{E |
C_j}\right] \textrm{Pr}\left[{C_j}\right]\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are two types of evidence: test executions with state
&lt;code&gt;PASS&lt;/code&gt; or &lt;code&gt;FAIL&lt;/code&gt; at suspect &lt;span
class="math inline"&gt;\(i\)&lt;/span&gt;. We indicate these as the events &lt;span
class="math inline"&gt;\(P_i\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(F_i\)&lt;/span&gt; respectively. The marginalized
probability of a &lt;code&gt;PASS&lt;/code&gt; at &lt;span
class="math inline"&gt;\(i\)&lt;/span&gt; given there is a culprit at &lt;span
class="math inline"&gt;\(j\)&lt;/span&gt; is: &lt;span
class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{P_i | C_j}\right] =
    \begin{cases}
      1 - \hat{f}_t  &amp;amp; \text{if } i &amp;lt; j \\
      0              &amp;amp; \text{otherwise}
    \end{cases}
  \label{eq:pass}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The marginalized probability of a &lt;code&gt;FAIL&lt;/code&gt; at &lt;span
class="math inline"&gt;\(i\)&lt;/span&gt; given there is a culprit at &lt;span
class="math inline"&gt;\(j\)&lt;/span&gt; is: &lt;span
class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{F_i | C_j}\right] =
    \begin{cases}
      \hat{f}_t      &amp;amp; \text{if } i &amp;lt; j \\
      1              &amp;amp; \text{otherwise}
    \end{cases}
  \label{eq:fail}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With Equation &lt;a href="#eq:pass" data-reference-type="ref"
data-reference="eq:pass"&gt;[eq:pass]&lt;/a&gt;, we write a Bayesian update for
&lt;code&gt;PASS&lt;/code&gt; at suspect &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; as
the following. Given &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | P_k}\right] = \frac{\textrm{Pr}\left[{P_k |
C_i}\right] \textrm{Pr}\left[{C_i}\right]}{
\textrm{Pr}\left[{P_k}\right]
  }\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;let &lt;span class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{P_k}\right] &amp;amp;= \sum_{j=1}^{n}
\textrm{Pr}\left[{P_k | C_j}\right] \textrm{Pr}\left[{C_j}\right] \\
           &amp;amp;= \sum_{j=k+1}^{n} (1 - \hat{f}_t)
\textrm{Pr}\left[{C_j}\right]
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class="algorithm"&gt;

&lt;/div&gt;
&lt;div class="algorithm"&gt;
&lt;p&gt;cdf = cumulativeDistribution(dist) thresholds = &lt;span
class="math inline"&gt;\(\left\{ \frac{i}{(k+1)} \; | \; i=1..k
\right\}\)&lt;/span&gt; tidx = 0 runs = list() runs&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;then &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | P_k}\right] =
  \begin{cases}
    \frac{(1 - \hat{f}_t)
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{P_k}\right]} &amp;amp;
\text{if } k &amp;lt; i \\
    0                                         &amp;amp; \text{otherwise} \\
  \end{cases}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The update using Equation &lt;a href="#eq:fail"
data-reference-type="ref" data-reference="eq:fail"&gt;[eq:fail]&lt;/a&gt; for
&lt;code&gt;FAIL&lt;/code&gt; at suspect &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; is
similar. Given &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | F_k}\right] = \frac{\textrm{Pr}\left[{F_k |
C_i}\right] \textrm{Pr}\left[{C_i}\right]}{
    \sum_{j=1}^{n} \textrm{Pr}\left[{F_k | C_j}\right]
\textrm{Pr}\left[{C_j}\right]
  }\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;let &lt;span class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{F_k}\right] &amp;amp;= \sum_{j=1}^{n}
\textrm{Pr}\left[{F_k | C_j}\right] \textrm{Pr}\left[{C_j}\right] \\
           &amp;amp;= \sum_{j=1}^{k} \textrm{Pr}\left[{C_j}\right] +
\sum_{j=k+1}^{n} \hat{f}_t \textrm{Pr}\left[{C_j}\right]
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;then &lt;span class="math display"&gt;\[\begin{aligned}
\textrm{Pr}\left[{C_i | F_k}\right] =
  \begin{cases}
    \frac{\hat{f}_t
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{F_k}\right]} &amp;amp;
\text{if } k &amp;lt; i \\
    \frac{\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{F_k}\right]}           &amp;amp;
\text{otherwise} \\
  \end{cases}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;

&lt;h2 id="the-facf-algorithm"&gt;The FACF Algorithm&lt;/h2&gt;

&lt;p&gt;&lt;span id="alg:facf" label="alg:facf"&gt;
&lt;a href="images/icst-2023/alg2.png"&gt;&lt;img alt="Algorithm FACF" src="images/icst-2023/alg2.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id="alg:nextRuns" label="alg:nextRuns"&gt;
&lt;a href="images/icst-2023/alg3.png"&gt;&lt;img alt="Algorithm nextRuns" src="images/icst-2023/alg3.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Algorithm &lt;a href="#alg:facf" data-reference-type="ref"
data-reference="alg:facf"&gt;[alg:facf]&lt;/a&gt; presents the pseudo code for a
culprit finding algorithm based on the above Bayesian formulation. It
takes a prior distribution (the uniform distribution can be used), an
estimated flake rate &lt;span class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt;, and
the suspects. It returns the most likely culprit or &lt;code&gt;NONE&lt;/code&gt;
(in the case that it is most likely there is no culprit). The FACF
algorithm builds a set of evidence &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; containing the observed passes and
failures so far. At each step, that evidence is converted to a
distribution using the math presented above. Then, FACF calls Algorithm
&lt;a href="#alg:nextRuns" data-reference-type="ref"
data-reference="alg:nextRuns"&gt;[alg:nextRuns]&lt;/a&gt; &lt;code&gt;NextRuns&lt;/code&gt;
to compute the next suspect to run.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NextRuns&lt;/code&gt; converts the probability distribution into a
cumulative distribution. It then selects the suspects with cumulative
probability over a set of thresholds and returns them. The version of
&lt;code&gt;NextRuns&lt;/code&gt; we present here has been generalized to allow for
&lt;span class="math inline"&gt;\(k\)&lt;/span&gt; parallel runs and also contains a
minor optimization on lines 10-11. If the suspect &lt;span
class="math inline"&gt;\(s_{i-1}\)&lt;/span&gt; prior to the selected suspect
&lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; hasn’t been selected to run and
its cumulative probability is above &lt;span
class="math inline"&gt;\(0\)&lt;/span&gt; then the prior suspect &lt;span
class="math inline"&gt;\(s_{i-1}\)&lt;/span&gt; is selected in favor of &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt;. This optimization ensures that we
look for the passing run preceding the most likely culprit before
looking for a failing run at the culprit.&lt;/p&gt;
&lt;h2 id="sec:thresholds"&gt;Choosing the Thresholds in NextRuns&lt;/h2&gt;
&lt;p&gt;Another optimization can be made to NextRuns by changing the
thresholds used to select suspects. Instead of simply dividing the
probability space up evenly it uses the observation that the information
gain from a PASS is more than the gain from a FAIL. Modeling that as
&lt;span class="math inline"&gt;\(I(P_i) = 1\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(I(F_i) = 1 - \hat{f}_t\)&lt;/span&gt;, we can compute
the expected information gain for new evidence at suspect &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt; as &lt;span
class="math inline"&gt;\(\textrm{E}\left[{I(s_i)}\right] = I(P_i)P_i +
I(F_i)F_i\)&lt;/span&gt;. Expanding from the marginalized equations &lt;a
href="#eq:pass" data-reference-type="ref"
data-reference="eq:pass"&gt;[eq:pass]&lt;/a&gt; and &lt;a href="#eq:fail"
data-reference-type="ref" data-reference="eq:fail"&gt;[eq:fail]&lt;/a&gt; gives
&lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{E}\left[{I(s_i)}\right] = (1 - \hat{f}_t)(n + i \hat{f}_t -
1)\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Computing the cumulative expectation over &lt;span
class="math inline"&gt;\(\{ \textrm{E}\left[{I(s_1)}\right], ...,
\textrm{E}\left[{I(s_{n+1})}\right] \}\)&lt;/span&gt; and normalizing allows
us to select the suspect that maximizes the information gain. The
empirical evaluation (Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;)
examines the effect of this optimization. The importance of the supplied
flake rate estimate (&lt;span class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt;) is
also examined in Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="sec:priors"&gt;Prior Probabilities for FACF&lt;/h2&gt;
&lt;p&gt;The prior distribution supplied to Algorithm &lt;a href="#alg:facf"
data-reference-type="ref" data-reference="alg:facf"&gt;[alg:facf]&lt;/a&gt; does
not have to be a uniform distribution. Instead, it can be informed by
any other data the culprit finding system might find useful. At Google,
we are exploring many such sources of data, but so far are only using
two primary sources in production.&lt;/p&gt;
&lt;p&gt;First, we have always used the static build dependence graph to
filter out non-affecting suspects before culprit finding &lt;span
class="citation" data-cites="Gupta2011"&gt;(&lt;a href="#ref-Gupta2011"
role="doc-biblioref"&gt;Gupta, Ivey, and Penix 2011&lt;/a&gt;)&lt;/span&gt;. In FACF,
we take that a step further and, using a stale snapshot for scalability,
we compute the minimum distance in the build graph between the files
changed in each suspect and the test being culprit-found &lt;span
class="citation" data-cites="Memon2017"&gt;(&lt;a href="#ref-Memon2017"
role="doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;)&lt;/span&gt;. Using these
distances, we form a probability distribution where the suspects with
files closer to the test have a higher probability of being the culprit
&lt;span class="citation" data-cites="Ziftci2013a Ziftci2017"&gt;(&lt;a
href="#ref-Ziftci2013a" role="doc-biblioref"&gt;Ziftci and Ramavajjala
2013&lt;/a&gt;; &lt;a href="#ref-Ziftci2017" role="doc-biblioref"&gt;Ziftci and
Reardon 2017&lt;/a&gt;)&lt;/span&gt;. Second, we look at the historical likelihood
of a true breakage (versus a flaky one) to determine the probability
that there is no culprit. Combining that distribution with the one from
the minimum build distance forms a prior distribution we use for culprit
finding.&lt;/p&gt;
&lt;p&gt;Using this scheme, flaky tests tend to get reruns at the &lt;span
class="math inline"&gt;\(s_n\)&lt;/span&gt; (the suspect where the failure was
detected) as their first test executions during culprit finding. This is
intentional as a large fraction of the search ranges we culprit find
tend to be flaky and this reduces the cost of proving that. In the
empirical evaluation (Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;), we
examine the effect of the prior distribution on both cost and
accuracy.&lt;/p&gt;
&lt;h1 id="sec:verification"&gt;Culprit Verification for Accuracy
Evaluation&lt;/h1&gt;
&lt;p&gt;The previous sections detailed our culprit finding algorithm (FACF)
and the baseline algorithm we compare against (deflaked bisection). In
order to evaluate the accuracy of a culprit finding system for a
particular dataset, we need to know which commits are the actual culprit
commits. If the actual culprit commits are known, then we can compute
the usual measures (True Positive Rate, False Positive Rate, Accuracy,
Precision, Recall, etc...) &lt;span class="citation"
data-cites="Tharwat2021"&gt;(&lt;a href="#ref-Tharwat2021"
role="doc-biblioref"&gt;Tharwat 2021&lt;/a&gt;)&lt;/span&gt;. Previous work in culprit
finding, test case selection, fault localization and related research
areas have often used curated datasets (e.g. Defects4J &lt;span
class="citation" data-cites="Just2014"&gt;(&lt;a href="#ref-Just2014"
role="doc-biblioref"&gt;Just, Jalali, and Ernst 2014&lt;/a&gt;)&lt;/span&gt;), bug
databases &lt;span class="citation"
data-cites="Bhattacharya2010 Herzig2015 Najafi2019a"&gt;(&lt;a
href="#ref-Bhattacharya2010" role="doc-biblioref"&gt;Bhattacharya and
Neamtiu 2010&lt;/a&gt;; &lt;a href="#ref-Herzig2015" role="doc-biblioref"&gt;Herzig
et al. 2015&lt;/a&gt;; &lt;a href="#ref-Najafi2019a" role="doc-biblioref"&gt;Najafi,
Rigby, and Shang 2019&lt;/a&gt;)&lt;/span&gt;, and synthetic benchmarks with
injected bugs &lt;span class="citation"
data-cites="Henderson2018 Henderson2019"&gt;(&lt;a href="#ref-Henderson2018"
role="doc-biblioref"&gt;Henderson and Podgurski 2018&lt;/a&gt;; &lt;a
href="#ref-Henderson2019" role="doc-biblioref"&gt;Henderson, Podgurski, and
Kucuk 2019&lt;/a&gt;)&lt;/span&gt;. But our primary interest is in the accuracy of
the algorithms in the Google environment, not the accuracy on open
source bug databases, curated datasets, or synthetic bugs.&lt;/p&gt;
&lt;p&gt;At Google, we do not have a comprehensive database of developer
investigations of every continuous integration test failure – indeed
there are far too many failures per day for developers to examine and
evaluate every single one. We also didn’t want to completely rely on
human feedback to tell us when our culprit finding systems have
identified the wrong culprit – as had been previously done &lt;span
class="citation" data-cites="Ziftci2017"&gt;(&lt;a href="#ref-Ziftci2017"
role="doc-biblioref"&gt;Ziftci and Reardon 2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead, we wanted to be able to verify whether or not an identified
culprit for a particular test and search range is correct or not. The
verification needs to be automated such that the we can operationally
monitor the accuracy of the deployed system and catch any performance
regressions in the culprit finder. Continuous monitoring ensures a high
quality user experience by alerting our team to accuracy
degradations.&lt;/p&gt;
&lt;h2 id="culprit-finding-conclusions"&gt;Culprit Finding Conclusions&lt;/h2&gt;
&lt;p&gt;A culprit finder may draw one of two conclusions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;FI&lt;/strong&gt;: &lt;code&gt;FLAKE_IDENTIFIED&lt;/code&gt; &lt;em&gt;the
culprit finder determined the status transition was caused by
non-deterministic behavior.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;CVI&lt;/strong&gt;:
&lt;code&gt;CULPRIT_VERSION_IDENTIFIED(version)&lt;/code&gt; &lt;em&gt;the culprit finder
determined a specific &lt;code&gt;version&lt;/code&gt; was identified as the
culprit&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The verification system will label each conclusion examined with
either &lt;code&gt;CORRECT&lt;/code&gt; or &lt;code&gt;INCORRECT&lt;/code&gt; from which we can
define the standard counts for True and False Positives and Negatives.
Accuracy, Precision, and Recall can then be directly computed.&lt;/p&gt;
&lt;h2 id="when-is-a-culprit-incorrect"&gt;When is a Culprit Incorrect?&lt;/h2&gt;
&lt;p&gt;For ease of discussion, we will use the notation introduced in
Section &lt;a href="#sec:background:cf" data-reference-type="ref"
data-reference="sec:background:cf"&gt;2.2&lt;/a&gt; for the search range &lt;span
class="math inline"&gt;\(S\)&lt;/span&gt; with &lt;span
class="math inline"&gt;\(s_0\)&lt;/span&gt; indicating the previous passing
version, &lt;span class="math inline"&gt;\(s_n\)&lt;/span&gt; indicating the version
where the failure was first detected, and &lt;span
class="math inline"&gt;\(s_k\)&lt;/span&gt; indicating the identified culprit.
Now, consider the two cases&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;FLAKE_IDENTIFIED&lt;/code&gt; these are incorrect if no
subsequent runs pass at the version where the original failure was
detected (no flaky behavior can be reproduced).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;CULPRIT_VERSION_IDENTIFIED(version)&lt;/code&gt; these are
incorrect if we can prove any of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A pass at &lt;span class="math inline"&gt;\(s_n\)&lt;/span&gt; indicates the
failure was a flake.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A pass at the culprit &lt;span class="math inline"&gt;\(s_k\)&lt;/span&gt;
indicates that the culprit was incorrectly identified.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No passes detected at the version prior to the culprit, &lt;span
class="math inline"&gt;\(s_{k-1}\)&lt;/span&gt;, indicates that the culprit was
incorrectly identified.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additionally the following conditions indicate the culprit may have
been misidentified due to a test dependency on either time or some
unknown external factor:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;No passes are identified at &lt;span
class="math inline"&gt;\(s_0\)&lt;/span&gt;, indicating the test failure may be
time or environment dependent (and the conclusion cannot be
trusted).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No pass at &lt;span class="math inline"&gt;\(s_0\)&lt;/span&gt; is detected
prior (in the time dimension) to a failure at &lt;span
class="math inline"&gt;\(s_n\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No failure at &lt;span class="math inline"&gt;\(s_n\)&lt;/span&gt; is
detected prior (in the time dimension) to a pass at &lt;span
class="math inline"&gt;\(s_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No pass at &lt;span class="math inline"&gt;\(s_{k-1}\)&lt;/span&gt; is
detected prior (in the time dimension) to a failure at &lt;span
class="math inline"&gt;\(s_k\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No failure at &lt;span class="math inline"&gt;\(s_k\)&lt;/span&gt; is
detected prior (in the time dimension) to a pass at &lt;span
class="math inline"&gt;\(s_{k-1}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Verification is not reproducible 17 hours later.&lt;a href="#fn4"
class="footnote-ref" id="fnref4"
role="doc-noteref"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, our approach for determining whether or not a culprit is
correct is to do extensive reruns for a particular conclusion.&lt;/p&gt;
&lt;h2 id="how-many-reruns-to-conduct-to-verify-a-conclusion"&gt;How Many
Reruns to Conduct to Verify a Conclusion&lt;/h2&gt;
&lt;p&gt;A test run &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; times fails every
time with the rate &lt;span class="math inline"&gt;\(\hat{f}_t^x\)&lt;/span&gt;. If
we want to achieve confidence level &lt;span
class="math inline"&gt;\(C\)&lt;/span&gt; (ex. &lt;span class="math inline"&gt;\(C =
.99999999\)&lt;/span&gt;) of verifier correctness, we can compute the number
of reruns to conduct for each check with &lt;span class="math inline"&gt;\(x =
\left\lceil{\frac{\,\textrm{log}\!\!\left({1 -
C}\right)}{\,\textrm{log}\!\!\left({\hat{f}_t}\right)}}\right\rceil\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="sampling-strategy"&gt;Sampling Strategy&lt;/h2&gt;
&lt;p&gt;While it would be ideal to verify every single culprit for every
single test, the expense, in practice, of such an operation is too high.
We conduct verification with a very high confidence level (&lt;span
class="math inline"&gt;\(C = 0.99999999\)&lt;/span&gt;) and a minimum on the
estimated flakiness rate of &lt;span class="math inline"&gt;\(0.1\)&lt;/span&gt; to
ensure a minimum of 8 reruns for each check even for targets that have
not been historically flaky. With such an expensive configuration, we
can only afford the following sampling strategy.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Randomly sample at &lt;span class="math inline"&gt;\(1\%\)&lt;/span&gt; of
culprit finding conclusions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sample the first conclusion for each unique culprit
version.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We tag which sampling strategy was used to sample a particular
conclusion allowing us to compute statistics for each sampling frame
independently.&lt;/p&gt;
&lt;h2 id="the-verified-culprits-dataset"&gt;The Verified Culprits
Dataset&lt;/h2&gt;
&lt;p&gt;Using our production culprit finders based on the FACF algorithm and
a verifier implementing the above verification strategy, we have
produced an internal dataset with 144,130 verified conclusions in the
last 60 days, which we use in Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt; to
evaluate the performance of the algorithms presented in this paper.
Note, because the evaluation of an algorithm is also expensive, we only
conduct the evaluation on a subset of the whole dataset.&lt;/p&gt;
&lt;p&gt;While we are unable to make the dataset public, we encourage others
to use our verification approach on their own culprit finding systems
and when possible share these datasets with the external community for
use in future studies. Such datasets are not only useful for culprit
finding research but also for fault localization, test prioritization,
test selection, automatic program repair, and other topics.&lt;/p&gt;

&lt;h1 id="sec:evaluation"&gt;Empirical Evaluation&lt;/h1&gt;

&lt;p&gt;We empirically evaluated the behavior of the probabilistic Flake
Aware Culprit Finding (FACF) algorithm, compared it to the traditional
Bisect algorithm &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;,
evaluated the effectiveness of adding deflaking runs to Bisect, and
evaluated the effectiveness of two optimizations for FACF.&lt;/p&gt;
&lt;h2 id="research-questions"&gt;Research Questions&lt;/h2&gt;
&lt;p&gt;The following research questions were considered. All questions were
considered with respect to the Google environment (see Section &lt;a
href="#sec:dataset" data-reference-type="ref"
data-reference="sec:dataset"&gt;5.5&lt;/a&gt; for more details about the dataset
used).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Which algorithm (FACF or Bisect) was more accurate in identifying
culprit commits?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What difference do algorithmic tweaks (such as adding deflaking
runs to Bisect) make to accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How efficient is each algorithm as measured in number of test
executions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What difference do algorithmic tweaks (such as adding deflaking
runs to Bisect) make to efficiency.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Does flakiness affect culprit finding accuracy and cost?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="measuring-culprit-finding-accuracy"&gt;Measuring Culprit Finding
Accuracy&lt;/h2&gt;
&lt;p&gt;To measure the accuracy of the culprit finders we produce a binary
variable (“correct”) which is true if the culprit finder identified
version matched the culprit in the dataset. Accuracy is then defined as
the sample proportion: &lt;span class="math inline"&gt;\(\textbf{ACC} =
\frac{\text{\# correct}}{\text{Total}}\)&lt;/span&gt;. Culprit finder “A” is
judged more accurate than culprit finder “B” if A’s &lt;strong&gt;ACC&lt;/strong&gt;
value is higher than B’s and a statistical test (described in Section &lt;a
href="#sec:stat-tests" data-reference-type="ref"
data-reference="sec:stat-tests"&gt;5.4&lt;/a&gt;) determines the difference is
significant.&lt;/p&gt;
&lt;h2 id="measuring-culprit-finding-cost"&gt;Measuring Culprit Finding
Cost&lt;/h2&gt;
&lt;p&gt;We measure cost as the number of test executions performed by the
algorithm. Culprit finder “A” is judged lower cost than culprit finder
“B” if the mean number of test executions is less than “B” and a
statistical test (described in Section &lt;a href="#sec:stat-tests"
data-reference-type="ref" data-reference="sec:stat-tests"&gt;5.4&lt;/a&gt;)
determines the difference is significant.&lt;/p&gt;
&lt;h2 id="sec:stat-tests"&gt;On Our Usage of Statistical Tests&lt;/h2&gt;
&lt;p&gt;For answering RQ1 and RQ2, we observe the distribution per
culprit-finding algorithm of whether or not the algorithm identified the
culprit (0 or 1). For answering RQ3 and RQ4, we observe the distribution
per culprit-finding algorithm of the number of test executions required
to complete culprit finding.&lt;/p&gt;
&lt;p&gt;We follow the recommendations of McCrum-Gardner &lt;span
class="citation" data-cites="McCrum-Gardner2008"&gt;(&lt;a
href="#ref-McCrum-Gardner2008" role="doc-biblioref"&gt;McCrum-Gardner
2008&lt;/a&gt;)&lt;/span&gt; as well as Walpole &lt;em&gt;et al.&lt;/em&gt; &lt;span
class="citation" data-cites="Walpole2007"&gt;(&lt;a href="#ref-Walpole2007"
role="doc-biblioref"&gt;Walpole 2007&lt;/a&gt;)&lt;/span&gt; as to the proper test
statistics for our analysis. Since data points for a specific culprit
range have a correspondence across algorithms, we perform paired
statistical tests when feasible. The analysis is complicated by the
observation that neither the dichotomous distribution of correctness nor
the numerical distribution of number of test executions are normally
distributed. Following McCrum-Gardner we opt to use non-parametric
tests. Additionally different tests are used for accuracy (dichotomous
nominal variable) and number of executions (numerical variable)
respectively.&lt;/p&gt;
&lt;p&gt;As we have more than 2 groups to consider (10 algorithms), we
initially perform omnibus tests over all algorithms to justify the
existence of statistical significance over all algorithms. In response
to the multiple comparisons problem &lt;span class="citation"
data-cites="Walpole2007"&gt;(&lt;a href="#ref-Walpole2007"
role="doc-biblioref"&gt;Walpole 2007&lt;/a&gt;)&lt;/span&gt;, we compute the corrected
significance bound using the formula for the family-wise error rate. Our
experimental design has us first conduct the ANOVA significance test to
determine if any of the algorithms differed in performance. Then we
conduct post-hoc testing to determine which algorithm’s performance
differed. For each considered question we conduct &lt;span
class="math inline"&gt;\(\binom{10}{2} + 1\)&lt;/span&gt; significance tests. We
have &lt;span class="math inline"&gt;\(6\)&lt;/span&gt; considered questions which
we use significance testing to answer, giving us a total &lt;span
class="math inline"&gt;\(r =
6\left(\binom{10}{2} + 1\right) = 276\)&lt;/span&gt; tests.&lt;/p&gt;
&lt;p&gt;Solving for the &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; in the
experiment-wise error rate formula from Walpole (pg. 529 &lt;span
class="citation" data-cites="Walpole2007"&gt;(&lt;a href="#ref-Walpole2007"
role="doc-biblioref"&gt;Walpole 2007&lt;/a&gt;)&lt;/span&gt;) with &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt; standing for the probability of a false
rejection of at least one hypothesis in the experiment gives: &lt;span
class="math inline"&gt;\(\alpha = 1 - (1 - p)^{\frac{1}{r}}\)&lt;/span&gt;. Given
&lt;span class="math inline"&gt;\(r = 276\)&lt;/span&gt; tests and an intended
experiment-wise error rate of &lt;span class="math inline"&gt;\(p =
0.001\)&lt;/span&gt;, we conservatively arrive at &lt;span
class="math inline"&gt;\(\alpha =
10^{-6}\)&lt;/span&gt; after rounding down to control for family-wise error
rate. We use &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; as our
significance level in all tests.&lt;/p&gt;
&lt;p&gt;Cochran’s Q Test is used for significance testing of the accuracy
distributions and Friedman’s &lt;span
class="math inline"&gt;\(\chi^{2}\)&lt;/span&gt; Test for number of executions.
Post-hoc testing was conducted with McNemar’s test and Wilcoxon Signed
Rank Test respectively.&lt;/p&gt;
&lt;h2 id="sec:dataset"&gt;Evaluation Dataset&lt;/h2&gt;
&lt;p&gt;For the study we used a subset of the dataset produced by the
production Culprit Verifier (described in Section &lt;a
href="#sec:verification" data-reference-type="ref"
data-reference="sec:verification"&gt;4&lt;/a&gt;). The evaluation dataset
consists of 13600 test breakages with their required build flags, search
range of versions, and the verified culprit. The verified culprit may be
an indicator that there was no culprit and detected transition was
caused by a flake. Approximately 60% of the items had a culprit version
and 40% did not (as these were caused by a flaky failure).&lt;/p&gt;

&lt;p&gt;Multiple tests may be broken by the same version. When looking at
unique search ranges, we actually see approximately twice as many flaky
ranges as non-flaky ones. This hints towards having tighter bounds on
which ranges are worth culprit finding as all of the runs for two-thirds
of ranges would have been better utilized on more deflaking runs if
these two categories are accurately discernable.&lt;/p&gt;
&lt;p&gt;We therefore examine the overall dataset for discrepancies between
the two categories. For example, flaky ranges have, on average, 8 tests
failing while non-flaky ranges have 224. For the objective of finding
unique real culprits, we are interested in the minimum flake rate over
targets in a range for flaky and non-flaky ranges, respectively. Fig &lt;a
href="#fig:min-flake-rate" data-reference-type="ref"
data-reference="fig:min-flake-rate"&gt;[fig:min-flake-rate]&lt;/a&gt; shows us
that, in fact, 99% of real culprit ranges have targets below a flake
rate of 75%.&lt;/p&gt;

&lt;h2 id="results"&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;span id="fig:intervals" label="fig:intervals"&gt;
&lt;a href="images/icst-2023/fig2-4.png"&gt;&lt;img alt="Figure 2-4" src="images/icst-2023/fig2-4.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id="fig:min-flake-rate" label="fig:min-flake-rate"&gt;
&lt;a href="images/icst-2023/fig5.png"&gt;&lt;img alt="Figure 5" src="images/icst-2023/fig5.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq1-which-algorithm-facf-or-bisect-was-more-accurate-in-identifying-culprit-commits"&gt;RQ1:
Which algorithm (FACF or Bisect) was more accurate in identifying
culprit commits?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;All versions of FACF were more accurate overall than all versions of
Bisect (Figure &lt;a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:acc"&gt;2&lt;/a&gt;). The difference was
significant with a p-value &lt;span class="math inline"&gt;\(&amp;lt;
10^{-12}\)&lt;/span&gt;, below the adjusted &lt;span
class="math inline"&gt;\(10^{-6}\)&lt;/span&gt; significance level required.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq2-what-difference-do-algorithmic-tweaks-such-as-adding-deflaking-runs-to-bisect-make-to-accuracy"&gt;RQ2:
What difference do algorithmic tweaks (such as adding deflaking runs to
Bisect) make to accuracy?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;The production version of FACF, FACF(all), was most accurate and
significantly different than all other algorithms (Figure &lt;a
href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:acc"&gt;2&lt;/a&gt;). This was followed by
FACF(fr=.1) which set the floor of 10% on the estimated flake rate for
tests. The other FACF variants were equally accurate.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq3-how-efficient-is-each-algorithm-as-measured-in-number-of-test-executions"&gt;RQ3:
How efficient is each algorithm as measured in number of test
executions?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:builds"&gt;2&lt;/a&gt; shows the most efficient
algorithm was unsurprisingly Bisect(1) which does not attempt to do any
extra executions to deflake test failures. The next most efficient
algorithm was FACF(all) which was also the most accurate. As can be seen
in the boxplot in Figure &lt;a href="#fig:intervals"
data-reference-type="ref" data-reference="fig:intervals:builds"&gt;2&lt;/a&gt;,
the FACF variants tend to have many more extreme outliers in their cost
distribution than the Bisect variants. This is because they account for
the prior flake rate. An organization may control their cost by not
culprit finding tests which are more than (for instance) 50% flaky. No
such upper bound was established for this experiment.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq4-what-difference-do-algorithmic-tweaks-such-as-adding-deflaking-runs-to-bisect-make-to-efficiency"&gt;RQ4:
What difference do algorithmic tweaks (such as adding deflaking runs to
Bisect) make to efficiency?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;First, both FACF(all) and FACF(priors) had less cost than the
standard FACF. This indicates that while FACF(priors) does not improve
accuracy it does improve efficiency of the algorithm. In particular many
more culprits were found with just two builds.&lt;/p&gt;
&lt;p&gt;Second, FACF(heur-thresholds) had no significant effect on cost nor
did it show any improvement accuracy. We conclude that although the
mathematical motivation may be sound, in the Google environment, we
cannot validate a benefit to this optimization at this time.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq5-does-flakiness-affect-culprit-finding-accuracy-and-cost"&gt;RQ5:
Does flakiness affect culprit finding accuracy and cost?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;Figures &lt;a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals"&gt;3&lt;/a&gt; and &lt;a
href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals"&gt;4&lt;/a&gt; show the
accuracy and cost of culprit finding flaky and non-flaky culprits. For
search ranges without a culprit, FACF(all) clearly dominates Bisect in
accuracy and even has a small advantage in cost over Bisect(1).&lt;/p&gt;
&lt;p&gt;When there is a culprit, the FACF variants are all more accurate than
the Bisect variants according to the statistical tests. However, the
difference is much less pronounced with Bisect(1) accuracy &lt;span
class="math inline"&gt;\(\sim\)&lt;/span&gt;96.1% and FACF(all) accuracy &lt;span
class="math inline"&gt;\(\sim\)&lt;/span&gt;97.5%. In terms of cost, Bisect(1)
and Bisect(2) are the most efficient followed by the FACF variants.&lt;/p&gt;
&lt;p&gt;We will note, a production culprit finder cannot know a priori if a
breakage was caused by a flake. It would need to do deflaking runs to
establish that, costing approximately &lt;span
class="math inline"&gt;\(\left\lceil{\frac{\,\textrm{log}\!\!\left({p}\right)}{\,\textrm{log}\!\!\left({\hat{f}_t}\right)}}\right\rceil\)&lt;/span&gt;
where &lt;span class="math inline"&gt;\(p\)&lt;/span&gt; is your desired confidence
level. For &lt;span class="math inline"&gt;\(p=10^{-4}\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\hat{f} = .2\)&lt;/span&gt;, this amounts to 6 extra
runs, which would significantly increase the cost of bisection. However,
as noted in Section &lt;a href="#sec:dataset" data-reference-type="ref"
data-reference="sec:dataset"&gt;5.5&lt;/a&gt;, prior flakiness rates could be
used to prune out a significant portion of flaky ranges (lowering cost)
with a minimal hit to accuracy.&lt;/p&gt;

&lt;h1 id="related-work"&gt;Related Work&lt;/h1&gt;
&lt;h2 id="culprit-finding"&gt;Culprit Finding&lt;/h2&gt;
&lt;p&gt;Despite its industrial importance, culprit finding has been
relatively less studied in the literature in comparison to topics such
as Fault Localization &lt;span class="citation"
data-cites="Agarwal2014 Wong2016"&gt;(&lt;a href="#ref-Agarwal2014"
role="doc-biblioref"&gt;Agarwal and Agrawal 2014&lt;/a&gt;; &lt;a
href="#ref-Wong2016" role="doc-biblioref"&gt;Wong et al. 2016&lt;/a&gt;)&lt;/span&gt;
(coverage based &lt;span class="citation"
data-cites="Jones2002 Jones2005 Lucia2014 Henderson2018 Henderson2019 Kucuk2021"&gt;(&lt;a
href="#ref-Jones2002" role="doc-biblioref"&gt;J. a. Jones, Harrold, and
Stasko 2002&lt;/a&gt;; &lt;a href="#ref-Jones2005" role="doc-biblioref"&gt;J. A.
Jones and Harrold 2005&lt;/a&gt;; &lt;a href="#ref-Lucia2014"
role="doc-biblioref"&gt;Lucia et al. 2014&lt;/a&gt;; &lt;a href="#ref-Henderson2018"
role="doc-biblioref"&gt;Henderson and Podgurski 2018&lt;/a&gt;; &lt;a
href="#ref-Henderson2019" role="doc-biblioref"&gt;Henderson, Podgurski, and
Kucuk 2019&lt;/a&gt;; &lt;a href="#ref-Kucuk2021" role="doc-biblioref"&gt;Kucuk,
Henderson, and Podgurski 2021&lt;/a&gt;)&lt;/span&gt;, information retrieval based
&lt;span class="citation" data-cites="Zhou2012 Youm2015 Ciborowska2022"&gt;(&lt;a
href="#ref-Zhou2012" role="doc-biblioref"&gt;J. Zhou, Zhang, and Lo
2012&lt;/a&gt;; &lt;a href="#ref-Youm2015" role="doc-biblioref"&gt;Youm et al.
2015&lt;/a&gt;; &lt;a href="#ref-Ciborowska2022" role="doc-biblioref"&gt;Ciborowska
and Damevski 2022&lt;/a&gt;)&lt;/span&gt;, or slicing based &lt;span class="citation"
data-cites="Podgurski1990 Horwitz1992 Ren2004"&gt;(&lt;a
href="#ref-Podgurski1990" role="doc-biblioref"&gt;Podgurski and Clarke
1990&lt;/a&gt;; &lt;a href="#ref-Horwitz1992" role="doc-biblioref"&gt;Horwitz and
Reps 1992&lt;/a&gt;; &lt;a href="#ref-Ren2004" role="doc-biblioref"&gt;Ren et al.
2004&lt;/a&gt;)&lt;/span&gt;), Bug Prediction &lt;span class="citation"
data-cites="Lewis2013 Punitha2013 Osman2017"&gt;(&lt;a href="#ref-Lewis2013"
role="doc-biblioref"&gt;Lewis et al. 2013&lt;/a&gt;; &lt;a href="#ref-Punitha2013"
role="doc-biblioref"&gt;Punitha and Chitra 2013&lt;/a&gt;; &lt;a
href="#ref-Osman2017" role="doc-biblioref"&gt;Osman et al.
2017&lt;/a&gt;)&lt;/span&gt;, Test Selection &lt;span class="citation"
data-cites="Engstrom2010 pan2022"&gt;(&lt;a href="#ref-Engstrom2010"
role="doc-biblioref"&gt;Engström, Runeson, and Skoglund 2010&lt;/a&gt;; &lt;a
href="#ref-pan2022" role="doc-biblioref"&gt;Pan et al. 2022&lt;/a&gt;)&lt;/span&gt;,
Test Prioritization &lt;span class="citation"
data-cites="DeS.CamposJunior2017 pan2022"&gt;(&lt;a
href="#ref-DeS.CamposJunior2017" role="doc-biblioref"&gt;de S. Campos
Junior et al. 2017&lt;/a&gt;; &lt;a href="#ref-pan2022" role="doc-biblioref"&gt;Pan
et al. 2022&lt;/a&gt;)&lt;/span&gt; and the SZZ algorithm for identifying bug
inducing commits in the context of research studies &lt;span
class="citation"
data-cites="Sliwerski2005 Rodriguez-Perez2018 Borg2019 Wen2019"&gt;(&lt;a
href="#ref-Sliwerski2005" role="doc-biblioref"&gt;Śliwerski, Zimmermann,
and Zeller 2005&lt;/a&gt;; &lt;a href="#ref-Rodriguez-Perez2018"
role="doc-biblioref"&gt;Rodríguez-Pérez, Robles, and González-Barahona
2018&lt;/a&gt;; &lt;a href="#ref-Borg2019" role="doc-biblioref"&gt;Borg et al.
2019&lt;/a&gt;; &lt;a href="#ref-Wen2019" role="doc-biblioref"&gt;Wen et al.
2019&lt;/a&gt;)&lt;/span&gt;. This is perhaps understandable as the problem only
emerges in environments were it becomes to costly to run every test at
every code submission.&lt;/p&gt;
&lt;p&gt;The idea of using a binary search to locate the culprit is common,
but it is not completely clear who invented it first. The Christian
Couder of &lt;code&gt;git bisect&lt;/code&gt; wrote a comprehensive paper on its
implementation &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt; but
indicates there are other similar tools: “So what are the tools used to
fight regressions? [...] The only specific tools are test suites and
tools similar to &lt;code&gt;git bisect&lt;/code&gt;.” The first version of the
&lt;code&gt;git bisect&lt;/code&gt; command appears to have been written by Linus
Torvalds in 2005.&lt;a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; Torvald’s commit message simply
states “This is useful for doing binary searching for problems.” The
source code for the mercurial bisect command &lt;code&gt;hg bisect&lt;/code&gt;
states that it was inspired by the git command.&lt;a href="#fn6"
class="footnote-ref" id="fnref6" role="doc-noteref"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; The
Subversion (SVN) command, &lt;code&gt;svn bisect&lt;/code&gt;, also cites the git
command as inspiration.&lt;a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; Therefore, as far as the authors can
tell bisection for finding culprits appears to have been invented by
Torvalds.&lt;/p&gt;
&lt;p&gt;The Couder paper &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;
cites a GitHub repository for a program called “BBChop,” which is
lightly documented and appears not entirely completed by a user
enigmatically named Ealdwulf Wuffinga (likely a pseudonym, as that was a
name of the King of East Anglia from 664-713).&lt;a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;
This program claims to have also implemented a Bayesian search for
culprits. We gladly cede the throne for first invention of applying
Bayes rule to culprit finding to Ealdwulf and note that our invention
was independent of the work in BBChop.&lt;/p&gt;
&lt;p&gt;More recently, work has been emerging on the industrial practice of
Culprit Finding. In 2013, Ziftci and Ramavajjala gave a public talk at
the Google Test Automation Conference in on Culprit Finding at Google
&lt;span class="citation" data-cites="Ziftci2013a"&gt;(&lt;a
href="#ref-Ziftci2013a" role="doc-biblioref"&gt;Ziftci and Ramavajjala
2013&lt;/a&gt;)&lt;/span&gt;. They note, that by 2013 Google has already been using
&lt;span class="math inline"&gt;\(m\)&lt;/span&gt;-ary bisection based culprit
finding for “small” and “medium” tests. They then give a preview of an
approach elaborated on in Ziftci and Reardon’s 2017 paper &lt;span
class="citation" data-cites="Ziftci2017"&gt;(&lt;a href="#ref-Ziftci2017"
role="doc-biblioref"&gt;Ziftci and Reardon 2017&lt;/a&gt;)&lt;/span&gt; for culprit
finding integration tests where test executions take a long time to run.
The authors propose a suspiciousness metric based on the minimum build
dependence distance and conduct a case study on the efficacy of using
such a metric to surface potential culprits to developers. We utilize a
similar build dependence distance as input to construct a prior
probability (see Section &lt;a href="#sec:priors" data-reference-type="ref"
data-reference="sec:priors"&gt;3.3&lt;/a&gt;) in this work but do not directly
surface the metric to our end users. Finally, for ground truth they rely
on the developers to report what the culprit version is. In contrast,
the empirical evaluation we present in this new work is fully automated
and does not rely on developers.&lt;/p&gt;
&lt;p&gt;In 2017, Saha and Gligoric &lt;span class="citation"
data-cites="Saha2017"&gt;(&lt;a href="#ref-Saha2017" role="doc-biblioref"&gt;Saha
and Gligoric 2017&lt;/a&gt;)&lt;/span&gt; proposed accelerating the traditional
bisect algorithm via Test Selection techniques. They use Coverage Based
Test Selection &lt;span class="citation" data-cites="Zhou2010 Musa2015"&gt;(&lt;a
href="#ref-Zhou2010" role="doc-biblioref"&gt;Z. Q. Zhou 2010&lt;/a&gt;; &lt;a
href="#ref-Musa2015" role="doc-biblioref"&gt;Musa et al. 2015&lt;/a&gt;)&lt;/span&gt;
to choose which commits are most likely to affect the test results at
the failing commit. This technique can be viewed as fine grained,
dynamic version of the static build dependence selection strategy we
employ at Google.&lt;/p&gt;
&lt;p&gt;In 2019, Najafi &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"
data-cites="Najafi2019a"&gt;(&lt;a href="#ref-Najafi2019a"
role="doc-biblioref"&gt;Najafi, Rigby, and Shang 2019&lt;/a&gt;)&lt;/span&gt; looked at
a similar problem to the traditional culprit finding problem we examine
here. In their scenario, there is a submission queue that queues up
commits to integrate into the main development branch. It batches these
waiting commits and tests them together. If a test fails, they need to
culprit find to determine the culprit commit. The authors evaluate using
a ML-driven risk based batching approach. In 2022, the same research
group re-evaluated the 2019 paper on an open source dataset in
Beheshtian &lt;em&gt;et al.&lt;/em&gt;’s 2022 paper &lt;span class="citation"
data-cites="Beheshtian2022"&gt;(&lt;a href="#ref-Beheshtian2022"
role="doc-biblioref"&gt;Beheshtian, Bavand, and Rigby 2022&lt;/a&gt;)&lt;/span&gt;.
They found that in the new environment the risk based batching didn’t do
as well as a new and improved combinatorics based batching scheme for
identifying the culprit commits.&lt;/p&gt;
&lt;p&gt;James Keenan gave a presentation in 2019 at the Perl Conference in
which he presented a concept, &lt;em&gt;multisection&lt;/em&gt;, for dealing with
multiple bugs in the same search range &lt;span class="citation"
data-cites="keenan2019"&gt;(&lt;a href="#ref-keenan2019"
role="doc-biblioref"&gt;Keenan 2019&lt;/a&gt;)&lt;/span&gt;. Ideally, industrial
culprit finders would automatically detect that there are multiple
distinct bug inducing commits.&lt;/p&gt;
&lt;p&gt;In 2021, two papers looked at using coverage based data for
accelerating bug localization. An and Yoo &lt;span class="citation"
data-cites="An2021"&gt;(&lt;a href="#ref-An2021" role="doc-biblioref"&gt;An and
Yoo 2021&lt;/a&gt;)&lt;/span&gt; used coverage data to accelerate both bisection and
SZZ &lt;span class="citation" data-cites="Sliwerski2005"&gt;(&lt;a
href="#ref-Sliwerski2005" role="doc-biblioref"&gt;Śliwerski, Zimmermann,
and Zeller 2005&lt;/a&gt;)&lt;/span&gt; and found significant speedups. Wen &lt;em&gt;et
al.&lt;/em&gt; &lt;span class="citation" data-cites="Wen2021"&gt;(&lt;a
href="#ref-Wen2021" role="doc-biblioref"&gt;Wen et al. 2021&lt;/a&gt;)&lt;/span&gt;
combined traditional Coverage Based Statistical Fault Localization &lt;span
class="citation" data-cites="Jones2002"&gt;(&lt;a href="#ref-Jones2002"
role="doc-biblioref"&gt;J. a. Jones, Harrold, and Stasko 2002&lt;/a&gt;)&lt;/span&gt;
with historical information to create a new suspiciousness score &lt;span
class="citation" data-cites="Sun2016"&gt;(&lt;a href="#ref-Sun2016"
role="doc-biblioref"&gt;Sun and Podgurski 2016&lt;/a&gt;)&lt;/span&gt;. This fault
localization method could be used in a culprit finding context by using
the line rankings to rank the commits.&lt;/p&gt;
&lt;p&gt;Finally, in 2022 Frolin Ocariza &lt;span class="citation"
data-cites="Ocariza2022"&gt;(&lt;a href="#ref-Ocariza2022"
role="doc-biblioref"&gt;Ocariza 2022&lt;/a&gt;)&lt;/span&gt; published a paper on
bisecting performance regressions. There are strong connections between
a flaky test and a performance regression as a performance regression
may not happen 100% of the time. Furthermore, the “baseline” version may
randomly have poor performance due to environmental or other factors.
Ocariza also arrives at a probabilistic approach, using a Bayesian model
to determine whether a run at a particular version is exhibiting the
performance regression or not. While mathematically related to our work
here and complementary, the approach is distinct. The authors note that
their technique can be combined with a Noisy Binary Search citing Karp
&lt;span class="citation" data-cites="Karp2007"&gt;(&lt;a href="#ref-Karp2007"
role="doc-biblioref"&gt;Karp and Kleinberg 2007&lt;/a&gt;)&lt;/span&gt; or with a
multisection search citing Keenan &lt;span class="citation"
data-cites="keenan2019"&gt;(&lt;a href="#ref-keenan2019"
role="doc-biblioref"&gt;Keenan 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="noisy-binary-search"&gt;Noisy Binary Search&lt;/h2&gt;
&lt;p&gt;There is a vast literature on Rényi-Ulam games and Noisy Searching
problems (doubly so when considering their connection to error
correcting codes)! We direct readers to the Pelc survey &lt;span
class="citation" data-cites="Pelc2002"&gt;(&lt;a href="#ref-Pelc2002"
role="doc-biblioref"&gt;Pelc 2002&lt;/a&gt;)&lt;/span&gt; as a starting point. We will
highlight a few articles here. First and foremost, the Ben Or and
Hassidim paper &lt;span class="citation" data-cites="Ben-Or2008"&gt;(&lt;a
href="#ref-Ben-Or2008" role="doc-biblioref"&gt;Ben-Or and Hassidim
2008&lt;/a&gt;)&lt;/span&gt; from 2008 most clearly explains the problem in terms of
a Bayesian inference, which is the formulation we use here. Second,
Waeber &lt;em&gt;et al.&lt;/em&gt; in 2013 &lt;span class="citation"
data-cites="Waeber2013"&gt;(&lt;a href="#ref-Waeber2013"
role="doc-biblioref"&gt;Waeber, Frazier, and Henderson 2013&lt;/a&gt;)&lt;/span&gt;
discuss the theoretical foundations of these &lt;em&gt;probabilistic bisection
algorithms&lt;/em&gt;. Finally, according to Waeber, Horstein first described
this algorithm in the context of error correcting codes in 1963 &lt;span
class="citation" data-cites="Horstein1963"&gt;(&lt;a href="#ref-Horstein1963"
role="doc-biblioref"&gt;Horstein 1963&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id="sec:conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Flake Aware Culprit Finding (FACF) is both accurate, efficient, and
able to flexibly incorporate prior information on the location of a
culprit. A large scale empirical study on real test and build breakages
found FACF to be significantly more effective than a traditional
bisection search while not increasing cost over a deflaked
bisection.&lt;/p&gt;

&lt;h1 id="sec:refereces"&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography"&gt;
&lt;ol&gt;
&lt;li&gt;&lt;div id="ref-Agarwal2014" class="csl-entry" role="doc-biblioentry"&gt;
Agarwal, Pragya, and Arun Prakash Agrawal. 2014.
&lt;span&gt;“Fault-Localization &lt;span&gt;Techniques&lt;/span&gt; for &lt;span&gt;Software
Systems&lt;/span&gt;: &lt;span&gt;A Literature Review&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;SIGSOFT
Softw. Eng. Notes&lt;/em&gt; 39 (5): 1–8. &lt;a
href="https://doi.org/10.1145/2659118.2659125"&gt;https://doi.org/10.1145/2659118.2659125&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-An2021" class="csl-entry" role="doc-biblioentry"&gt;
An, Gabin, and Shin Yoo. 2021. &lt;span&gt;“Reducing the Search Space of Bug
Inducing Commits Using Failure Coverage.”&lt;/span&gt; In &lt;em&gt;Proceedings of
the 29th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on &lt;span&gt;European Software
Engineering Conference&lt;/span&gt; and &lt;span&gt;Symposium&lt;/span&gt; on the
&lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;,
1459–62. &lt;span&gt;Athens Greece&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3468264.3473129"&gt;https://doi.org/10.1145/3468264.3473129&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ananthanarayanan2019" class="csl-entry"
role="doc-biblioentry"&gt;
Ananthanarayanan, Sundaram, Masoud Saeida Ardekani, Denis Haenikel,
Balaji Varadarajan, Simon Soriano, Dhaval Patel, and Ali Reza
Adl-Tabatabai. 2019. &lt;span&gt;“Keeping Master Green at Scale.”&lt;/span&gt;
&lt;em&gt;Proceedings of the 14th EuroSys Conference 2019&lt;/em&gt;. &lt;a
href="https://doi.org/10.1145/3302424.3303970"&gt;https://doi.org/10.1145/3302424.3303970&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Beheshtian2022" class="csl-entry" role="doc-biblioentry"&gt;
Beheshtian, Mohammad Javad, Amir Hossein Bavand, and Peter C. Rigby.
2022. &lt;span&gt;“Software &lt;span&gt;Batch Testing&lt;/span&gt; to &lt;span&gt;Save Build
Test Resources&lt;/span&gt; and to &lt;span&gt;Reduce Feedback Time&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt; 48 (8): 2784–2801. &lt;a
href="https://doi.org/10.1109/TSE.2021.3070269"&gt;https://doi.org/10.1109/TSE.2021.3070269&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ben-Or2008" class="csl-entry" role="doc-biblioentry"&gt;
Ben-Or, Michael, and Avinatan Hassidim. 2008. &lt;span&gt;“The &lt;span&gt;Bayesian
Learner&lt;/span&gt; Is &lt;span&gt;Optimal&lt;/span&gt; for &lt;span&gt;Noisy Binary
Search&lt;/span&gt; (and &lt;span&gt;Pretty Good&lt;/span&gt; for &lt;span&gt;Quantum&lt;/span&gt; as
&lt;span&gt;Well&lt;/span&gt;).”&lt;/span&gt; In &lt;em&gt;2008 49th &lt;span&gt;Annual IEEE
Symposium&lt;/span&gt; on &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Computer
Science&lt;/span&gt;&lt;/em&gt;, 221–30. &lt;span&gt;Philadelphia, PA, USA&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/FOCS.2008.58"&gt;https://doi.org/10.1109/FOCS.2008.58&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Bhattacharya2010" class="csl-entry" role="doc-biblioentry"&gt;
Bhattacharya, Pamela, and Iulian Neamtiu. 2010. &lt;span&gt;“Fine-Grained
Incremental Learning and Multi-Feature Tossing Graphs to Improve Bug
Triaging.”&lt;/span&gt; In &lt;em&gt;2010 &lt;span&gt;IEEE International Conference&lt;/span&gt;
on &lt;span&gt;Software Maintenance&lt;/span&gt;&lt;/em&gt;, 1–10. &lt;span&gt;Timi oara,
Romania&lt;/span&gt;: &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSM.2010.5609736"&gt;https://doi.org/10.1109/ICSM.2010.5609736&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Binkley1992" class="csl-entry" role="doc-biblioentry"&gt;
Binkley, D. 1992. &lt;span&gt;“Using Semantic Differencing to Reduce the Cost
of Regression Testing.”&lt;/span&gt; In &lt;em&gt;Proceedings
&lt;span&gt;Conference&lt;/span&gt; on &lt;span&gt;Software Maintenance&lt;/span&gt; 1992&lt;/em&gt;,
41–50. &lt;span&gt;Orlando, FL, USA&lt;/span&gt;: &lt;span&gt;IEEE Comput. Soc.
Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSM.1992.242560"&gt;https://doi.org/10.1109/ICSM.1992.242560&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Borg2019" class="csl-entry" role="doc-biblioentry"&gt;
Borg, Markus, Oscar Svensson, Kristian Berg, and Daniel Hansson. 2019.
&lt;span&gt;“&lt;span&gt;SZZ&lt;/span&gt; Unleashed: An Open Implementation of the
&lt;span&gt;SZZ&lt;/span&gt; Algorithm - Featuring Example Usage in a Study of
Just-in-Time Bug Prediction for the &lt;span&gt;Jenkins&lt;/span&gt;
Project.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 3rd &lt;span&gt;ACM SIGSOFT
International Workshop&lt;/span&gt; on &lt;span&gt;Machine Learning
Techniques&lt;/span&gt; for &lt;span&gt;Software Quality Evaluation&lt;/span&gt; -
&lt;span&gt;MaLTeSQuE&lt;/span&gt; 2019&lt;/em&gt;, 7–12. &lt;span&gt;Tallinn, Estonia&lt;/span&gt;:
&lt;span&gt;ACM Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3340482.3342742"&gt;https://doi.org/10.1145/3340482.3342742&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ciborowska2022" class="csl-entry" role="doc-biblioentry"&gt;
Ciborowska, Agnieszka, and Kostadin Damevski. 2022. &lt;span&gt;“Fast
Changeset-Based Bug Localization with &lt;span&gt;BERT&lt;/span&gt;.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 44th &lt;span&gt;International Conference&lt;/span&gt; on
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 946–57. &lt;span&gt;Pittsburgh
Pennsylvania&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3510003.3510042"&gt;https://doi.org/10.1145/3510003.3510042&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Couder2008" class="csl-entry" role="doc-biblioentry"&gt;
Couder, Christian. 2008. &lt;span&gt;“Fighting Regressions with Git
Bisect.”&lt;/span&gt; &lt;em&gt;The Linux Kernel Archives&lt;/em&gt; 4 (5). &lt;a
href="https://www. kernel. org/pub/software/scm/git/doc s/git-
                  bisect-lk2009.html"&gt;https://www. kernel.
org/pub/software/scm/git/doc s/git- bisect-lk2009.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-DeS.CamposJunior2017" class="csl-entry"
role="doc-biblioentry"&gt;
de S. Campos Junior, Heleno, Marco Antônio P Araújo, José Maria N David,
Regina Braga, Fernanda Campos, and Victor Ströele. 2017. &lt;span&gt;“Test
&lt;span&gt;Case Prioritization&lt;/span&gt;: &lt;span&gt;A Systematic Review&lt;/span&gt; and
&lt;span&gt;Mapping&lt;/span&gt; of the &lt;span&gt;Literature&lt;/span&gt;.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 31st &lt;span&gt;Brazilian Symposium&lt;/span&gt; on
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 34–43. &lt;span&gt;New York, NY,
USA&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3131151.3131170"&gt;https://doi.org/10.1145/3131151.3131170&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Engstrom2010" class="csl-entry" role="doc-biblioentry"&gt;
Engström, Emelie, Per Runeson, and Mats Skoglund. 2010. &lt;span&gt;“A
Systematic Review on Regression Test Selection Techniques.”&lt;/span&gt;
&lt;em&gt;Information and Software Technology&lt;/em&gt; 52 (1): 14–30. &lt;a
href="https://doi.org/10.1016/j.infsof.2009.07.001"&gt;https://doi.org/10.1016/j.infsof.2009.07.001&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Fowler2006" class="csl-entry" role="doc-biblioentry"&gt;
Fowler, Martin. 2006. &lt;span&gt;“Continuous
&lt;span&gt;Integration&lt;/span&gt;.”&lt;/span&gt; &lt;a
href="https://martinfowler.com/articles/
                  continuousIntegration.html"&gt;https://martinfowler.com/articles/
continuousIntegration.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Gupta2011" class="csl-entry" role="doc-biblioentry"&gt;
Gupta, Pooja, Mark Ivey, and John Penix. 2011. &lt;span&gt;“Testing at the
Speed and Scale of &lt;span&gt;Google&lt;/span&gt;.”&lt;/span&gt; &lt;a
href="http://google-engtools.blogspot.com/2011/06/testing-at-
                  speed-and-scale-of-google.html"&gt;http://google-engtools.blogspot.com/2011/06/testing-at-
speed-and-scale-of-google.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Henderson2018" class="csl-entry" role="doc-biblioentry"&gt;
Henderson, Tim A. D., and Andy Podgurski. 2018. &lt;span&gt;“Behavioral
&lt;span&gt;Fault Localization&lt;/span&gt; by &lt;span&gt;Sampling Suspicious Dynamic
Control Flow Subgraphs&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2018 &lt;span&gt;IEEE&lt;/span&gt;
11th &lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Software
Testing&lt;/span&gt;, &lt;span&gt;Verification&lt;/span&gt; and &lt;span&gt;Validation&lt;/span&gt;
(&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;, 93–104. &lt;span&gt;Vasteras&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICST.2018.00019"&gt;https://doi.org/10.1109/ICST.2018.00019&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Henderson2019" class="csl-entry" role="doc-biblioentry"&gt;
Henderson, Tim A. D., Andy Podgurski, and Yigit Kucuk. 2019.
&lt;span&gt;“Evaluating &lt;span&gt;Automatic Fault Localization Using Markov
Processes&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019 19th &lt;span&gt;International Working
Conference&lt;/span&gt; on &lt;span&gt;Source Code Analysis&lt;/span&gt; and
&lt;span&gt;Manipulation&lt;/span&gt; (&lt;span&gt;SCAM&lt;/span&gt;)&lt;/em&gt;, 115–26.
&lt;span&gt;Cleveland, OH, USA&lt;/span&gt;: &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/SCAM.2019.00021"&gt;https://doi.org/10.1109/SCAM.2019.00021&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Herzig2015" class="csl-entry" role="doc-biblioentry"&gt;
Herzig, Kim, Michaela Greiler, Jacek Czerwonka, and Brendan Murphy.
2015. &lt;span&gt;“The &lt;span&gt;Art&lt;/span&gt; of &lt;span&gt;Testing Less&lt;/span&gt; Without
&lt;span&gt;Sacrificing Quality&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 37th &lt;span&gt;IEEE International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 1:483–93.
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE.2015.66"&gt;https://doi.org/10.1109/ICSE.2015.66&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Herzig2015a" class="csl-entry" role="doc-biblioentry"&gt;
Herzig, Kim, and Nachiappan Nagappan. 2015. &lt;span&gt;“Empirically
&lt;span&gt;Detecting False Test Alarms Using Association
Rules&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt;
37th &lt;span&gt;IEEE International Conference&lt;/span&gt; on &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 39–48. &lt;span&gt;Florence, Italy&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE.2015.133"&gt;https://doi.org/10.1109/ICSE.2015.133&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Horstein1963" class="csl-entry" role="doc-biblioentry"&gt;
Horstein, M. 1963. &lt;span&gt;“Sequential Transmission Using Noiseless
Feedback.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 9
(3): 136–43. &lt;a
href="https://doi.org/10.1109/TIT.1963.1057832"&gt;https://doi.org/10.1109/TIT.1963.1057832&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Horwitz1992" class="csl-entry" role="doc-biblioentry"&gt;
Horwitz, S, and T Reps. 1992. &lt;span&gt;“The Use of Program Dependence
Graphs in Software Engineering.”&lt;/span&gt; In &lt;em&gt;International
&lt;span&gt;Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;,
9:349. &lt;span&gt;Springer&lt;/span&gt;. &lt;a
href="http://portal.acm.org/citation.cfm?id=24041&amp;amp;amp;dl="&gt;http://portal.acm.org/citation.cfm?id=24041&amp;amp;amp;dl=&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Jones2002" class="csl-entry" role="doc-biblioentry"&gt;
Jones, J.a., M. J. Harrold, and J. Stasko. 2002. &lt;span&gt;“Visualization of
Test Information to Assist Fault Localization.”&lt;/span&gt; &lt;em&gt;Proceedings
of the 24th International Conference on Software Engineering. ICSE
2002&lt;/em&gt;. &lt;a
href="https://doi.org/10.1145/581339.581397"&gt;https://doi.org/10.1145/581339.581397&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Jones2005" class="csl-entry" role="doc-biblioentry"&gt;
Jones, James A., and Mary Jean Harrold. 2005. &lt;span&gt;“Empirical
&lt;span&gt;Evaluation&lt;/span&gt; of the &lt;span class="nocase"&gt;Tarantula Automatic
Fault-localization Technique&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the
20th &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM International Conference&lt;/span&gt; on
&lt;span&gt;Automated Software Engineering&lt;/span&gt;&lt;/em&gt;, 273–82. &lt;span&gt;New
York, NY, USA&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/1101908.1101949"&gt;https://doi.org/10.1145/1101908.1101949&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Just2014" class="csl-entry" role="doc-biblioentry"&gt;
Just, René, Darioush Jalali, and Michael D Ernst. 2014.
&lt;span&gt;“&lt;span&gt;Defects4J&lt;/span&gt;: &lt;span&gt;A Database&lt;/span&gt; of &lt;span&gt;Existing
Faults&lt;/span&gt; to &lt;span&gt;Enable Controlled Testing Studies&lt;/span&gt; for
&lt;span&gt;Java Programs&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 2014
&lt;span&gt;International Symposium&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;
and &lt;span&gt;Analysis&lt;/span&gt;&lt;/em&gt;, 437–40. &lt;span&gt;New York, NY, USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/2610384.2628055"&gt;https://doi.org/10.1145/2610384.2628055&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Karp2007" class="csl-entry" role="doc-biblioentry"&gt;
Karp, Richard M., and Robert Kleinberg. 2007. &lt;span&gt;“Noisy Binary Search
and Its Applications.”&lt;/span&gt; In &lt;em&gt;Proceedings of the Eighteenth
Annual &lt;span&gt;ACM-SIAM&lt;/span&gt; Symposium on Discrete Algorithms&lt;/em&gt;,
881–90. &lt;span&gt;SODA&lt;/span&gt; ’07. &lt;span&gt;USA&lt;/span&gt;: &lt;span&gt;Society for
Industrial and Applied Mathematics&lt;/span&gt;. &lt;a
href="https://dl.acm.org/doi/abs/10.5555/1283383.1283478"&gt;https://dl.acm.org/doi/abs/10.5555/1283383.1283478&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-keenan2019" class="csl-entry" role="doc-biblioentry"&gt;
Keenan, James. 2019. &lt;span&gt;“James &lt;span&gt;E&lt;/span&gt;. &lt;span&gt;Keenan&lt;/span&gt; -
"&lt;span&gt;Multisection&lt;/span&gt;: &lt;span&gt;When Bisection Isn&lt;/span&gt;’t
&lt;span&gt;Enough&lt;/span&gt; to &lt;span&gt;Debug&lt;/span&gt; a
&lt;span&gt;Problem&lt;/span&gt;".”&lt;/span&gt; &lt;a
href="https://www.youtube.com/watch?v=05CwdTRt6AM"&gt;https://www.youtube.com/watch?v=05CwdTRt6AM&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Kowalczyk2020" class="csl-entry" role="doc-biblioentry"&gt;
Kowalczyk, Emily, Karan Nair, Zebao Gao, Leo Silberstein, Teng Long, and
Atif Memon. 2020. &lt;span&gt;“Modeling and Ranking Flaky Tests at
&lt;span&gt;Apple&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the
&lt;span&gt;ACM&lt;/span&gt;/&lt;span&gt;IEEE&lt;/span&gt; 42nd &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;&lt;/em&gt;, 110–19. &lt;span&gt;Seoul
South Korea&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3377813.3381370"&gt;https://doi.org/10.1145/3377813.3381370&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Kucuk2021" class="csl-entry" role="doc-biblioentry"&gt;
Kucuk, Yigit, Tim A. D. Henderson, and Andy Podgurski. 2021.
&lt;span&gt;“Improving &lt;span&gt;Fault Localization&lt;/span&gt; by &lt;span&gt;Integrating
Value&lt;/span&gt; and &lt;span&gt;Predicate Based Causal Inference
Techniques&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2021
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 43rd &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;
(&lt;span&gt;ICSE&lt;/span&gt;)&lt;/em&gt;, 649–60. &lt;span&gt;Madrid, ES&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE43902.2021.00066"&gt;https://doi.org/10.1109/ICSE43902.2021.00066&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Leong2019" class="csl-entry" role="doc-biblioentry"&gt;
Leong, Claire, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John
Micco. 2019. &lt;span&gt;“Assessing &lt;span&gt;Transition-Based Test Selection
Algorithms&lt;/span&gt; at &lt;span&gt;Google&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 101–10. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00019"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00019&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Lewis2013" class="csl-entry" role="doc-biblioentry"&gt;
Lewis, Chris, Zhongpeng Lin, Caitlin Sadowski, and Xiaoyan Zhu. 2013.
&lt;span&gt;“Does Bug Prediction Support Human Developers? Findings from a
Google Case Study.”&lt;/span&gt; In &lt;em&gt;Proceedings of the …&lt;/em&gt;, 372–81. &lt;a
href="http://dl.acm.org/citation.cfm?id=2486838"&gt;http://dl.acm.org/citation.cfm?id=2486838&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Lucia2014" class="csl-entry" role="doc-biblioentry"&gt;
Lucia, David Lo, Lingxiao Jiang, Ferdian Thung, and Aditya Budi. 2014.
&lt;span&gt;“Extended Comprehensive Study of Association Measures for Fault
Localization.”&lt;/span&gt; &lt;em&gt;Journal of Software: Evolution and
Process&lt;/em&gt; 26 (2): 172–219. &lt;a
href="https://doi.org/10.1002/smr.1616"&gt;https://doi.org/10.1002/smr.1616&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Machalica2020" class="csl-entry" role="doc-biblioentry"&gt;
Machalica, Mateusz, Wojtek Chmiel, Stanislaw Swierc, and Ruslan
Sakevych. 2020. &lt;span&gt;“Probabilistic &lt;span&gt;Flakiness&lt;/span&gt;:
&lt;span&gt;How&lt;/span&gt; Do You Test Your Tests?”&lt;/span&gt; &lt;em&gt;DEVINFRA&lt;/em&gt;. &lt;a
href="https://engineering.fb.com/2020/12/10/developer-tools/
                  probabilistic-flakiness/"&gt;https://engineering.fb.com/2020/12/10/developer-tools/
probabilistic-flakiness/&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Machalica2019" class="csl-entry" role="doc-biblioentry"&gt;
Machalica, Mateusz, Alex Samylkin, Meredith Porth, and Satish Chandra.
2019. &lt;span&gt;“Predictive &lt;span&gt;Test Selection&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 91–100. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00018"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00018&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-McCrum-Gardner2008" class="csl-entry"
role="doc-biblioentry"&gt;
McCrum-Gardner, Evie. 2008. &lt;span&gt;“Which Is the Correct Statistical Test
to Use?”&lt;/span&gt; &lt;em&gt;British Journal of Oral and Maxillofacial
Surgery&lt;/em&gt; 46 (1): 38–41. &lt;a
href="https://doi.org/10.1016/j.bjoms.2007.09.002"&gt;https://doi.org/10.1016/j.bjoms.2007.09.002&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Memon2017" class="csl-entry" role="doc-biblioentry"&gt;
Memon, Atif, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob
Siemborski, and John Micco. 2017. &lt;span&gt;“Taming &lt;span
class="nocase"&gt;Google-scale&lt;/span&gt; Continuous Testing.”&lt;/span&gt; In
&lt;em&gt;2017 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 39th &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice Track&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 233–42. &lt;span&gt;Piscataway, NJ, USA&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2017.16"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.16&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Micco2012" class="csl-entry" role="doc-biblioentry"&gt;
Micco, John. 2012. &lt;span&gt;“Tools for &lt;span&gt;Continuous Integration&lt;/span&gt;
at &lt;span&gt;Google Scale&lt;/span&gt;.”&lt;/span&gt; Tech {{Talk}}. &lt;span&gt;Google
NYC&lt;/span&gt;. &lt;a
href="https://youtu.be/KH2_sB1A6lA"&gt;https://youtu.be/KH2_sB1A6lA&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Micco2013" class="csl-entry" role="doc-biblioentry"&gt;
———. 2013. &lt;span&gt;“Continuous &lt;span&gt;Integration&lt;/span&gt; at &lt;span&gt;Google
Scale&lt;/span&gt;.”&lt;/span&gt; Lecture. &lt;span&gt;EclipseCon 2013&lt;/span&gt;. &lt;a
href="https://web.archive.org/web/20140705215747/https://
                  www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
                  03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf"&gt;https://web.archive.org/web/20140705215747/https://
www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Musa2015" class="csl-entry" role="doc-biblioentry"&gt;
Musa, Samaila, Abu Bakar Md Sultan, Abdul Azim Bin Abd-Ghani, and Salmi
Baharom. 2015. &lt;span&gt;“Regression &lt;span&gt;Test Cases&lt;/span&gt; Selection for
&lt;span&gt;Object-Oriented Programs&lt;/span&gt; Based on &lt;span&gt;Affected
Statements&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;International Journal of Software
Engineering and Its Applications&lt;/em&gt; 9 (10): 91–108. &lt;a
href="https://doi.org/10.14257/ijseia.2015.9.10.10"&gt;https://doi.org/10.14257/ijseia.2015.9.10.10&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Najafi2019a" class="csl-entry" role="doc-biblioentry"&gt;
Najafi, Armin, Peter C. Rigby, and Weiyi Shang. 2019. &lt;span&gt;“Bisecting
Commits and Modeling Commit Risk During Testing.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on
&lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
&lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 279–89. &lt;span&gt;New York, NY, USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3338906.3338944"&gt;https://doi.org/10.1145/3338906.3338944&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ocariza2022" class="csl-entry" role="doc-biblioentry"&gt;
Ocariza, Frolin S. 2022. &lt;span&gt;“On the &lt;span&gt;Effectiveness&lt;/span&gt; of
&lt;span&gt;Bisection&lt;/span&gt; in &lt;span&gt;Performance Regression
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Empirical Software Engineering&lt;/em&gt; 27
(4): 95. &lt;a
href="https://doi.org/10.1007/s10664-022-10152-3"&gt;https://doi.org/10.1007/s10664-022-10152-3&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Osman2017" class="csl-entry" role="doc-biblioentry"&gt;
Osman, Haidar, Mohammad Ghafari, Oscar Nierstrasz, and Mircea Lungu.
2017. &lt;span&gt;“An &lt;span&gt;Extensive Analysis&lt;/span&gt; of &lt;span&gt;Efficient Bug
Prediction Configurations&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 13th
&lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Predictive Models&lt;/span&gt;
and &lt;span&gt;Data Analytics&lt;/span&gt; in &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 107–16. &lt;span&gt;Toronto Canada&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3127005.3127017"&gt;https://doi.org/10.1145/3127005.3127017&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-pan2022" class="csl-entry" role="doc-biblioentry"&gt;
Pan, Rongqi, Mojtaba Bagherzadeh, Taher A. Ghaleb, and Lionel Briand.
2022. &lt;span&gt;“Test Case Selection and Prioritization Using Machine
Learning: A Systematic Literature Review.”&lt;/span&gt; &lt;em&gt;Empirical Software
Engineering&lt;/em&gt; 27 (2): 29. &lt;a
href="https://doi.org/10.1007/s10664-021-10066-6"&gt;https://doi.org/10.1007/s10664-021-10066-6&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Pelc2002" class="csl-entry" role="doc-biblioentry"&gt;
Pelc, Andrzej. 2002. &lt;span&gt;“Searching Games with Errorsfifty Years of
Coping with Liars.”&lt;/span&gt; &lt;em&gt;Theoretical Computer Science&lt;/em&gt; 270
(1-2): 71–109. &lt;a
href="https://doi.org/10.1016/S0304-3975(01)00303-6"&gt;https://doi.org/10.1016/S0304-3975(01)00303-6&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Podgurski1990" class="csl-entry" role="doc-biblioentry"&gt;
Podgurski, A, and L A Clarke. 1990. &lt;span&gt;“A &lt;span&gt;Formal Model&lt;/span&gt;
of &lt;span&gt;Program Dependences&lt;/span&gt; and &lt;span&gt;Its Implications&lt;/span&gt;
for &lt;span&gt;Software Testing&lt;/span&gt;, &lt;span&gt;Debugging&lt;/span&gt;, and
&lt;span&gt;Maintenance&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions of Software
Engineering&lt;/em&gt; 16 (9): 965–79. &lt;a
href="https://doi.org/10.1109/32.58784"&gt;https://doi.org/10.1109/32.58784&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Potvin2016" class="csl-entry" role="doc-biblioentry"&gt;
Potvin, Rachel, and Josh Levenberg. 2016. &lt;span&gt;“Why Google Stores
Billions of Lines of Code in a Single Repository.”&lt;/span&gt;
&lt;em&gt;Communications of the ACM&lt;/em&gt; 59 (7): 78–87. &lt;a
href="https://doi.org/10.1145/2854146"&gt;https://doi.org/10.1145/2854146&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Punitha2013" class="csl-entry" role="doc-biblioentry"&gt;
Punitha, K., and S. Chitra. 2013. &lt;span&gt;“Software Defect Prediction
Using Software Metrics - &lt;span&gt;A&lt;/span&gt; Survey.”&lt;/span&gt; In &lt;em&gt;2013
&lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Information
Communication&lt;/span&gt; and &lt;span&gt;Embedded Systems&lt;/span&gt;
(&lt;span&gt;ICICES&lt;/span&gt;)&lt;/em&gt;, 555–58. &lt;span&gt;Chennai&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICICES.2013.6508369"&gt;https://doi.org/10.1109/ICICES.2013.6508369&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ren2004" class="csl-entry" role="doc-biblioentry"&gt;
Ren, Xiaoxia, Fenil Shah, Frank Tip, Barbara G. Ryder, and Ophelia
Chesley. 2004. &lt;span&gt;“Chianti: A Tool for Change Impact Analysis of Java
Programs.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 19th Annual &lt;span&gt;ACM
SIGPLAN&lt;/span&gt; Conference on &lt;span class="nocase"&gt;Object-oriented&lt;/span&gt;
Programming, Systems, Languages, and Applications&lt;/em&gt;, 432–48.
&lt;span&gt;Vancouver BC Canada&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/1028976.1029012"&gt;https://doi.org/10.1145/1028976.1029012&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Rivest1978" class="csl-entry" role="doc-biblioentry"&gt;
Rivest, R. L., A. R. Meyer, and D. J. Kleitman. 1978. &lt;span&gt;“Coping with
Errors in Binary Search Procedures (&lt;span&gt;Preliminary
Report&lt;/span&gt;).”&lt;/span&gt; In &lt;em&gt;Proceedings of the Tenth Annual
&lt;span&gt;ACM&lt;/span&gt; Symposium on &lt;span&gt;Theory&lt;/span&gt; of Computing -
&lt;span&gt;STOC&lt;/span&gt; ’78&lt;/em&gt;, 227–32. &lt;span&gt;San Diego, California, United
States&lt;/span&gt;: &lt;span&gt;ACM Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/800133.804351"&gt;https://doi.org/10.1145/800133.804351&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Rodriguez-Perez2018" class="csl-entry"
role="doc-biblioentry"&gt;
Rodríguez-Pérez, Gema, Gregorio Robles, and Jesús M. González-Barahona.
2018. &lt;span&gt;“Reproducibility and Credibility in Empirical Software
Engineering: &lt;span&gt;A&lt;/span&gt; Case Study Based on a Systematic Literature
Review of the Use of the &lt;span&gt;SZZ&lt;/span&gt; Algorithm.”&lt;/span&gt;
&lt;em&gt;Information and Software Technology&lt;/em&gt; 99 (July): 164–76. &lt;a
href="https://doi.org/10.1016/j.infsof.2018.03.009"&gt;https://doi.org/10.1016/j.infsof.2018.03.009&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Saha2017" class="csl-entry" role="doc-biblioentry"&gt;
Saha, Ripon, and Milos Gligoric. 2017. &lt;span&gt;“Selective &lt;span&gt;Bisection
Debugging&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Fundamental &lt;span&gt;Approaches&lt;/span&gt; to
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, edited by Marieke Huisman and
Julia Rubin, 10202:60–77. &lt;span&gt;Berlin, Heidelberg&lt;/span&gt;:
&lt;span&gt;Springer Berlin Heidelberg&lt;/span&gt;. &lt;a
href="https://doi.org/10.1007/978-3-662-54494-5_4"&gt;https://doi.org/10.1007/978-3-662-54494-5_4&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Sliwerski2005" class="csl-entry" role="doc-biblioentry"&gt;
Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller. 2005.
&lt;span&gt;“When Do Changes Induce Fixes?”&lt;/span&gt; &lt;em&gt;ACM SIGSOFT Software
Engineering Notes&lt;/em&gt; 30 (4): 1. &lt;a
href="https://doi.org/10.1145/1082983.1083147"&gt;https://doi.org/10.1145/1082983.1083147&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Sun2016" class="csl-entry" role="doc-biblioentry"&gt;
Sun, Shih-Feng, and Andy Podgurski. 2016. &lt;span&gt;“Properties of
&lt;span&gt;Effective Metrics&lt;/span&gt; for &lt;span&gt;Coverage-Based Statistical
Fault Localization&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2016 &lt;span&gt;IEEE International
Conference&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;,
&lt;span&gt;Verification&lt;/span&gt; and &lt;span&gt;Validation&lt;/span&gt;
(&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;, 124–34. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICST.2016.31"&gt;https://doi.org/10.1109/ICST.2016.31&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Tharwat2021" class="csl-entry" role="doc-biblioentry"&gt;
Tharwat, Alaa. 2021. &lt;span&gt;“Classification Assessment Methods.”&lt;/span&gt;
&lt;em&gt;Applied Computing and Informatics&lt;/em&gt; 17 (1): 168–92. &lt;a
href="https://doi.org/10.1016/j.aci.2018.08.003"&gt;https://doi.org/10.1016/j.aci.2018.08.003&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Tip1995" class="csl-entry" role="doc-biblioentry"&gt;
Tip, Frank. 1995. &lt;span&gt;“A Survey of Program Slicing Techniques.”&lt;/span&gt;
&lt;em&gt;Journal of Programming Languages&lt;/em&gt; 3 (3): 121–89.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Waeber2013" class="csl-entry" role="doc-biblioentry"&gt;
Waeber, Rolf, Peter I. Frazier, and Shane G. Henderson. 2013.
&lt;span&gt;“Bisection &lt;span&gt;Search&lt;/span&gt; with &lt;span&gt;Noisy
Responses&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;SIAM Journal on Control and
Optimization&lt;/em&gt; 51 (3): 2261–79. &lt;a
href="https://doi.org/10.1137/120861898"&gt;https://doi.org/10.1137/120861898&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Walpole2007" class="csl-entry" role="doc-biblioentry"&gt;
Walpole, Ronald E., ed. 2007. &lt;em&gt;Probability &amp;amp; Statistics for
Engineers &amp;amp; Scientists&lt;/em&gt;. 8th ed. &lt;span&gt;Upper Saddle River,
NJ&lt;/span&gt;: &lt;span&gt;Pearson Prentice Hall&lt;/span&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wang2020" class="csl-entry" role="doc-biblioentry"&gt;
Wang, Kaiyuan, Greg Tener, Vijay Gullapalli, Xin Huang, Ahmed Gad, and
Daniel Rall. 2020. &lt;span&gt;“Scalable Build Service System with Smart
Scheduling Service.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 29th &lt;span&gt;ACM
SIGSOFT International Symposium&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;
and &lt;span&gt;Analysis&lt;/span&gt;&lt;/em&gt;, 452–62. &lt;span&gt;Virtual Event USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3395363.3397371"&gt;https://doi.org/10.1145/3395363.3397371&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wen2021" class="csl-entry" role="doc-biblioentry"&gt;
Wen, Ming, Junjie Chen, Yongqiang Tian, Rongxin Wu, Dan Hao, Shi Han,
and Shing-Chi Cheung. 2021. &lt;span&gt;“Historical &lt;span&gt;Spectrum Based Fault
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software
Engineering&lt;/em&gt; 47 (11): 2348–68. &lt;a
href="https://doi.org/10.1109/TSE.2019.2948158"&gt;https://doi.org/10.1109/TSE.2019.2948158&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wen2019" class="csl-entry" role="doc-biblioentry"&gt;
Wen, Ming, Rongxin Wu, Yepang Liu, Yongqiang Tian, Xuan Xie, Shing-Chi
Cheung, and Zhendong Su. 2019. &lt;span&gt;“Exploring and Exploiting the
Correlations Between Bug-Inducing and Bug-Fixing Commits.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on
&lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
&lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 326–37. &lt;span&gt;Tallinn Estonia&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3338906.3338962"&gt;https://doi.org/10.1145/3338906.3338962&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wong2016" class="csl-entry" role="doc-biblioentry"&gt;
Wong, W. Eric, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016.
&lt;span&gt;“A &lt;span&gt;Survey&lt;/span&gt; on &lt;span&gt;Software Fault
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software
Engineering&lt;/em&gt; 42 (8): 707–40. &lt;a
href="https://doi.org/10.1109/TSE.2016.2521368"&gt;https://doi.org/10.1109/TSE.2016.2521368&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Youm2015" class="csl-entry" role="doc-biblioentry"&gt;
Youm, Klaus Changsun, June Ahn, Jeongho Kim, and Eunseok Lee. 2015.
&lt;span&gt;“Bug &lt;span&gt;Localization Based&lt;/span&gt; on &lt;span&gt;Code Change
Histories&lt;/span&gt; and &lt;span&gt;Bug Reports&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015
&lt;span&gt;Asia-Pacific Software Engineering Conference&lt;/span&gt;
(&lt;span&gt;APSEC&lt;/span&gt;)&lt;/em&gt;, 190–97. &lt;span&gt;New Delhi&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/APSEC.2015.23"&gt;https://doi.org/10.1109/APSEC.2015.23&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Zhou2012" class="csl-entry" role="doc-biblioentry"&gt;
Zhou, Jian, Hongyu Zhang, and David Lo. 2012. &lt;span&gt;“Where Should the
Bugs Be Fixed? &lt;span&gt;More&lt;/span&gt; Accurate Information Retrieval-Based
Bug Localization Based on Bug Reports.”&lt;/span&gt; &lt;em&gt;Proceedings -
International Conference on Software Engineering&lt;/em&gt;, 14–24. &lt;a
href="https://doi.org/10.1109/ICSE.2012.6227210"&gt;https://doi.org/10.1109/ICSE.2012.6227210&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Zhou2010" class="csl-entry" role="doc-biblioentry"&gt;
Zhou, Zhi Quan. 2010. &lt;span&gt;“Using Coverage Information to Guide Test
Case Selection in &lt;span&gt;Adaptive Random Testing&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;Proceedings - International Computer Software and Applications
Conference&lt;/em&gt;, 208–13. &lt;a
href="https://doi.org/10.1109/COMPSACW.2010.43"&gt;https://doi.org/10.1109/COMPSACW.2010.43&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ziftci2013a" class="csl-entry" role="doc-biblioentry"&gt;
Ziftci, Celal, and Vivek Ramavajjala. 2013. &lt;span&gt;“Finding
&lt;span&gt;Culprits Automatically&lt;/span&gt; in &lt;span&gt;Failing Builds&lt;/span&gt; -
i.e. &lt;span&gt;Who Broke&lt;/span&gt; the &lt;span&gt;Build&lt;/span&gt;?”&lt;/span&gt; &lt;a
href="https://www.youtube.com/watch?v=SZLuBYlq3OM"&gt;https://www.youtube.com/watch?v=SZLuBYlq3OM&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ziftci2017" class="csl-entry" role="doc-biblioentry"&gt;
Ziftci, Celal, and Jim Reardon. 2017. &lt;span&gt;“Who Broke the Build?
&lt;span&gt;Automatically&lt;/span&gt; Identifying Changes That Induce Test Failures
in Continuous Integration at Google Scale.”&lt;/span&gt; &lt;em&gt;Proceedings -
2017 IEEE/ACM 39th International Conference on Software Engineering:
Software Engineering in Practice Track, ICSE-SEIP 2017&lt;/em&gt;, 113–22. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2017.13"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.13&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;section class="footnotes footnotes-end-of-document"
role="doc-endnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1" role="doc-endnote"&gt;&lt;p&gt;In practice the restriction on
network usage on TAP is somewhat loose. There are legacy tests that have
been tagged to allow them to utilize the network and there is an on
going effort to fully eliminate network usage on TAP.&lt;a href="#fnref1"
class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2" role="doc-endnote"&gt;&lt;p&gt;Other CI platforms exist for
integration tests that span multiple machines, use large amounts of RAM,
or take a very long time to run. The algorithm presented in this paper
has also been implemented for one of those CI systems but we will not
present an empirical evaluation of its performance in that environment
in this paper. In general, Culprit Finding is much more difficult in
such environments as tests may use a mixture of code from different
versions and network resources may be shared across tests.&lt;a
href="#fnref2" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3" role="doc-endnote"&gt;&lt;p&gt;An internal version of Bazel &lt;a
href="https://bazel.build/" class="uri"&gt;https://bazel.build/&lt;/a&gt;.&lt;a
href="#fnref3" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4" role="doc-endnote"&gt;&lt;p&gt;The exact number of hours isn’t so
important. The main thing is to repeat the verification at a suitable
interval from the original attempt.&lt;a href="#fnref4"
class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://github.com/git/git/commit/8b3a1e056f2107deedfdada86046971c9ad7bb87"
class="uri"&gt;https://github.com/git/git/commit/8b3a1e056f2107deedfdada86046971c9ad7bb87&lt;/a&gt;&lt;a
href="#fnref5" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://www.mercurial-scm.org/repo/hg/file/52464a20add0/mercurial/hbisect.py"
class="uri"&gt;https://www.mercurial-scm.org/repo/hg/file/52464a20add0/mercurial/hbisect.py&lt;/a&gt;,
&lt;a href="https://www.mercurial-scm.org/repo/hg/rev/a7678cbd7c28"
class="uri"&gt;https://www.mercurial-scm.org/repo/hg/rev/a7678cbd7c28&lt;/a&gt;&lt;a
href="#fnref6" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://metacpan.org/release/INFINOID/App-SVN-Bisect-1.1/source/README"
class="uri"&gt;https://metacpan.org/release/INFINOID/App-SVN-Bisect-1.1/source/README&lt;/a&gt;&lt;a
href="#fnref7" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://github.com/Ealdwulf/bbchop"
class="uri"&gt;https://github.com/Ealdwulf/bbchop&lt;/a&gt;&lt;a href="#fnref8"
class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="Paper"></category></entry><entry><title>Improving fault localization by integrating value and predicate based causal inference techniques</title><link href="https://hackthology.com/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html" rel="alternate"></link><published>2021-05-26T00:00:00-04:00</published><updated>2021-05-26T00:00:00-04:00</updated><author><name>Yiğit Küçük, &lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2021-05-26:/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html</id><summary type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;Improving fault localization by integrating value and predicate based causal inference techniques&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icse-2021/icse-2021-papers/72/Improving-Fault-Localization-by-Integrating-Value-and-Predicate-Based-Causal-Inferenc"&gt;ICSE 2021&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/ICSE43902.2021.00066"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://doi.org/10.5281/zenodo.4441439"&gt;ARTIFACT&lt;/a&gt;.
&lt;a href="https://hackthology.com/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical fault localization (SFL) techniques use execution profiles and
success/failure information from software executions, in conjunction …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;Improving fault localization by integrating value and predicate based causal inference techniques&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icse-2021/icse-2021-papers/72/Improving-Fault-Localization-by-Integrating-Value-and-Predicate-Based-Causal-Inferenc"&gt;ICSE 2021&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/ICSE43902.2021.00066"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://doi.org/10.5281/zenodo.4441439"&gt;ARTIFACT&lt;/a&gt;.
&lt;a href="https://hackthology.com/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical fault localization (SFL) techniques use execution profiles and
success/failure information from software executions, in conjunction with
statistical inference, to automatically score program elements based on how
likely they are to be faulty. SFL techniques typically employ one type of
profile data: either coverage data, predicate outcomes, or variable values. Most
SFL techniques actually measure correlation, not causation, between profile
values and success/failure, and so they are subject to confounding bias that
distorts the scores they produce. This paper presents a new SFL technique, named
UniVal, that uses causal inference techniques and machine learning to integrate
information about both predicate outcomes and variable values to more accurately
estimate the true failure-causing effect of program statements. UniVal was
empirically compared to several coverage-based, predicate-based, and value-based
SFL techniques on 800 program versions with real faults.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hackthology.com/pdfs/icse-2021.pdf"&gt;Read the paper&lt;/a&gt;&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>The Impact of Rare Failures on Statistical Fault Localization: the Case of the Defects4J Suite</title><link href="https://hackthology.com/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html" rel="alternate"></link><published>2019-10-03T00:00:00-04:00</published><updated>2019-10-03T00:00:00-04:00</updated><author><name>Yiğit Küçük, &lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2019-10-03:/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html</id><summary type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;The Impact of Rare Failures on Statistical Fault Localization: the Case of the Defects4J Suite&lt;/em&gt;.  &lt;a href="https://icsme2019.github.io/"&gt;ICSME 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icsme-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical Fault Localization (SFL) uses coverage profiles (or "spectra")
collected from passing and failing tests, together …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;The Impact of Rare Failures on Statistical Fault Localization: the Case of the Defects4J Suite&lt;/em&gt;.  &lt;a href="https://icsme2019.github.io/"&gt;ICSME 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icsme-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical Fault Localization (SFL) uses coverage profiles (or "spectra")
collected from passing and failing tests, together with statistical metrics,
which are typically composed of simple estimators, to identify which elements of
a program are most likely to have caused observed failures. Previous SFL
research has not thoroughly examined how the effectiveness of SFL metrics is
related to the proportion of failures in test suites and related quantities. To
address this issue, we studied the Defects4J benchmark suite of programs and
test suites and found that if a test suite has very few failures, SFL performs
poorly. To better understand this phenomenon, we investigated the precision of
some statistical estimators of which SFL metrics are composed, as measured by
their coefficients of variation.  The precision of an embedded estimator, which
depends on the dataset, was found to correlate with the effectiveness of a
metric containing it: low precision is associated with poor effectiveness.
Boosting precision by adding test cases was found to improve overall SFL
effectiveness. We present our findings and discuss their implications for the
evaluation and use of SFL metrics.&lt;/p&gt;
&lt;p&gt;(&lt;a href="https://hackthology.com/pdfs/icsme-2019.pdf"&gt;Read the rest of the paper as a pdf&lt;/a&gt;)&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Evaluating Automatic Fault Localization Using Markov Processes</title><link href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html" rel="alternate"></link><published>2019-09-30T00:00:00-04:00</published><updated>2019-09-30T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, Yiğit Küçük, and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2019-09-30:/evaluating-automatic-fault-localization-using-markov-processes.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Yiğit Küçük, and Andy Podgurski
&lt;em&gt;Evaluating Automatic Fault Localization Using Markov Processes&lt;/em&gt;.  &lt;a href="http://www.ieee-scam.org/2019/"&gt;SCAM 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/SCAM.2019.00021"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract …&lt;/h4&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Yiğit Küçük, and Andy Podgurski
&lt;em&gt;Evaluating Automatic Fault Localization Using Markov Processes&lt;/em&gt;.  &lt;a href="http://www.ieee-scam.org/2019/"&gt;SCAM 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/SCAM.2019.00021"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;
Statistical fault localization (SFL) techniques are commonly compared and
evaluated using a measure known as "Rank Score" and its associated evaluation
process.  In the latter process each SFL technique under comparison is used to
produce a list of program locations, ranked by their suspiciousness scores.
Each technique then receives a Rank Score for each faulty program it is applied
to, which is equal to the rank of the first faulty location in the
corresponding list. The SFL technique whose average Rank Score is lowest is
judged the best overall, based on the assumption that a programmer will examine
each location in rank order until a fault is found.  However, this assumption
*oversimplifies* how an SFL technique would be used in practice.
Programmers are likely to regard suspiciousness ranks as just one source of
information among several that are relevant to locating faults. This paper
provides a new evaluation approach using first-order Markov models of debugging
processes, which can incorporate multiple additional kinds of information,
e.g., about code locality, dependences, or even intuition.  Our approach,
&lt;span&gt;&lt;span class="math inline"&gt;\( \textrm{HT}_{\textrm{Rank}} \)&lt;/span&gt;&lt;/span&gt;, scores SFL techniques based on the expected number of steps a
programmer would take through the Markov model before reaching a faulty
location. Unlike previous evaluation methods, &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; can compare techniques
even when they produce fault localization reports differing in structure or
information granularity.  To illustrate the approach, we present a case study
comparing two existing fault localization techniques that produce results
varying in form and granularity.
&lt;/p&gt;

&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Automatic fault localization is a software engineering technique to assist a programmer during the debugging process by suggesting "suspicious" locations that may contain or overlap with a fault (bug, defect) that is the root cause of observed failures. The big idea behind automatic fault localization (or just fault localization) is that pointing the programmer towards the right area of the program will enable them to find the relevant fault more quickly.&lt;/p&gt;
&lt;p&gt;A much-investigated approach to fault localization is &lt;em&gt;Coverage-Based Statistical Fault Localization&lt;/em&gt; (CBSFL), which is also known as &lt;em&gt;Spectrum-Based Fault Localization&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]-[&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;]&lt;/span&gt;. This approach uses code-coverage profiles and success/failure information from testing or field use of software to rank statements or other generic program locations (e.g., basic blocks, methods, or classes) from most "suspicious" (likely to be faulty) to least suspicious. To perform CBSFL, each test case is run using a version of the program being debugged that has been instrumented to record which potential fault locations were actually executed on that test. A human or automated &lt;em&gt;test oracle&lt;/em&gt; labels each test to indicate whether it passed or failed. The coverage profiles are also referred to as &lt;em&gt;coverage spectra&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the usage scenario typically envisioned for CBSFL, a programmer uses the ranked list of program locations to guide debugging. Starting at the top of the list and moving down, they examine each location to determine if it is faulty. If the location of a fault is near the top of the list, the programmer saves time by avoiding consideration of most of the non-faulty locations in the program. However, if there is no fault near the top of the list, the programmer instead wastes time examining many more locations than necessary. CBSFL techniques are typically evaluated empirically in terms of their ability to rank faulty locations near the top of the list &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;], [&lt;a href="#ref-Pearson2017"&gt;5&lt;/a&gt;]&lt;/span&gt;, as measured by each technique's "Rank Score", which is the location of the first faulty location in the list. A CBSFL technique that consistently ranks faulty statements from a wide range of faulty programs near the top of the corresponding lists is considered a good technique.&lt;/p&gt;
&lt;p&gt;One pitfall of using the ranked-list evaluation regime outlined above is that it can be applied fairly only when the techniques being compared provide results as a prioritized list of program elements of the same granularity. This means that if technique A produces a prioritized list of basic blocks, technique B produces an unordered set of sub-expressions, and technique C produces a prioritized list of classes then it is not valid to use the Standard Rank Score to compare them. The Standard Rank Score can only be applied to ordered lists and thus cannot be used directly to evaluate technique B, and it requires the techniques being compared to have the same granularity. Our new evaluation metric (called &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;) accounts for these differences in granularity and report construction, allowing a direct comparison between different styles of fault localization. We present a case study in Section 27 comparing &lt;em&gt;behavioral&lt;/em&gt; fault localization (which produces fragments of the dynamic control flow graph) to standard CBSFL.&lt;/p&gt;
&lt;p&gt;Second, it is evident that the imagined usage scenario for CBSFL, in which a programmer examines each possible fault location in rank order until a fault is found, is an oversimplification of programmer behavior &lt;span class="citation"&gt;[&lt;a href="#ref-Parnin2011"&gt;6&lt;/a&gt;]&lt;/span&gt;. Programmers are likely to deviate from this scenario, e.g.: by choosing not to re-examine locations that have already been carefully examined and have not changed; by examining the locations around a highly ranked one, regardless of their ranks; by examining locations that a highly ranked location is dependent upon or that are dependent upon it &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt;; by employing a conventional debugger (such as gdb, WinDB, or Visual Studio's debugger); or simply by using their knowledge and intuition about the program.&lt;/p&gt;
&lt;p&gt;To support more flexible and nuanced evaluation criteria for CBSFL and other fault localization techniques, we present a new approach to evaluating them that is based on constructing and analyzing first-order Markov models of debugging processes and that uses a new evaluation metric (&lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;) based on the "hitting time" of a Markov process. This approach allows researchers to directly compare different fault localization techniques by incorporating their results and other relevant information into an appropriate model. To illustrate the approach, we present models for two classes of fault localization techniques: CBSFL and Suspicious Behavior Based Fault Localization (SBBFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. The models we present are also easy to update, allowing researchers to incorporate results of future studies of programmers' behavior during debugging.&lt;/p&gt;
&lt;p&gt;Our new debugging model (and its &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; metric) can be thought of as a first-order simulation of the debugging process as conducted by a programmer. As such, we intend for it to be a practical alternative to conducting an expensive user study. The model is capable of incorporating a variety of behaviors a programmer may exhibit while debugging, allowing researchers to evaluate the performance of their tool against multiple debugging "styles."&lt;/p&gt;
&lt;h1 id="background-and-related-work"&gt;Background and Related Work&lt;/h1&gt;
&lt;p&gt;Coverage Based Statistical Fault Localization (CBSFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;], [&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt; techniques attempt to quantify the likelihood that individual program locations are faulty using sample statistics, called &lt;em&gt;suspiciousness metrics&lt;/em&gt; or &lt;em&gt;fault localization metrics&lt;/em&gt;, which are computed from PASS/FAIL labels assigned to test executions and from coverage profiles (coverage spectra) collected from those executions. A CBSFL suspiciousness metric (of which there are a great many &lt;span class="citation"&gt;[&lt;a href="#ref-Lucia2014"&gt;2&lt;/a&gt;], [&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;]&lt;/span&gt;) measures the statistical association between the occurrence of test failures and the coverage of individual program locations (program elements) of a certain kind.&lt;/p&gt;
&lt;p&gt;Some statistical fault localization techniques use additional information beyond basic coverage information to either improve accuracy or provide more explainable results. For instance, work on &lt;em&gt;Causal Statistical Fault Localization&lt;/em&gt; uses information about the execution of program dependence predecessors of a target statement to adjust for confounding bias that can distort suspiciousness scores &lt;span class="citation"&gt;[&lt;a href="#ref-Baah2010"&gt;10&lt;/a&gt;]&lt;/span&gt;. By contrast, &lt;em&gt;Suspicious-Behavior-Based Fault Localization&lt;/em&gt; (SBBFL) techniques use runtime control-flow information (the behavior) to identify groups of "collaborating" suspicious elements &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. These techniques typically leverage data mining techniques &lt;span class="citation"&gt;[&lt;a href="#ref-Aggarwal2014"&gt;11&lt;/a&gt;]&lt;/span&gt; such as frequent pattern mining &lt;span class="citation"&gt;[&lt;a href="#ref-Agrawal1993"&gt;12&lt;/a&gt;]-[&lt;a href="#ref-Aggarwal2014a"&gt;14&lt;/a&gt;]&lt;/span&gt; or significant pattern mining &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;], [&lt;a href="#ref-Yan2008"&gt;15&lt;/a&gt;]&lt;/span&gt;. Unlike CBSFL techniques, SBBFL techniques output a ranked list of &lt;em&gt;patterns&lt;/em&gt; (subgraphs, itemsets, trees) which each contain multiple program locations. This makes it difficult to directly compare SBBFL and CBSFL techniques using traditional evaluation methods.&lt;/p&gt;
&lt;p&gt;Finally, a variety of non-statistical (or hybrid) approaches to fault localization have been explored &lt;span class="citation"&gt;[&lt;a href="#ref-Abreu2006"&gt;16&lt;/a&gt;]-[&lt;a href="#ref-Wong2016"&gt;19&lt;/a&gt;]&lt;/span&gt;. These approaches range from delta debugging &lt;span class="citation"&gt;[&lt;a href="#ref-Zeller1999"&gt;20&lt;/a&gt;]&lt;/span&gt; to nearest neighbor queries &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt; to program slicing &lt;span class="citation"&gt;[&lt;a href="#ref-Tip1995"&gt;21&lt;/a&gt;], [&lt;a href="#ref-Mao2014"&gt;22&lt;/a&gt;]&lt;/span&gt; to information retrieval &lt;span class="citation"&gt;[&lt;a href="#ref-Marcus2004"&gt;23&lt;/a&gt;]-[&lt;a href="#ref-Le2015"&gt;25&lt;/a&gt;]&lt;/span&gt; to test case generation &lt;span class="citation"&gt;[&lt;a href="#ref-Artzi2010"&gt;26&lt;/a&gt;]-[&lt;a href="#ref-Perez2014"&gt;28&lt;/a&gt;]&lt;/span&gt;. Despite technical and theoretical differences in these approaches, they all suggest locations (or groups of locations) for programmers to consider when debugging.&lt;/p&gt;
&lt;h2 id="the-tarantula-evaluation"&gt;The Tarantula Evaluation&lt;/h2&gt;
&lt;p&gt;Some of the earliest papers on fault localization do not provide a quantitative method for evaluating performance (as is seen in later papers &lt;span class="citation"&gt;[&lt;a href="#ref-Pearson2017"&gt;5&lt;/a&gt;]&lt;/span&gt;). For instance, the earliest CBSFL paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt;, by Jones &lt;em&gt;et al.&lt;/em&gt;, proposes a technique and evaluates it qualitatively using data visualization. At the time, this was entirely appropriate as Jones was proposing a technique for visualizing the relative suspiciousness of different statements, as estimated with what is now called a suspiciousness metric (Tarantula). The visualization used for evaluating this technique aggregated the visualizations for all of the subject programs included in the study.&lt;/p&gt;
&lt;p&gt;While the evaluation method used in the Jones &lt;em&gt;et al.&lt;/em&gt; paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt; effectively communicated the potential of CBSFL (and interested many researchers in the idea) it was not good way to compare multiple fault localization techniques. In 2005 Jones and Harrold &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt; published a study that compared their Tarantula technique to three other techniques: Set Union and Intersection &lt;span class="citation"&gt;[&lt;a href="#ref-Agrawal1995"&gt;29&lt;/a&gt;]&lt;/span&gt;, Nearest Neighbor &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt;, and Cause-Transitions &lt;span class="citation"&gt;[&lt;a href="#ref-Cleve2005"&gt;30&lt;/a&gt;]&lt;/span&gt;. These techniques involved different approaches toward the fault localization problem and originally had been evaluated in different ways. Jones and Harrold re-evaluated all of the techniques under a new common evaluation framework.&lt;/p&gt;
&lt;p&gt;In their 2005 paper, Jones and Harrold evaluated the effectiveness of each fault localization technique by using it to rank the statements in each subject program version from most likely to be the root cause of observed program failures to least likely. For their technique Tarantula, the statements were ranked using the Tarantula suspiciousness score.&lt;a href="#fn1" class="footnoteRef" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; To compare the effectiveness of the techniques, another kind of score was assigned to each faulty version of each subject program. This score is based on the "rank score":&lt;/p&gt;
&lt;h4 id="definition.-tarantula-rank-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Tarantula Rank Score &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; with their suspiciousness scores &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; for &lt;span class="math inline"&gt;\(l
  \in L\)&lt;/span&gt; the Rank Score &lt;span class="math inline"&gt;\(r(l)\)&lt;/span&gt; for a faulty location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; is: &lt;span class="math display"&gt;\[\begin{aligned}
    {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) \ge s(l) \right\} }\right|}}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For Set Union and Intersection, Nearest Neighbor, and Cause-Transitions, Jones and Harrold used an idea of Renieres and Reiss &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt; and ranked a program's statements based on consideration of its System Dependence Graph (SDG) &lt;span class="citation"&gt;[&lt;a href="#ref-Horwitz1990"&gt;31&lt;/a&gt;]&lt;/span&gt;. The surrogate suspiciousness score of a program location &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; is the inverse of the size of the smallest dependence sphere around &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; that contains a faulty location. The surrogate scores are then used to calculate the Tarantula Rank Score (Def. 14).&lt;/p&gt;
&lt;p&gt;In Jones's and Harrold's evaluation the authors did not use the Tarantula Rank Score directly but instead used a version of it that is normalized by program size:&lt;/p&gt;
&lt;h4 id="definition.-tarantula-effectiveness-score-expense"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Tarantula Effectiveness Score (Expense) &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This score is the proportion of program locations that do &lt;strong&gt;not&lt;/strong&gt; need to be examined to find a fault when the locations are examined in rank order. Formally, let &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; be the total number of program locations, and let &lt;span class="math inline"&gt;\(r(f)\)&lt;/span&gt; be the Tarantula Rank Score (Def. 14) of the faulty location &lt;span class="math inline"&gt;\(f\)&lt;/span&gt;. Then the score is: &lt;span class="math display"&gt;\[\begin{aligned}
    \frac{n-r(f)}{n}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using the normalized effectiveness score, Jones and Harrold directly compared the fault localization effectiveness of the techniques they considered. They did this in two ways. First, they presented a table that bucketed all the buggy versions of all the programs by their Tarantula Effectiveness Scores (given as percentages). Second, they presented a figure that showed the same data as a cumulative curve.&lt;/p&gt;
&lt;p&gt;The core ideas of Jones' and Harrold's Effectivness/Expense score now underlie most evaluations of CBSFL techniques. Faulty statements are scored, ranked, rank-scored, normalized, and then aggregated over all programs and versions to provide an overall representation of a fault localization method's performance (e.g., &lt;span class="citation"&gt;[&lt;a href="#ref-Steimann2013"&gt;53&lt;/a&gt;]&lt;/span&gt;, [&lt;a href="#ref-Lucia2014"&gt;2&lt;/a&gt;], [&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;], [&lt;a href="#ref-Wong2008"&gt;32&lt;/a&gt;]-[&lt;a href="#ref-Zheng2018"&gt;34&lt;/a&gt;]&lt;/span&gt;). However, some refinements have been made to both the Rank Score and the Effectiveness Score.&lt;/p&gt;
&lt;h2 id="the-implied-debugging-models"&gt;The Implied Debugging Models&lt;/h2&gt;
&lt;p&gt;It is worth (re)stating here the debugging model implied in the Jones and Harrold evaluation &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;. The programmer receives from the fault localization tool a ranked list of statements with the most suspicious statements at the top. The programmer then moves down the list examining each location in turn. If multiple statements have the same rank (the same suspiciousness score) all of those statements are examined before the programmer makes a determination on whether or not the bug has been located. This rule is captured in the mathematical definition of the Tarantula Rank Score (Definition 14).&lt;/p&gt;
&lt;p&gt;For the non-CBSFL methods which Jones compared CBSFL against, the ranks of the program locations were once again compared using the method of Renieres and Reiss &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;], [&lt;a href="#ref-Cleve2005"&gt;30&lt;/a&gt;], [&lt;a href="#ref-ChaoLiu2006"&gt;35&lt;/a&gt;]&lt;/span&gt; which is sometimes called &lt;em&gt;T-Score&lt;/em&gt;. As a reminder, this method computes a surrogate suspiciousness score based on the size of smallest &lt;em&gt;dependence sphere&lt;/em&gt;&lt;a href="#fn2" class="footnoteRef" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; centered around the locations indicated in the fault localization report that contain the faulty code. This implies a debugging model in which the programmer examines each "shell" of the dependence sphere in turn before moving onto the next larger shell (see Figure 7 in &lt;span class="citation"&gt;[&lt;a href="#ref-Cleve2005"&gt;30&lt;/a&gt;]&lt;/span&gt; for a visualization).&lt;/p&gt;
&lt;p&gt;Neither of these debugging models are realistic. Programmers may be reasonably expected to deviate from the ordering implied by the ranking. During the debugging process a programmer may use a variety of information sources — including intuition — to decide on the next element to examine. They may examine the same element multiple times. They may take a highly circuitous route to the buggy code or via intuition jump immediately to the fault. The models described above allow for none of these subtleties.&lt;/p&gt;
&lt;h2 id="refinements-to-the-evaluation-framework"&gt;Refinements to the Evaluation Framework&lt;/h2&gt;
&lt;p&gt;Wong &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Wong2008"&gt;32&lt;/a&gt;]&lt;/span&gt; introduced the most commonly used effectiveness score, which is called the &lt;span class="math inline"&gt;\(\mathcal{EXAM}\)&lt;/span&gt; score. This score is essentially the same as the Expense score (Def. 15) except that it gives the percentage of locations that need to be examined rather than those avoided.&lt;/p&gt;
&lt;h4 id="definition.-mathcalexam-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. &lt;span class="math inline"&gt;\(\mathcal{EXAM}\)&lt;/span&gt; Score &lt;span class="citation"&gt;[&lt;a href="#ref-Wong2008"&gt;32&lt;/a&gt;]&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{aligned}
    \frac{r(f)}{n}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ali &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Ali2009"&gt;37&lt;/a&gt;]&lt;/span&gt; identified an important problem with Jones' and Harrold's evaluation method: some fault localization techniques always assign different locations distinct suspiciousness scores, but others do not. Ali &lt;em&gt;et al.&lt;/em&gt; pointed out that when comparing techniques, the Tarantula Effectiveness Score may favor a technique that generates more distinct suspiciousness scores than the other techniques. The fix they propose is to assign to a location in a group of locations with the same suspiciousness score a rank score that reflects developers having to examine half the locations in the group on average.&lt;/p&gt;
&lt;h4 id="definition.-standard-rank-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Standard Rank Score&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This score is the expected number of locations a programmer would inspect before locating a fault. Formally, given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; with their suspiciousness scores &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; for &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt;, the Rank Score for a location &lt;span class="math inline"&gt;\(l
  \in L\)&lt;/span&gt; is &lt;span class="citation"&gt;[&lt;a href="#ref-Ali2009"&gt;37&lt;/a&gt;]&lt;/span&gt;: &lt;span class="math display"&gt;\[\begin{aligned}
    {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) &amp;gt; s(l) \right\} }\right|}} +
    \frac{
      {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \right\} }\right|}}
    }{
      2
    }
  \end{aligned}\]&lt;/span&gt; Note: when we refer to the "Standard Rank Score" this is the definition we are referring to.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Parnin and Orso &lt;span class="citation"&gt;[&lt;a href="#ref-Parnin2011"&gt;6&lt;/a&gt;]&lt;/span&gt; conducted a study of programmers' actual use of a statistical fault localization tool (Tarantula &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt;). One of their findings was that programmers did not look deeply through the ranked list of locations and would instead only consider the first few locations. Consequently, they encouraged researchers to no longer report effectiveness scores as percentages. Most CBSFL studies now report absolute (non-percentage) rank scores. This is desirable for another reason: larger programs can have much larger absolute ranks than small programs, for the same percentage rank. Consider, for instance a program with 100,000 lines. If a fault's Rank Score is 10,000 its percentage Exam Score would be 10%. A 10% Exam Score might look like a reasonably good localization (and would be if the program had 100 lines) but no programmer will be willing to look through 10,000 lines. By themselves, percentage evaluation metrics (like Exam Score) produce inherently misleading results for large programs.&lt;/p&gt;
&lt;p&gt;Steinmann &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Steimann2013"&gt;53&lt;/a&gt;]&lt;/span&gt; identified a number of threats to validity in CBSFL studies, including: heterogeneous subject programs, poor test suites, small sample sizes, unclear sample spaces, flaky tests, total number of faults, and masked faults. For evaluation they used the Standard Rank Score of Definition 17 modified to deal with &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; faults tied at the same rank.&lt;/p&gt;
&lt;h4 id="definition.-steinmann-rank-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Steinmann Rank Score&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This score is the expected number of locations a programmer would inspect before finding a fault when multiple faulty statements may have the same rank. Formally, given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; with their suspiciousness scores &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; for &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt;, the Rank Score for a location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; is &lt;span class="citation"&gt;[&lt;a href="#ref-Steimann2013"&gt;53&lt;/a&gt;]&lt;/span&gt;: &lt;span class="math display"&gt;\[\begin{aligned}
    &amp;amp; {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) &amp;gt; s(l) \right\} }\right|}}\\
    &amp;amp; + \frac{
          {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \right\} }\right|}} + 1
        }{
          {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \wedge x \text{ is a
          faulty location} \right\}}\right|}} + 1
        }
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Moon &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Moon2014"&gt;38&lt;/a&gt;]&lt;/span&gt; proposed Locality Information Loss (LIL) as an alternative evaluation framework. LIL models the localization result as a probability distribution constructed from the suspiciousness scores:&lt;/p&gt;
&lt;h4 id="definition.-lil-probability-distribution"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. LIL Probability Distribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt; be a suspicious metric normalized to the &lt;span class="math inline"&gt;\([0,1]\)&lt;/span&gt; range of reals. Let &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; be the number of locations in the program and let &lt;span class="math inline"&gt;\(L = \{l_1,\ldots,
  l_n\}\)&lt;/span&gt; be the set of locations. The constructed probability distribution is given by: &lt;span class="math display"&gt;\[\begin{aligned}
    P_{\tau}(l_i) = \frac{\tau(l_i)}{\sum^{n}_{j=1} \tau(l_j)}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LIL uses the Kullback-Leibler measure of divergence between distributions to compute a score indicating how different the distribution constructed for a suspiciousness metric of interest is from the distribution constructed from an "ideal" metric, which gives a score of 1 to the faulty location(s) and gives negligible scores to every other location. The advantage of the LIL framework is that it does not depend on a list of ranked statements and can be applied to non-statistical methods (using a synthetic &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt;). The disadvantage of LIL is that it does not reflect programmer effort (as the Rank Scores do). However, it may be a better metric to use when evaluating fault localization systems as components of automated fault repair systems.&lt;/p&gt;
&lt;p&gt;Pearson &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Pearson2017"&gt;5&lt;/a&gt;]&lt;/span&gt; re-evaluated a number of previous results using new real world subject programs with real defects and test suites. In contrast to previous work they made use of statistical hypothesis testing and confidence intervals to characterize the uncertainty of the results. To evaluate the performance of each technique under study they used the &lt;span class="math inline"&gt;\(\mathcal{EXAM}\)&lt;/span&gt; score, reporting best, average, and worst case results for multi-statement faults.&lt;/p&gt;
&lt;h1 id="a-new-approach-to-evaluation"&gt;A New Approach to Evaluation&lt;/h1&gt;
&lt;p&gt;Programmers consider multiple sources of information when performing debugging tasks and use them to guide their exploration of the source code. In our new approach to evaluating fault localization techniques, a model is constructed for each technique &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; and each program &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; of how a programmer using &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; might move from examining one location in &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; to examining another. The model for &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; is used to compute a statistical estimate of the expected number of moves a programmer using &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; would make before encountering a fault in &lt;span class="math inline"&gt;\(P\)&lt;/span&gt;. This estimate is used to compute a "hitting-time rank score" for technique &lt;span class="math inline"&gt;\(T\)&lt;/span&gt;. The scores for all the techniques can then be compared to determine which performed best on program &lt;span class="math inline"&gt;\(P\)&lt;/span&gt;. This section presents the general approach and specific example models. The models make use of CBSFL reports and information about static program structure and dynamic control flow.&lt;/p&gt;
&lt;p&gt;In order to support very flexible modeling and tractable analysis of debugging processes, we use first-order Markov chains (described below) to model them. Our first example models the debugging process assumed in previous work, in which a programmer follows a ranked list of suspicious program locations until a fault is found. Then we describe how to incorporate structural information about the program (which could influence a programmer's debugging behavior). Finally, we show how to model and compare CBSFL to a recent &lt;em&gt;Suspicious Behavioral Based Fault Localization&lt;/em&gt; (SBBFL) algorithm &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; that identifies suspicious subgraphs of dynamic control flow graphs. This third model demonstrates the flexibility of our approach and could be adapted to evaluate other SBBFL techniques &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Liu2005"&gt;39&lt;/a&gt;]-[&lt;a href="#ref-Yousefi2013"&gt;47&lt;/a&gt;]&lt;/span&gt;. It also demonstrates that our approach can be used to compare statistical and non-statistical fault localization techniques under the same assumptions about programmer debugging behavior.&lt;/p&gt;
&lt;p&gt;It is important to emphasize that the quality and value of the evaluation results obtained with our approach depend primarily on the appropriateness of the model of the debugging process that is created. This model represents the evaluators' knowledge about likely programmer behavior during debugging and the factors that influence it. To avoid biasing their evaluation, evaluators must commit to an evaluation model and refrain from "tweaking" it after applying it to the data. &lt;span class="citation"&gt;[&lt;a href="#ref-Ioannidis2005"&gt;48&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="background-on-ergodic-markov-chains"&gt;Background on Ergodic Markov Chains&lt;/h2&gt;
&lt;p&gt;A finite state Markov chain consists of a set of &lt;em&gt;states&lt;/em&gt; &lt;span class="math inline"&gt;\(S = \{s_1, s_2,
..., s_n \}\)&lt;/span&gt; and an &lt;span class="math inline"&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt;, called the &lt;em&gt;transition matrix&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Grinstead2012"&gt;49&lt;/a&gt;]&lt;/span&gt;. Entry &lt;span class="math inline"&gt;\({\bf P}_{i,j}\)&lt;/span&gt; gives the probability for a &lt;em&gt;Markov process&lt;/em&gt; in state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; to move to state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt;. The probability of a Markov process moving from one state to another only depends on the state the process is currently in. This is known as the &lt;em&gt;Markov property&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A Markov chain is said to be &lt;em&gt;ergodic&lt;/em&gt; if, given enough steps, it can move from any state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; to any state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt;, i.e. &lt;span class="math inline"&gt;\({\textrm{Pr}\left[{s_i \xrightarrow{*} s_j}\right]} &amp;gt; 0\)&lt;/span&gt;. Thus, there are no states in an ergodic chain that the process can never leave.&lt;/p&gt;
&lt;p&gt;Ergodic Markov chains have &lt;em&gt;stationary distributions&lt;/em&gt;. Let &lt;span class="math inline"&gt;\({\bf v}\)&lt;/span&gt; be an arbitrary probability vector. The stationary distribution is a probability vector &lt;span class="math inline"&gt;\({\bf w}\)&lt;/span&gt; such that &lt;span class="math display"&gt;\[\lim_{n \rightarrow \infty} {\bf v}{\bf P}^{n} = {\bf w}\]&lt;/span&gt; The vector &lt;span class="math inline"&gt;\({\bf w}\)&lt;/span&gt; is a fixed point on &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; implying &lt;span class="math inline"&gt;\({\bf w}{\bf P} = {\bf
w}\)&lt;/span&gt;. Stationary distributions give the long term behavior of a Markov chain - meaning that after many steps the chance a Markov process ends in state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; is given by &lt;span class="math inline"&gt;\({\bf w}_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;expected hitting time&lt;/em&gt; of a state in a Markov chain is the expected number of steps (transitions) a Markov process will make before it encounters the state for the first time. Our new evaluation metric (&lt;span class="math inline"&gt;\(\text{HT}_{\text{Rank}}\)&lt;/span&gt;) uses the expected hitting time of the state representing a faulty program location to score a fault localization technique's performance. Lower expected hitting times yield better localization scores.&lt;/p&gt;
&lt;h4 id="definition.-expected-hitting-time"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Expected Hitting Time&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider a Markov chain with transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt;. Let &lt;span class="math inline"&gt;\(T_{i,j}\)&lt;/span&gt; be a random variable denoting the time at which a Markov process that starts at state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; reaches state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt;. The expected hitting time (or just hitting time) of state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt; for such a process is the expected value of &lt;span class="math inline"&gt;\(T_{i,j}\)&lt;/span&gt; &lt;span class="math display"&gt;\[\begin{aligned}
    {\textrm{E}\left[{T_{i,j}}\right]} = \sum_{k=1}^{\infty} k \cdot {\textrm{Pr}\left[{T_{i,j}=k}\right]}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In general, a hitting time for a single state may be computed in &lt;span class="math inline"&gt;\(\mathcal{O}(n^3)\)&lt;/span&gt; steps &lt;span class="citation"&gt;[&lt;a href="#ref-Kemeny1960"&gt;50&lt;/a&gt;]&lt;/span&gt;. Somewhat less time is required for sparse transition matrices &lt;span class="citation"&gt;[&lt;a href="#ref-Davis2004"&gt;51&lt;/a&gt;]&lt;/span&gt;. Chapter 11 of Grinstead and Snell &lt;span class="citation"&gt;[&lt;a href="#ref-Grinstead2012"&gt;49&lt;/a&gt;]&lt;/span&gt; provides an accessible introduction to hitting time computation.&lt;/p&gt;
&lt;p&gt;Some programs may have too many elements for exact hitting time computations (our case study contains one such program). To deal with large programs the expected hitting time can also be estimated by taking repeated random walks through the Markov chain to obtain a sample of hitting times. The sample can then be used to estimate the expected hitting time by computing the sample mean.&lt;a href="#fn3" class="footnoteRef" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="expected-hitting-time-rank-score-textrmht_textrmrank"&gt;Expected Hitting Time Rank Score (&lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;)&lt;/h2&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;Has&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;
&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Key&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Key&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;left&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Has&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;right&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Has&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Listing 1.&lt;/strong&gt;
A bug in the implementation of the "Has" method in an AVL tree.
&lt;/div&gt;
&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;Table I: Reduced CBSFL Results for Listing 1&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;rank&lt;/th&gt;
&lt;th align="center"&gt;R. F1&lt;/th&gt;
&lt;th align="left"&gt;Function (Basic Block)&lt;/th&gt;
&lt;th align="left"&gt;rank&lt;/th&gt;
&lt;th align="center"&gt;R. F1&lt;/th&gt;
&lt;th align="left"&gt;Function (BB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;1.5&lt;/td&gt;
&lt;td align="center"&gt;0.98&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (2)&lt;/td&gt;
&lt;td align="left"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;0.95&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (3)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;1.5&lt;/td&gt;
&lt;td align="center"&gt;0.98&lt;/td&gt;
&lt;td align="left"&gt;Node.String (3)&lt;/td&gt;
&lt;td align="left"&gt;7.5&lt;/td&gt;
&lt;td align="center"&gt;0.94&lt;/td&gt;
&lt;td align="left"&gt;main (1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;1.5&lt;/td&gt;
&lt;td align="center"&gt;0.98&lt;/td&gt;
&lt;td align="left"&gt;Node.Verify (4)&lt;/td&gt;
&lt;td align="left"&gt;7.5&lt;/td&gt;
&lt;td align="center"&gt;0.94&lt;/td&gt;
&lt;td align="left"&gt;Scanner.Scan (24)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;0.97&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (4)&lt;/td&gt;
&lt;td align="left"&gt;7.5&lt;/td&gt;
&lt;td align="center"&gt;0.94&lt;/td&gt;
&lt;td align="left"&gt;Node.Verify (0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;0.97&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (6)&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src="images/scam/markov-rank-list.png" alt="image" /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig. 1:&lt;/strong&gt; A simplified version of the Markov model for evaluating the ranked list of suspicious locations for the bug in Listing 1.
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;img src="images/scam/markov-rank-list-with-jumps.png" alt="image" /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig. 2:&lt;/strong&gt; An example Markov model showing how “jump” edges can be added to represent how a programmer might examine locations which are near the location they are currently reviewing. Compare to Figure 1.
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;This section introduces our new evaluation metric &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; produces a "rank score" similar to the score produced by the Standard Rank Score of Definition 17. In the standard score, program locations are ranked by their CBSFL suspiciousness scores. A location's position in the ordered list is that location's Rank Score (see the definition for details).&lt;/p&gt;
&lt;p&gt;The new &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; score is obtained as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A Markov debugging model is supplied (as a Markov chain).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The expected hitting times (Def. 22) for each location in the program are computed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The locations are ordered by their expected hitting times.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; for a location is its position in the ordered list.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="definition.-textrmht_textrmrank"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; and a Markov chain &lt;span class="math inline"&gt;\((S, {\bf P})\)&lt;/span&gt; that represents the debugging process and has start state &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, the Hitting-Time Rank Score &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; for a location &lt;span class="math inline"&gt;\(l \in L\cap S\)&lt;/span&gt; is: &lt;span class="math display"&gt;\[\begin{aligned}
    &amp;amp; {{\left|{ \left\{ x ~:~
        x \in L \cap S \wedge
        {\textrm{E}\left[{T_{0,x}}\right]} &amp;lt; {\textrm{E}\left[{T_{0,l}}\right]}
        \right\}
    }\right|}} +
    \\
    &amp;amp; \frac{
        {{\left|{ \left\{ x ~:~
        x \in L \cap S \wedge
        {\textrm{E}\left[{T_{0,x}}\right]} = {\textrm{E}\left[{T_{0,l}}\right]} \right\}
        }\right|}}
    }{
      2
    }
  \end{aligned}\]&lt;/span&gt; Note: this is almost identical to Definition 17, but it replaces the suspiciousness score with the expected hitting time. Definition 18 can also be modified in a similar way for multi-fault programs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="markov-debugging-models"&gt;Markov Debugging Models&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; is parameterized by a "debugging model" expressed as a Markov chain. As noted above, a Markov chain is made up a of set of states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt; and transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt;. In a debugging model, there are two types of states: 1) textual locations in the source code of a program and 2) synthetic states. Figures 6 and 7 show examples of debugging models constructed for an implementation of an AVL tree. In the figures, the square nodes are Markov states representing basic blocks in the source code. (Note that CBSFL techniques typically compute the same suspiciousness score for all statements in a basic block.) The smaller, circular nodes are Markov states which are synthetic. They are there for structural reasons but do not represent particular locations in the source code. The edges in the graphs represent possible transitions between states, and they are annotated with transition probabilities. All outgoing edges from a node should sum to 1.&lt;/p&gt;
&lt;p&gt;In a Markov debugging model a programmer is represented by the Markov process. When the Markov process is simulated the debugging actions of a programmer are being simulated. This is a "first order" simulation, which means the actions of the simulated programmer (Markov process) only depend on the current location being examined. Thus, the Markov model provides a simple and easy-to-construct mathematical model of a programmer looking through the source code of a program to find the faulty statement(s). The simulated programmer begins at some starting state and moves from state to state until the faulty location is found. We require that all Markov models are &lt;em&gt;ergodic&lt;/em&gt;, ensuring that every state (program location) is eventually reachable in the model.&lt;/p&gt;
&lt;h2 id="an-extensible-markov-model-for-cbsfl"&gt;An Extensible Markov Model for CBSFL&lt;/h2&gt;
&lt;p&gt;As described in Section 12 a CBSFL report is made up of a ranked list of locations in the subject program. Our extensible Markov model includes a representation of the CBSFL ranked list. By itself, using &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; with a Markov model of a ranked list is just a mathematically complex way of restating Definition 17. However, with a Markov model of a CBSFL report in hand we can add further connections &lt;em&gt;between&lt;/em&gt; program locations (represented as Markov states) to represent other actions a programmer might take besides traversing down the ranked list. For instance, in Section 25 we note that programmers use graphical debuggers to traverse the dynamic control flow in a program - allowing them to visit statements in the order they are executed. We embed the dynamic control flow graph into the transition matrix of the Markov model to reflect this observation.&lt;/p&gt;
&lt;h3 id="a-ranked-list-as-a-markov-chain"&gt;A Ranked List as a Markov Chain&lt;/h3&gt;
&lt;p&gt;Figure 6 provides a graphical example of a Markov chain for a ranked list. Since the nodes in a graphical representation of a Markov chain represent states and the edges represent the transition matrix, the probabilities associated with outgoing edges of each node should sum to 1. In Figure 6, each circular node represents a rank in the list and the square nodes represent associated program locations, which are identified by their function names and static basic-block id number. The square nodes that are grouped together all have the same suspiciousness scores. We will provide here a brief, informal description of the structure of the transition matrix. A formal description of the chain is provided in Definition 28 in the Appendix. The exact choice of transition matrix in the formal chain was driven by a proof of equivalence between &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; with this Markov model (as defined in Definition 28) and the Standard Rank Score (Definition 17).&lt;a href="#fn4" class="footnoteRef" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The transition matrix boils down to a couple of simple connections. The nodes representing groups form a doubly linked list (see the circular nodes in Figure 6). The ordering of the "group nodes" matches the ordering of the ranks in the ranked list. The links between the node are weighted so that a Markov process will tend to end up in a highly ranked node. More formally, the model was constructed so that if you ordered the Markov states by the probabilities in the &lt;em&gt;stationary distribution&lt;/em&gt; (which characterizes the long term behavior of the Markov process) from highest to lowest that ordering would match the order of the ranked list.&lt;/p&gt;
&lt;p&gt;The second type of connection is from a group node to its program location nodes. Each location node connects to exactly one group node. The transition probabilities (as shown in Figure 6) are set up such that there is an equal chance of moving to any of the locations in the group.&lt;/p&gt;
&lt;p&gt;The final connection is from a location node back to its group node. This is always assigned probability &lt;span class="math inline"&gt;\(1\)&lt;/span&gt; (see Figure 6). Again, see Definition 28 in the Appendix for the formal description.&lt;/p&gt;
&lt;h3 id="adding-local-jumps-to-the-cbsfl-chain"&gt;Adding Local Jumps to the CBSFL Chain&lt;/h3&gt;
&lt;p&gt;Figure 7 shows a modified version of the model in Figure 6. The modified model allows the Markov process to jump or "teleport" between program locations which are not adjacent in the CBSFL Ranked List. These "jump" connections are setup to model other ways a programmer might move through the source code of a program when debugging (see below). In the figure, these jumps are shown with added red and blue edges. For the Markov model depicted in the figure, the Markov process will with probability &lt;span class="math inline"&gt;\(\frac{1}{2}\)&lt;/span&gt; move back to the rank list and with probability &lt;span class="math inline"&gt;\(\frac{1}{2}\)&lt;/span&gt; teleport or jump to an alternative program location that is structurally adjacent (in the program's source code) to the current one.&lt;/p&gt;
&lt;p&gt;Informally, the modified model is set up so that if the Markov process is in a state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; which represents program location &lt;span class="math inline"&gt;\(l_x\)&lt;/span&gt; it will with probability &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; move to a state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt; which represents program location &lt;span class="math inline"&gt;\(l_y\)&lt;/span&gt; instead of returning to the rank list. The locations that the process can move to from &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; are defined in a jump matrix &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; which is parameter to the Markov model. The matrix &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; encodes assumptions or observations about how programmers behave when they are debugging. A formal definition of the CBSFL chain with jumps is presented in the Appendix (see Definition 29).&lt;/p&gt;
&lt;p&gt;Definition 26 defines one general-purpose jump matrix &lt;span class="math inline"&gt;\({\bf
J}\)&lt;/span&gt;. It encodes two assumptions about programmer behavior. First, when a programmer considers a location inside a function they will also potentially examine other locations in that function. Second, when a programmer is examining a location they may examine locations preceding or succeeding it the dynamic control flow (e.g., with the assistance of a graphical debugger). Definition 26 encodes both assumptions by setting the relevant &lt;span class="math inline"&gt;\({\bf
J}_{i,j}\)&lt;/span&gt; entries to &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id="definition.-spacial-behavioral-jumps"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Spacial + Behavioral Jumps&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{aligned}
    {\bf J}_{i,j} &amp;amp;=
    \left\{
      \begin{array}{cll}
          1 &amp;amp;
          \text{if}
          &amp;amp;    \text{$s_i$ and $s_j$ represent locations} \\
          &amp;amp; &amp;amp;  \text{in the same function}
               \\
        \\
          1 &amp;amp;
          \text{if}
          &amp;amp;    \text{$s_i$ and $s_j$ are adjacent locations in the} \\
          &amp;amp; &amp;amp;  \text{program&amp;#39;s dynamic control flow graph} \\
        \\
          0
          &amp;amp; \text{otherwise}
      \end{array}
    \right.
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In addition to &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt;, the new chain is parameterized by the probability &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; of a jump occurring when the process visits a state representing a program location. As &lt;span class="math inline"&gt;\(p_{\text{jump}} \rightarrow 0\)&lt;/span&gt; the transition matrix of new chain approaches the transition matrix for the chain in Definition 28 (see the Appendix). We suggest setting &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; to &lt;span class="math inline"&gt;\(0.5\)&lt;/span&gt; in the absence of data from a user study.&lt;/p&gt;
&lt;h2 id="modeling-sbbfl"&gt;Modeling SBBFL&lt;/h2&gt;
&lt;p&gt;As noted earlier, Markov models can be constructed for alternative fault localization techniques. Suspicious Behavior Based Fault Localization (SBBFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; techniques return a report containing a ranked list of subgraphs or subtrees. Each subgraph contains multiple program locations usually drawn from a dynamic control flow graph of the program. Comparing this output to CBSFL using the Standard Rank Score can be difficult as a location may appear multiple times in graphs returned by the SBBFL algorithm. However, &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; can produce an accurate and comparable score by utilizing the expected hitting times of the states representing the faulty location. Definition 30 in the Appendix provides an example Markov chain which models a ranked list of suspicious subgraphs. It can be extended (not shown due to space constraints) to add a Jump matrix in the manner of Definition 29 (see the Appendix).&lt;/p&gt;
&lt;h1 id="case-study"&gt;Case Study&lt;/h1&gt;
&lt;p&gt;To illustrate our new approach to evaluation, we performed a case study in which it was used to evaluate several fault localization techniques of two different types: CBSFL &lt;span class="citation"&gt;[&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;], [&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt; and Suspicious-Behavior-Based Fault Localization (SBBFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. We investigated the following research questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RQ1&lt;/strong&gt;: How Accurate is &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Estimation? (Table III)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RQ2&lt;/strong&gt;: Does it make a difference whether &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; or the Standard Rank Score is used? (Table IV, Figs. 5 and 4)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also considered an important general question about fault localization that a typical research study might investigate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RQ3&lt;/strong&gt;: Which kind of fault localization technique performs better, SBBFL or CBSFL? (Table IV)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to use an SBBFL technique, more complex profiling data (dynamic control flow graphs or DCFGs) are needed than for CBSFL (which requires only coverage data). Therefore, a more specialized profiler was required for this study. We used Dynagrok (https://github.com/timtadh/dynagrok) &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; - a profiling tool for the Go Programming Language. We also reused subject programs used in a previous study &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; (see Table II). They are all real-world Go programs of various sizes that were injected with mutation faults. The test cases for the programs are all either real-world inputs or test cases from the system regression testing suites that are distributed with the programs. Six representative suspiciousness metrics were considered: Ochiai, F1, Jaccard, Relative Ochiai, Relative F1, and Relative Jaccard &lt;span class="citation"&gt;[&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;]&lt;/span&gt;. These measures were all previously adapted to the SBBFL context &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;Table II: Datasets used in the evaluation&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;Program&lt;/th&gt;
&lt;th align="right"&gt;L.O.C.&lt;/th&gt;
&lt;th align="right"&gt;Mutants&lt;/th&gt;
&lt;th align="left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;AVL (github.com/timtadh/dynagrok)&lt;/td&gt;
&lt;td align="right"&gt;483&lt;/td&gt;
&lt;td align="right"&gt;19&lt;/td&gt;
&lt;td align="left"&gt;An AVL tree&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Blackfriday (github.com/russross/blackfriday)&lt;/td&gt;
&lt;td align="right"&gt;8,887&lt;/td&gt;
&lt;td align="right"&gt;19&lt;/td&gt;
&lt;td align="left"&gt;Markdown processor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;HTML (golang.org/x/net/html)&lt;/td&gt;
&lt;td align="right"&gt;9,540&lt;/td&gt;
&lt;td align="right"&gt;20&lt;/td&gt;
&lt;td align="left"&gt;An HTML parser&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Otto (github.com/robertkrimen/otto)&lt;/td&gt;
&lt;td align="right"&gt;39,426&lt;/td&gt;
&lt;td align="right"&gt;20&lt;/td&gt;
&lt;td align="left"&gt;Javascript interpreter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;&lt;span&gt;&lt;code&gt;gc&lt;/code&gt;&lt;/span&gt; (go.googlesource.com/go)&lt;/td&gt;
&lt;td align="right"&gt;51,873&lt;/td&gt;
&lt;td align="right"&gt;16&lt;/td&gt;
&lt;td align="left"&gt;The Go compiler&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;p&gt;Note: The AVL tree is in the examples directory.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;All applications of techniques were replicated to address random variation in the results as the SBBFL algorithm that we used employs sampling &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. The results from the replications were averaged and, unless otherwise noted, the average Rank Scores are presented. Finally, the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; score was computed for 4 of the 5 programs. For the fifth program, the Go compiler, although &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; can be computed exactly, this requires so much time for each run to complete (&lt;span class="math inline"&gt;\(\approx\)&lt;/span&gt; 4 hours) that it precluded computing exact results for each program version (including all models for both SBBFL and CBSFL). Therefore, expected hitting times for the Go compiler were estimated using the method outlined in Section 21. Specifically, we collected 500 samples of hitting times of all states in the Markov debugging model by taking random walks (with a maximum walk length of 1,000,000 steps). The expected hitting times were estimated by taking the sample mean.&lt;/p&gt;
&lt;h2 id="the-chosen-textrmht_textrmrank-model"&gt;The Chosen &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Model&lt;/h2&gt;
&lt;p&gt;To avoid bias, it was important to choose the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; model before the case study was conducted, which we did. We used &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; with jumps (Definition 29) together with the jump matrix specified in Definition 26. We chose this matrix because we believe the order in which programmers examine program elements during debugging is driven in part by the structure of the program, even when they are debugging with the assistance of a fault localization report. The &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; probability was set to &lt;span class="math inline"&gt;\(0.5\)&lt;/span&gt; to indicate an equal chance of the programmer using or not using the fault localization report. A future large scale user study could empirically characterize the behavior of programmers while performing debugging to inform the choice of the &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; parameter. However, without such a study it is reasonable to use &lt;span class="math inline"&gt;\(0.5\)&lt;/span&gt;, which indicates "no information."&lt;/p&gt;
&lt;h2 id="rq1-how-accurate-is-textrmht_textrmrank-estimation"&gt;RQ1: How Accurate is &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Estimation?&lt;/h2&gt;
&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;&lt;span&gt;&lt;span class="math inline"&gt;Table III: \(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Estimation Error&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;Subject Program&lt;/th&gt;
&lt;th align="center"&gt;avl&lt;/th&gt;
&lt;th align="center"&gt;blackfriday&lt;/th&gt;
&lt;th align="center"&gt;html&lt;/th&gt;
&lt;th align="center"&gt;otto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;mean   median   stdev   p-value&lt;/td&gt;
&lt;td align="center"&gt;0.781&lt;/td&gt;
&lt;td align="center"&gt;0.469&lt;/td&gt;
&lt;td align="center"&gt;0.473&lt;/td&gt;
&lt;td align="center"&gt;0.476&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;p&gt;Percentage error of the estimated &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; versus the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; (see note under Def 22). Letting &lt;span class="math inline"&gt;\(y\)&lt;/span&gt; be the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; and &lt;span class="math inline"&gt;\(\hat{y}\)&lt;/span&gt; be the estimated &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;, the percentage error is &lt;span class="math inline"&gt;\(\frac{|\hat{y} - y|}{y} * 100\)&lt;/span&gt;. &lt;span&gt;-2.0em&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For large programs the cost of computing the expected hitting times for &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; may be too high. Therefore, we have suggested estimating the expected hitting times (see note under Definition 22). To assess the accuracy of estimating the expected hitting time instead of computing it exactly, we did both for four subject programs, using the Relative F1 measure. For each program version, SFL technique, and Markov chain type the estimation error was computed as a percentage of the true value. Table III presents descriptive statistics characterizing these errors for each subject program. The last row of the table gives p-values from a independent two-sample T-test comparing the estimated &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; values and the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; values. The null hypothesis is that the expected estimate &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; is the same as the expected exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. All p-values are well above the &lt;span class="math inline"&gt;\(0.05\)&lt;/span&gt; significance level that would suggest rejecting the null hypothesis. Therefore, we accept the null hypotheses that the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; estimates are not significantly different from the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; values. As indicated in the table, the maximum estimation error was around 3.7%. Therefore, if estimation is used we recommend considering &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; scores that are within 5% of each other to be equivalent.&lt;/p&gt;
&lt;h2 id="rq2-does-it-make-a-difference-whether-textrmht_textrmrank-or-the-standard-rank-score-is-used"&gt;RQ2: Does it make a difference whether &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; or the Standard Rank Score is used? &lt;/h2&gt;
&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;Table IV: Fault Localization Performance&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="center" colspan=3&gt;Standard Rank Score&lt;/th&gt;
&lt;th align="center" colspan=3&gt;HTRank Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;th align="left"&gt;score&lt;/td&gt;
&lt;th align="left"&gt;subject&lt;/td&gt;
&lt;th align="center"&gt;CBSFL&lt;/td&gt;
&lt;th align="center"&gt;SBBFL&lt;/td&gt;
&lt;th align="center"&gt; %&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt; &lt;/td&gt;
&lt;th align="center"&gt;CBSFL&lt;/td&gt;
&lt;th align="center"&gt;SBBFL&lt;/td&gt;
&lt;th align="center"&gt; %&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;9.1&lt;/td&gt;
&lt;td align="center"&gt;4.9&lt;/td&gt;
&lt;td align="center"&gt;-47 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;8.8&lt;/td&gt;
&lt;td align="center"&gt;4.4&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;td align="center"&gt;42.6&lt;/td&gt;
&lt;td align="center"&gt;11.0&lt;/td&gt;
&lt;td align="center"&gt;-74 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;40.7&lt;/td&gt;
&lt;td align="center"&gt;25.3&lt;/td&gt;
&lt;td align="center"&gt;-38 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.2&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;102.2&lt;/td&gt;
&lt;td align="center"&gt;21.5&lt;/td&gt;
&lt;td align="center"&gt;-79 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;264.8&lt;/td&gt;
&lt;td align="center"&gt;98.9&lt;/td&gt;
&lt;td align="center"&gt;-63 %&lt;/td&gt;
&lt;td align="center"&gt;1148.9&lt;/td&gt;
&lt;td align="center"&gt;1475.1&lt;/td&gt;
&lt;td align="center"&gt;28 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;7.6&lt;/td&gt;
&lt;td align="center"&gt;6.3&lt;/td&gt;
&lt;td align="center"&gt;-16 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;8.8&lt;/td&gt;
&lt;td align="center"&gt;4.4&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;td align="center"&gt;43.6&lt;/td&gt;
&lt;td align="center"&gt;9.8&lt;/td&gt;
&lt;td align="center"&gt;-78 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;38.2&lt;/td&gt;
&lt;td align="center"&gt;22.4&lt;/td&gt;
&lt;td align="center"&gt;-41 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.7&lt;/td&gt;
&lt;td align="center"&gt;6.8&lt;/td&gt;
&lt;td align="center"&gt;-22 %&lt;/td&gt;
&lt;td align="center"&gt;99.2&lt;/td&gt;
&lt;td align="center"&gt;102.6&lt;/td&gt;
&lt;td align="center"&gt;3 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;262.2&lt;/td&gt;
&lt;td align="center"&gt;101.6&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;2888.8&lt;/td&gt;
&lt;td align="center"&gt;2984.1&lt;/td&gt;
&lt;td align="center"&gt;3 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;10.1&lt;/td&gt;
&lt;td align="center"&gt;5.1&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;16.0&lt;/td&gt;
&lt;td align="center"&gt;12.1&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;80.0&lt;/td&gt;
&lt;td align="center"&gt;176.2&lt;/td&gt;
&lt;td align="center"&gt;120 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;33.3&lt;/td&gt;
&lt;td align="center"&gt;24.4&lt;/td&gt;
&lt;td align="center"&gt;-27 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.3&lt;/td&gt;
&lt;td align="center"&gt;-23 %&lt;/td&gt;
&lt;td align="center"&gt;75.2&lt;/td&gt;
&lt;td align="center"&gt;19.9&lt;/td&gt;
&lt;td align="center"&gt;-74 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;747.9&lt;/td&gt;
&lt;td align="center"&gt;482.0&lt;/td&gt;
&lt;td align="center"&gt;-36 %&lt;/td&gt;
&lt;td align="center"&gt;1074.1&lt;/td&gt;
&lt;td align="center"&gt;1540.5&lt;/td&gt;
&lt;td align="center"&gt;43 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;10.4&lt;/td&gt;
&lt;td align="center"&gt;5.1&lt;/td&gt;
&lt;td align="center"&gt;-51 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;16.0&lt;/td&gt;
&lt;td align="center"&gt;12.1&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;81.9&lt;/td&gt;
&lt;td align="center"&gt;218.8&lt;/td&gt;
&lt;td align="center"&gt;167 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;33.9&lt;/td&gt;
&lt;td align="center"&gt;18.6&lt;/td&gt;
&lt;td align="center"&gt;-45 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.2&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;81.0&lt;/td&gt;
&lt;td align="center"&gt;28.5&lt;/td&gt;
&lt;td align="center"&gt;-65 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;746.8&lt;/td&gt;
&lt;td align="center"&gt;470.4&lt;/td&gt;
&lt;td align="center"&gt;-37 %&lt;/td&gt;
&lt;td align="center"&gt;1047.0&lt;/td&gt;
&lt;td align="center"&gt;1537.9&lt;/td&gt;
&lt;td align="center"&gt;47 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;5.9&lt;/td&gt;
&lt;td align="center"&gt;6.3&lt;/td&gt;
&lt;td align="center"&gt;7 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;14.8&lt;/td&gt;
&lt;td align="center"&gt;10.3&lt;/td&gt;
&lt;td align="center"&gt;-30 %&lt;/td&gt;
&lt;td align="center"&gt;51.2&lt;/td&gt;
&lt;td align="center"&gt;108.7&lt;/td&gt;
&lt;td align="center"&gt;112 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;30.7&lt;/td&gt;
&lt;td align="center"&gt;25.3&lt;/td&gt;
&lt;td align="center"&gt;-18 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;10.3&lt;/td&gt;
&lt;td align="center"&gt;8.4&lt;/td&gt;
&lt;td align="center"&gt;-19 %&lt;/td&gt;
&lt;td align="center"&gt;312.8&lt;/td&gt;
&lt;td align="center"&gt;607.7&lt;/td&gt;
&lt;td align="center"&gt;94 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;1091.6&lt;/td&gt;
&lt;td align="center"&gt;773.5&lt;/td&gt;
&lt;td align="center"&gt;-29 %&lt;/td&gt;
&lt;td align="center"&gt;3137.2&lt;/td&gt;
&lt;td align="center"&gt;2988.8&lt;/td&gt;
&lt;td align="center"&gt;-5 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;10.1&lt;/td&gt;
&lt;td align="center"&gt;5.1&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;16.0&lt;/td&gt;
&lt;td align="center"&gt;12.1&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;77.8&lt;/td&gt;
&lt;td align="center"&gt;187.7&lt;/td&gt;
&lt;td align="center"&gt;141 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;33.3&lt;/td&gt;
&lt;td align="center"&gt;24.6&lt;/td&gt;
&lt;td align="center"&gt;-26 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.2&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;75.3&lt;/td&gt;
&lt;td align="center"&gt;24.7&lt;/td&gt;
&lt;td align="center"&gt;-67 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;747.9&lt;/td&gt;
&lt;td align="center"&gt;485.4&lt;/td&gt;
&lt;td align="center"&gt;-35 %&lt;/td&gt;
&lt;td align="center"&gt;1078.8&lt;/td&gt;
&lt;td align="center"&gt;1435.6&lt;/td&gt;
&lt;td align="center"&gt;33 %&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;p&gt;Summarizes the fault localization performance for CBSFL and SBBFL. Each fault localization technique is evaluated using both the Standard Rank Score and the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score (Defs. 28 and 25). The mean rank scores are shown as well as the percentage changes from CBSFL to SBBFL. Lower ranks scores indicate better fault localization performance. A negative percentage difference (%&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt;) indicates SBBFL &lt;strong&gt;improved&lt;/strong&gt; on CBSFL. A positive %&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt; indicates CBSFL &lt;strong&gt;outperformed&lt;/strong&gt; SBBFL.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src="images/scam/model-cmp-otto.png" title="Fig: 3" alt="Empirical probability density plots for the subject program Otto - showing the distributions of both the Standard Rank Scores and the HT_Rank Scores across all runs of all versions of Otto. The CBSFL technique is shown in blue and the SBBFL technique is shown in orange. The only suspciousness score used in this plot is RelativeF1." /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig 3:&lt;/strong&gt;
Empirical probability density plots for the subject program Otto - showing the distributions of both the Standard Rank Scores and the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Scores across all runs of all versions of Otto. The CBSFL technique is shown in blue and the SBBFL technique is shown in orange. The only suspciousness score used in this plot is RelativeF1. 
&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;img src="images/scam/cbsfl-vs-sbbfl-by-rank-score.png" alt=" Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and HT_Rank (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1. " /&gt;
&lt;img style="margin-top: -2em;" src="images/scam/cbsfl-vs-sbbfl-by-htrank.png" alt=" Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and HT_Rank (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1. " /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig 4:&lt;/strong&gt;
Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1.
&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Table IV details the fault localization performance we observed for CBSFL and SBBFL under the Standard Rank Score and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; using six different suspiciousness metrics. The results obtained with &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; were substantially different from those obtained with the Standard Rank Score, in terms of absolute ranks and percentage differences (%&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt;). The mean Standard Rank Score for SBBFL is lower than that for CBSFL for &lt;em&gt;every&lt;/em&gt; suspiciousness metric and subject program. By contrast, for some programs and metrics, the mean &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; scores for CBSFL are lower than those for SBBFL. The mean &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; scores are also higher than the mean Standard Rank Scores overall.&lt;/p&gt;
&lt;p&gt;Another way to look at the same phenomenon is shown in Figure 5, which displays empirical probability density plots for the program Otto. Each plot compares the performance of CBSFL to SBBFL using a different evaluation method. The top plot uses the Standard Rank Score while the other plot uses &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. As shown in the top plot, the Standard Rank Scores for both CBSFL and SBBFL are concentrated mainly between 0 and 15, with no values over 60. In the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; plot, by contrast, the scores for both CBSFL and SBBFL are much more widely dispersed. Figure 4 compares CBSFL to SBBFL with respect to average ranks (on a log scale), under the Standard Rank Score (top) and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; (bottom), for each version of the program Otto. The suspiciousness metric is RelativeF1. The results for &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; are quite distict from those for the Standard Rank Score, with CBSFL showing much more variability under &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These results provide confirmation for the theoretical motivation for &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. Recall that one of the problems with the Standard Rank Score is that it does not account for either the differing structures of fault localization reports or differences in report granularity. SBBFL and CBSFL differ in both structure (ranked CFG fragments vs. ranked basic blocks) and granularity (multiple basic blocks vs. lone basic blocks). We expected the Standard Rank Score to unfairly favor SBBFL because it does not account for these differences and that is exactly what we see in Table IV. Under the Standard Rank Score SBBFL outperforms CBSFL on every program using every suspiciousness score.&lt;/p&gt;
&lt;p&gt;To be explicit, SBBFL reports ranked CFG fragments, each of which contains multiple basic blocks. Under the Standard Rank Score those fragments are ranked and the score is the rank of the first fragment in which the bug appears. CBSFL will, by contrast, rank each basic block independently. Now, consider the case where SBBFL reports a CFG fragment at rank 1 that contains 10 basic blocks, one of which contains the bug. Suppose CBSFL also reports each of those basic blocks as maximally suspicious. The Standard Rank Score for CBSFL will be 5 while for SBBFL it will be 1. Thus, the Standard Rank Score is unfairly biased toward SBBFL and against CBSFL. This is once again reflected in Table IV. The new metric &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; does not suffer from this problem. As shown in the table and discussed below, SBBFL often but not always outperforms CBSFL under &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;, suggesting that the theoretical correction we expect is indeed occurring. We therefore conclude that &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; provides a better metric for comparison when reports differ in structure and granularity.&lt;/p&gt;
&lt;h2 id="rq3-which-kind-of-fault-localization-technique-performs-better-sbbfl-or-cbsfl"&gt;RQ3: Which kind of fault localization technique performs better, SBBFL or CBSFL?&lt;/h2&gt;
&lt;p&gt;Referring once again to Table IV, the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; results indicate that SBBFL often but not always outperformed CBSFL. In particular, when the measure used was Relative F1 (which was found to be the best-performing measure for SBBFL in &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;), SBBFL performed better for all programs but the compiler. However, when Ochiai was used CBSFL outperformed SBBFL, although CBSFL with Relative F1 outperforms CBSFL with Ochiai. This indicates that SBBFL and Relative F1 may be the best combination tested with one caveat. For the compiler, SBBFL never outperforms CBSFL. However, CBSFL also performs very badly on this large program. This indicates that while CBSFL beats SBBFL on this program neither technique is effective at localizing faults in it.&lt;/p&gt;
&lt;h2 id="study-limitations"&gt;Study Limitations&lt;/h2&gt;
&lt;p&gt;The purpose of our case study is to illustrate the application of &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. It was not designed to settle controversies about the suspiciousness metrics we employed. Second, this study used real programs and tests but used synthetic bugs. In the future, we intend to provide an automated analysis system that builds upon the Defects4J dataset &lt;span class="citation"&gt;[&lt;a href="#ref-Just2014"&gt;52&lt;/a&gt;]&lt;/span&gt;. Third, our study included a representative but not exhaustive set of suspiciousness metrics &lt;span class="citation"&gt;[&lt;a href="#ref-Lucia2014"&gt;2&lt;/a&gt;]&lt;/span&gt; and may not generalize to other metrics.&lt;/p&gt;
&lt;p&gt;Finally, no user-study was performed to validate the chosen debugging model against actions an actual programmer would take. Although we would have liked to perform such a study at this time, our resources do not permit us to do so. To be conclusive, such a study would need to be large-scale and to utilize professional programmers who are skilled in traditional debugging and are able to spend significant time learning to use SFL techniques with comparable skill.&lt;/p&gt;
&lt;h1 id="discussion"&gt;Discussion&lt;/h1&gt;
&lt;p&gt;A new, flexible approach to evaluating automatic fault localization techniques was presented. The new &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score provides a principled and flexible way of comparing different fault localization techniques. It is robust to differences in granularity and allows complex fault localization reports (such as those produced by SBBFL) to be incorporated. Unlike previous attempts at cross-technique comparison (see Section 12) the final scores are based on the expected number of steps through a Markov model. The model can incorporate both information from the chosen fault localization technique as well as other information available to the programmer (such as the structure of the program). The &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score is sensitive to the model used (see Fig 5) and hence the choice of model is important. The choice should be made based on 1) the fault localization technique(s) being evaluated and 2) observations of programmer behavior. Choosing a model after viewing the results (and picking the model that gives the "best" results) leads to a biased outcome.&lt;/p&gt;
&lt;h2 id="recommendations-for-researchers" class="unnumbered"&gt;Recommendations for Researchers&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Report both Standard Rank Score and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If evaluating multi-line faults the Steinmann Rank Score (Def. 18) should be used as the basis for defining the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Report which model is being used and why it was chosen, and include a description of the model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the absence of user study data, set &lt;span class="math inline"&gt;\(p_{\text{jump}} = .5\)&lt;/span&gt; and set the weights in &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; uniformly.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By evaluating fault localization methods with &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; in addition to the Standard Rank Score researchers will be able to make valid cross-technique comparisons. This will enable the community to better understand the relationships between techniques and report-granularity while taking into account potential programmer behavior during debugging.&lt;/p&gt;
&lt;h1 id="acknowledgement" class="unnumbered"&gt;Acknowledgement&lt;/h1&gt;
&lt;p&gt;This work was partially supported by NSF award CCF-1525178 to Case Western Reserve University.&lt;/p&gt;
&lt;h1 id="markov-chain-definitions"&gt;Markov Chain Definitions&lt;/h1&gt;
&lt;h4 id="definition.-cbsfl-ranked-list-markov-chain"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. CBSFL Ranked List Markov Chain&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;To construct a Markov chain representing a list of program locations ranked in descending order by their suspiciousness scores:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; be the set of locations in the program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For a location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; let &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; be its CBSFL suspiciousness score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Partition the locations in &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; into a list of groups &lt;span class="math inline"&gt;\(G = \left\{ g_1 \subseteq L, g_2 \subseteq L, ..., g_n \subseteq L \right\}\)&lt;/span&gt; such that for each group &lt;span class="math inline"&gt;\(g_i\)&lt;/span&gt; all of the locations it contains have the same score: &lt;span class="math inline"&gt;\( \forall ~ {g_i \in G}, ~ \forall ~ {l, l&amp;#39; \in g_i} \left[
        s(l) = s(l&amp;#39;) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The score of a group &lt;span class="math inline"&gt;\(s(g_i)\)&lt;/span&gt; is defined to be the common score of its members: &lt;span class="math inline"&gt;\( \forall ~ {l \in g_i} \left[ s(g_i) = s(l) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Order &lt;span class="math inline"&gt;\(G\)&lt;/span&gt; by the scores of its groups, such that &lt;span class="math inline"&gt;\(g_0\)&lt;/span&gt; has the highest score and &lt;span class="math inline"&gt;\(g_n\)&lt;/span&gt; has the lowest: &lt;span class="math inline"&gt;\(s(g_0) &amp;gt; s(g_1) &amp;gt; ... &amp;gt; s(g_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now construct the set of states. There is one state for each group &lt;span class="math inline"&gt;\(g
      \in G\)&lt;/span&gt; and for each location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt;. &lt;span class="math display"&gt;\[S =
          \left\{ g : g \in G \right\}
          \cup
          \left\{ l : l \in L \right\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally construct the transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; for the states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt;. &lt;span class="math display"&gt;\[{\bf P}_{i,j} =
      \left\{
        \begin{array}{lll}
           1 &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in G \wedge s_i \in s_j \\
          \\
             \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
             \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in L \wedge s_j \in s_i \\
          \\
            0
            &amp;amp; \text{otherwise}
        \end{array}
      \right.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h4 id="definition.-cbsfl-with-jumps-markov-chain"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. CBSFL with Jumps Markov Chain&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This definition augments Definition 28.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; be a "jump" matrix representing ways a programmer might move through the program during debugging.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class="math inline"&gt;\({\bf J}_{x,y} &amp;gt; 0\)&lt;/span&gt; then locations &lt;span class="math inline"&gt;\(x,y \in L\)&lt;/span&gt; are "connected" by &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; be the probability that when visiting a location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; the Markov process "jumps" to another location. Let &lt;span class="math inline"&gt;\(1 -
      p_{\text{jump}}\)&lt;/span&gt; be the probability that the process returns to the state which represents its group instead of jumping. As &lt;span class="math inline"&gt;\(p_\text{{jump}}
      \rightarrow 0\)&lt;/span&gt; the behavior of the chain approaches the behavior of the chain in Definition 28.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The new transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; for the states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt; is&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{aligned}
      {\bf P}_{i,j} &amp;amp;=
      \left\{
        \begin{array}{cll}
            1 - p_{\text{jump}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in G \wedge s_i \in s_j \\
            &amp;amp; &amp;amp;  \wedge \left( \sum_{k}{{\bf J}_{i,k}} \right) &amp;gt; 0 \\
          \\
              p_{\text{jump}}
              \left( \frac{{\bf J}_{i,j}}{\sum_{k}{{\bf J}_{i,k}}} \right)
            &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in L \wedge {\bf J}_{i,j} &amp;gt; 0 \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in L \wedge s_j \in s_i \\
          \\
            0
            &amp;amp; \text{otherwise}
        \end{array}
      \right.
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id="definition.-suspicious-behavior-markov-chain"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Suspicious Behavior Markov Chain&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;A chain that models a ranked list of suspicious subgraphs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(H\)&lt;/span&gt; be a set of suspicious subgraphs (behaviors).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For a subgraph &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt; let &lt;span class="math inline"&gt;\(\varsigma(h)\)&lt;/span&gt; be its suspiciousness score &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; be the set of locations in the program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Partition the subgraphs in &lt;span class="math inline"&gt;\(H\)&lt;/span&gt; into a list of groups &lt;span class="math inline"&gt;\(G = \left\{ g_1 \subseteq H, g_2 \subseteq H, ..., g_n \subseteq H \right\}\)&lt;/span&gt; such that for each group &lt;span class="math inline"&gt;\(g_i\)&lt;/span&gt; all of the locations in &lt;span class="math inline"&gt;\(g_i\)&lt;/span&gt; have the same score: &lt;span class="math inline"&gt;\(\forall ~ {g_i \in G} ~ \forall ~ {a, b \in g_i} \left[
        \varsigma(a) = \varsigma(b) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let the score of a group &lt;span class="math inline"&gt;\(\varsigma(g_i)\)&lt;/span&gt; be the same as the scores of its members: &lt;span class="math inline"&gt;\( \forall ~ {g_i \in G} ~ \forall ~ {h \in g_i} \left[
        \varsigma(g_i) = \varsigma(h) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Order &lt;span class="math inline"&gt;\(G\)&lt;/span&gt; by the scores of its groups, such that &lt;span class="math inline"&gt;\(g_0\)&lt;/span&gt; has the highest score and &lt;span class="math inline"&gt;\(g_n\)&lt;/span&gt; has the lowest: &lt;span class="math inline"&gt;\( \varsigma(g_0) &amp;gt; \varsigma(g_1) &amp;gt; ... &amp;gt;
      \varsigma(g_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now construct the set of states. One state for each group &lt;span class="math inline"&gt;\(g \in G\)&lt;/span&gt;, one state for each subgraph &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt;, and one state for each location &lt;span class="math inline"&gt;\(l
      \in V_{h}\)&lt;/span&gt; for all &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt;. &lt;span class="math display"&gt;\[S =
          \left\{ g : g \in G \right\}
          \cup
          \left\{ h : h \in H \right\}
          \cup
          \left\{ l : l \in V_h,~ \forall~ h \in H \right\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(c: L \rightarrow \mathbb{N}^{+}\)&lt;/span&gt; be a function that gives the number of subgraphs &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt; which a location &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; appears in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally construct the transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; for the states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt;. &lt;span class="math display"&gt;\[{\bf P}_{i,j} =
      \left\{
        \begin{array}{cll}
            \frac{1}{c(s_i)} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in H \wedge s_i \in V_{s_j} \\
         \\
            \frac{1}{2}\frac{1}{{{\left|{V_{s_i}}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in H \wedge s_j \in L \wedge s_j \in V_{s_i} \\
          \\
            \frac{1}{2} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in H \wedge s_j \in G \wedge s_i \in s_j \\
          \\
            \frac{{{\left|{H}\right|}} - 1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
            \frac{1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{H}\right|}} - 1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in H \wedge s_j \in s_i \\
          \\
            0
            &amp;amp; \text{otherwise}
        \end{array}
      \right.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;div id="refs" class="references"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="ref-Jones2002"&gt;
&lt;p&gt;[1] J. Jones, M. Harrold, and J. Stasko, "Visualization of test information to assist fault localization," &lt;em&gt;Proceedings of the 24th International Conference on Software Engineering. ICSE 2002&lt;/em&gt;, 2002, doi:&lt;a href="https://doi.org/10.1145/581339.581397"&gt;10.1145/581339.581397&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Lucia2014"&gt;
&lt;p&gt;[2] Lucia, D. Lo, L. Jiang, F. Thung, and A. Budi, "Extended comprehensive study of association measures for fault localization," &lt;em&gt;Journal of Software: Evolution and Process&lt;/em&gt;, vol. 26, no. 2, pp. 172-219, Feb. 2014, doi:&lt;a href="https://doi.org/10.1002/smr.1616"&gt;10.1002/smr.1616&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Sun2016"&gt;
&lt;p&gt;[3] S.-F. Sun and A. Podgurski, "Properties of Effective Metrics for Coverage-Based Statistical Fault Localization," in &lt;em&gt;2016 ieee international conference on software testing, verification and validation (icst)&lt;/em&gt;, 2016, pp. 124-134, doi:&lt;a href="https://doi.org/10.1109/ICST.2016.31"&gt;10.1109/ICST.2016.31&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Jones2005"&gt;
&lt;p&gt;[4] J. A. Jones and M. J. Harrold, "Empirical Evaluation of the Tarantula Automatic Fault-localization Technique," in &lt;em&gt;Proceedings of the 20th ieee/acm international conference on automated software engineering&lt;/em&gt;, 2005, pp. 273-282, doi:&lt;a href="https://doi.org/10.1145/1101908.1101949"&gt;10.1145/1101908.1101949&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Pearson2017"&gt;
&lt;p&gt;[5] S. Pearson, J. Campos, R. Just, G. Fraser, R. Abreu, M. D. Ernst, D. Pang, and B. Keller, "Evaluating and Improving Fault Localization," in &lt;em&gt;Proceedings of the 39th international conference on software engineering&lt;/em&gt;, 2017, pp. 609-620, doi:&lt;a href="https://doi.org/10.1109/ICSE.2017.62"&gt;10.1109/ICSE.2017.62&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Parnin2011"&gt;
&lt;p&gt;[6] C. Parnin and A. Orso, "Are Automated Debugging Techniques Actually Helping Programmers?" in &lt;em&gt;ISSTA&lt;/em&gt;, 2011, pp. 199-209.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Renieres2003"&gt;
&lt;p&gt;[7] M. Renieres and S. Reiss, "Fault localization with nearest neighbor queries," in &lt;em&gt;18th ieee international conference on automated software engineering, 2003. proceedings.&lt;/em&gt;, 2003, pp. 30-39, doi:&lt;a href="https://doi.org/10.1109/ASE.2003.1240292"&gt;10.1109/ASE.2003.1240292&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Cheng2009a"&gt;
&lt;p&gt;[8] H. Cheng, D. Lo, Y. Zhou, X. Wang, and X. Yan, "Identifying Bug Signatures Using Discriminative Graph Mining," in &lt;em&gt;Proceedings of the eighteenth international symposium on software testing and analysis&lt;/em&gt;, 2009, pp. 141-152, doi:&lt;a href="https://doi.org/10.1145/1572272.1572290"&gt;10.1145/1572272.1572290&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Henderson2018"&gt;
&lt;p&gt;[9] T. A. D. Henderson and A. Podgurski, "Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow Subgraphs," in &lt;em&gt;IEEE conference on software testing, validation and verification&lt;/em&gt;, 2018.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Baah2010"&gt;
&lt;p&gt;[10] G. G. K. Baah, A. Podgurski, and M. J. M. Harrold, "Causal inference for statistical fault localization," in &lt;em&gt;Proceedings of the 19th international symposium on software testing and analysis&lt;/em&gt;, 2010, pp. 73-84, doi:&lt;a href="https://doi.org/10.1145/1831708.1831717"&gt;10.1145/1831708.1831717&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Aggarwal2014"&gt;
&lt;p&gt;[11] C. C. Aggarwal and J. Han, Eds., &lt;em&gt;Frequent Pattern Mining&lt;/em&gt;. Cham: Springer International Publishing, 2014.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Agrawal1993"&gt;
&lt;p&gt;[12] R. Agrawal, T. Imieliński, and A. Swami, "Mining association rules between sets of items in large databases," &lt;em&gt;ACM SIGMOD Record&lt;/em&gt;, vol. 22, no. 2, pp. 207-216, Jun. 1993, doi:&lt;a href="https://doi.org/10.1145/170036.170072"&gt;10.1145/170036.170072&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Yan2002"&gt;
&lt;p&gt;[13] X. Yan and J. Han, "gSpan: graph-based substructure pattern mining," in &lt;em&gt;2002 ieee international conference on data mining, 2002. proceedings.&lt;/em&gt;, 2002, pp. 721-724, doi:&lt;a href="https://doi.org/10.1109/ICDM.2002.1184038"&gt;10.1109/ICDM.2002.1184038&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Aggarwal2014a"&gt;
&lt;p&gt;[14] C. C. Aggarwal, M. A. Bhuiyan, and M. A. Hasan, "Frequent Pattern Mining Algorithms: A Survey," in &lt;em&gt;Frequent pattern mining&lt;/em&gt;, Cham: Springer International Publishing, 2014, pp. 19-64.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Yan2008"&gt;
&lt;p&gt;[15] X. Yan, H. Cheng, J. Han, and P. S. Yu, "Mining Significant Graph Patterns by Leap Search," in &lt;em&gt;Proceedings of the 2008 acm sigmod international conference on management of data&lt;/em&gt;, 2008, pp. 433-444, doi:&lt;a href="https://doi.org/10.1145/1376616.1376662"&gt;10.1145/1376616.1376662&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Abreu2006"&gt;
&lt;p&gt;[16] R. Abreu, P. Zoeteweij, and A. Van Gemund, "An Evaluation of Similarity Coefficients for Software Fault Localization," in &lt;em&gt;2006 12th pacific rim international symposium on dependable computing (prdc'06)&lt;/em&gt;, 2006, pp. 39-46, doi:&lt;a href="https://doi.org/10.1109/PRDC.2006.18"&gt;10.1109/PRDC.2006.18&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Abreu2009"&gt;
&lt;p&gt;[17] R. Abreu, P. Zoeteweij, R. Golsteijn, and A. J. C. van Gemund, "A practical evaluation of spectrum-based fault localization," &lt;em&gt;Journal of Systems and Software&lt;/em&gt;, vol. 82, no. 11, pp. 1780-1792, 2009, doi:&lt;a href="https://doi.org/10.1016/j.jss.2009.06.035"&gt;10.1016/j.jss.2009.06.035&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Agarwal2014"&gt;
&lt;p&gt;[18] P. Agarwal and A. P. Agrawal, "Fault-localization Techniques for Software Systems: A Literature Review," &lt;em&gt;SIGSOFT Softw. Eng. Notes&lt;/em&gt;, vol. 39, no. 5, pp. 1-8, Sep. 2014, doi:&lt;a href="https://doi.org/10.1145/2659118.2659125"&gt;10.1145/2659118.2659125&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Wong2016"&gt;
&lt;p&gt;[19] W. E. Wong, R. Gao, Y. Li, R. Abreu, and F. Wotawa, "A Survey on Software Fault Localization," &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt;, vol. 42, no. 8, pp. 707-740, Aug. 2016, doi:&lt;a href="https://doi.org/10.1109/TSE.2016.2521368"&gt;10.1109/TSE.2016.2521368&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Zeller1999"&gt;
&lt;p&gt;[20] A. Zeller, "Yesterday, My Program Worked. Today, It Does Not. Why?" &lt;em&gt;SIGSOFT Softw. Eng. Notes&lt;/em&gt;, vol. 24, no. 6, pp. 253-267, Oct. 1999, doi:&lt;a href="https://doi.org/10.1145/318774.318946"&gt;10.1145/318774.318946&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Tip1995"&gt;
&lt;p&gt;[21] F. Tip, "A survey of program slicing techniques," &lt;em&gt;Journal of programming languages&lt;/em&gt;, vol. 3, no. 3, pp. 121-189, 1995.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Mao2014"&gt;
&lt;p&gt;[22] X. Mao, Y. Lei, Z. Dai, Y. Qi, and C. Wang, "Slice-based statistical fault localization," &lt;em&gt;Journal of Systems and Software&lt;/em&gt;, vol. 89, no. 1, pp. 51-62, 2014, doi:&lt;a href="https://doi.org/10.1016/j.jss.2013.08.031"&gt;10.1016/j.jss.2013.08.031&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Marcus2004"&gt;
&lt;p&gt;[23] A. Marcus, A. Sergeyev, V. Rajlieh, and J. I. Maletic, "An information retrieval approach to concept location in source code," &lt;em&gt;Proceedings - Working Conference on Reverse Engineering, WCRE&lt;/em&gt;, pp. 214-223, 2004, doi:&lt;a href="https://doi.org/10.1109/WCRE.2004.10"&gt;10.1109/WCRE.2004.10&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Zhou2012"&gt;
&lt;p&gt;[24] J. Zhou, H. Zhang, and D. Lo, "Where should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports," &lt;em&gt;Proceedings - International Conference on Software Engineering&lt;/em&gt;, pp. 14-24, 2012, doi:&lt;a href="https://doi.org/10.1109/ICSE.2012.6227210"&gt;10.1109/ICSE.2012.6227210&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Le2015"&gt;
&lt;p&gt;[25] T.-D. B. Le, R. J. Oentaryo, and D. Lo, "Information retrieval and spectrum based bug localization: better together," in &lt;em&gt;Proceedings of the 2015 10th joint meeting on foundations of software engineering - esec/fse 2015&lt;/em&gt;, 2015, pp. 579-590, doi:&lt;a href="https://doi.org/10.1145/2786805.2786880"&gt;10.1145/2786805.2786880&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Artzi2010"&gt;
&lt;p&gt;[26] S. Artzi, J. Dolby, F. Tip, and M. Pistoia, "Directed Test Generation for Effective Fault Localization," in &lt;em&gt;Proceedings of the 19th international symposium on software testing and analysis&lt;/em&gt;, 2010, pp. 49-60, doi:&lt;a href="https://doi.org/10.1145/1831708.1831715"&gt;10.1145/1831708.1831715&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Sahoo2013"&gt;
&lt;p&gt;[27] S. K. Sahoo, J. Criswell, C. Geigle, and V. Adve, "Using likely invariants for automated software fault localization," in &lt;em&gt;Proceedings of the eighteenth international conference on architectural support for programming languages and operating systems&lt;/em&gt;, 2013, vol. 41, p. 139, doi:&lt;a href="https://doi.org/10.1145/2451116.2451131"&gt;10.1145/2451116.2451131&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Perez2014"&gt;
&lt;p&gt;[28] A. Perez, R. Abreu, and A. Riboira, "A Dynamic Code Coverage Approach to Maximize Fault Localization Efficiency," &lt;em&gt;J. Syst. Softw.&lt;/em&gt;, vol. 90, pp. 18-28, Apr. 2014, doi:&lt;a href="https://doi.org/10.1016/j.jss.2013.12.036"&gt;10.1016/j.jss.2013.12.036&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Agrawal1995"&gt;
&lt;p&gt;[29] H. Agrawal, J. Horgan, S. London, and W. Wong, "Fault localization using execution slices and dataflow tests," in &lt;em&gt;Proceedings of sixth international symposium on software reliability engineering. issre'95&lt;/em&gt;, 1995, pp. 143-151, doi:&lt;a href="https://doi.org/10.1109/ISSRE.1995.497652"&gt;10.1109/ISSRE.1995.497652&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Cleve2005"&gt;
&lt;p&gt;[30] H. Cleve and A. Zeller, "Locating causes of program failures," &lt;em&gt;Proceedings of the 27th international conference on Software engineering - ICSE '05&lt;/em&gt;, p. 342, 2005, doi:&lt;a href="https://doi.org/10.1145/1062455.1062522"&gt;10.1145/1062455.1062522&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Horwitz1990"&gt;
&lt;p&gt;[31] S. Horwitz, "Identifying the Semantic and Textual Differences Between Two Versions of a Program," &lt;em&gt;SIGPLAN Not.&lt;/em&gt;, vol. 25, no. 6, pp. 234-245, Jun. 1990, doi:&lt;a href="https://doi.org/10.1145/93548.93574"&gt;10.1145/93548.93574&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Wong2008"&gt;
&lt;p&gt;[32] E. Wong, T. Wei, Y. Qi, and L. Zhao, "A Crosstab-based Statistical Method for Effective Fault Localization," in &lt;em&gt;2008 international conference on software testing, verification, and validation&lt;/em&gt;, 2008, pp. 42-51, doi:&lt;a href="https://doi.org/10.1109/ICST.2008.65"&gt;10.1109/ICST.2008.65&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Landsberg2015"&gt;
&lt;p&gt;[33] D. Landsberg, H. Chockler, D. Kroening, and M. Lewis, "Evaluation of Measures for Statistical Fault Localisation and an Optimising Scheme," in &lt;em&gt;International conference on fundamental approaches to software engineering&lt;/em&gt;, 2015, vol. 9033, pp. 115-129, doi:&lt;a href="https://doi.org/10.1007/978-3-662-46675-9"&gt;10.1007/978-3-662-46675-9&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Zheng2018"&gt;
&lt;p&gt;[34] Y. Zheng, Z. Wang, X. Fan, X. Chen, and Z. Yang, "Localizing multiple software faults based on evolution algorithm," &lt;em&gt;Journal of Systems and Software&lt;/em&gt;, vol. 139, pp. 107-123, 2018, doi:&lt;a href="https://doi.org/10.1016/j.jss.2018.02.001"&gt;10.1016/j.jss.2018.02.001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-ChaoLiu2006"&gt;
&lt;p&gt;[35] C. Liu, L. Fei, X. Yan, J. Han, and S. P. Midkiff, "Statistical debugging: A hypothesis testing-based approach," &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt;, vol. 32, no. 10, pp. 831-847, Oct. 2006, doi:&lt;a href="https://doi.org/10.1109/TSE.2006.105"&gt;10.1109/TSE.2006.105&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Ferrante1987"&gt;
&lt;p&gt;[36] J. Ferrante, K. J. Ottenstein, and J. D. Warren, "The program dependence graph and its use in optimization," vol. 9. pp. 319-349, Jul-1987.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Ali2009"&gt;
&lt;p&gt;[37] S. Ali, J. H. Andrews, T. Dhandapani, and W. Wang, "Evaluating the Accuracy of Fault Localization Techniques," &lt;em&gt;2009 IEEE/ACM International Conference on Automated Software Engineering&lt;/em&gt;, pp. 76-87, 2009, doi:&lt;a href="https://doi.org/10.1109/ASE.2009.89"&gt;10.1109/ASE.2009.89&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Moon2014"&gt;
&lt;p&gt;[38] S. Moon, Y. Kim, M. Kim, and S. Yoo, "Ask the Mutants: Mutating faulty programs for fault localization," &lt;em&gt;Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation, ICST 2014&lt;/em&gt;, pp. 153-162, 2014, doi:&lt;a href="https://doi.org/10.1109/ICST.2014.28"&gt;10.1109/ICST.2014.28&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Liu2005"&gt;
&lt;p&gt;[39] C. Liu, H. Yu, P. S. Yu, X. Yan, H. Yu, J. Han, and P. S. Yu, "Mining Behavior Graphs for ‘Backtrace' of Noncrashing Bugs," in &lt;em&gt;Proceedings of the 2005 siam international conference on data mining&lt;/em&gt;, 2005, pp. 286-297, doi:&lt;a href="https://doi.org/10.1137/1.9781611972757.26"&gt;10.1137/1.9781611972757.26&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-DiFatta2006"&gt;
&lt;p&gt;[40] G. Di Fatta, S. Leue, and E. Stegantova, "Discriminative Pattern Mining in Software Fault Detection," in &lt;em&gt;Proceedings of the 3rd international workshop on software quality assurance&lt;/em&gt;, 2006, pp. 62-69, doi:&lt;a href="https://doi.org/10.1145/1188895.1188910"&gt;10.1145/1188895.1188910&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Eichinger2008"&gt;
&lt;p&gt;[41] F. Eichinger, K. Böhm, and M. Huber, "Mining Edge-Weighted Call Graphs to Localise Software Bugs," in &lt;em&gt;European conference machine learning and knowledge discovery in databases&lt;/em&gt;, 2008, pp. 333-348, doi:&lt;a href="https://doi.org/10.1007/978-3-540-87479-9_40"&gt;10.1007/978-3-540-87479-9_40&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Eichinger2010"&gt;
&lt;p&gt;[42] F. Eichinger, K. Krogmann, R. Klug, and K. Böhm, "Software-defect Localisation by Mining Dataflow-enabled Call Graphs," in &lt;em&gt;Proceedings of the 2010 european conference on machine learning and knowledge discovery in databases: Part i&lt;/em&gt;, 2010, pp. 425-441.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Mousavian2011"&gt;
&lt;p&gt;[43] Z. Mousavian, M. Vahidi-Asl, and S. Parsa, "Scalable Graph Analyzing Approach for Software Fault-localization," in &lt;em&gt;Proceedings of the 6th international workshop on automation of software test&lt;/em&gt;, 2011, pp. 15-21, doi:&lt;a href="https://doi.org/10.1145/1982595.1982599"&gt;10.1145/1982595.1982599&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Eichinger2011"&gt;
&lt;p&gt;[44] F. Eichinger, C. Oßner, and K. Böhm, "Scalable software-defect localisation by hierarchical mining of dynamic call graphs," &lt;em&gt;Proceedings of the 11th SIAM International Conference on Data Mining, SDM 2011&lt;/em&gt;, no. c, pp. 723-734, 2011.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Parsa2011"&gt;
&lt;p&gt;[45] S. Parsa, S. A. Naree, and N. E. Koopaei, "Software Fault Localization via Mining Execution Graphs," in &lt;em&gt;Proceedings of the 2011 international conference on computational science and its applications - volume part ii&lt;/em&gt;, 2011, pp. 610-623.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Mariani2011"&gt;
&lt;p&gt;[46] L. Mariani, F. Pastore, and M. Pezze, "Dynamic Analysis for Diagnosing Integration Faults," &lt;em&gt;IEEE Trans. Softw. Eng.&lt;/em&gt;, vol. 37, no. 4, pp. 486-508, Jul. 2011, doi:&lt;a href="https://doi.org/10.1109/TSE.2010.93"&gt;10.1109/TSE.2010.93&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Yousefi2013"&gt;
&lt;p&gt;[47] A. Yousefi and A. Wassyng, "A Call Graph Mining and Matching Based Defect Localization Technique," in &lt;em&gt;2013 ieee sixth international conference on software testing, verification and validation workshops&lt;/em&gt;, 2013, pp. 86-95, doi:&lt;a href="https://doi.org/10.1109/ICSTW.2013.17"&gt;10.1109/ICSTW.2013.17&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Ioannidis2005"&gt;
&lt;p&gt;[48] J. P. A. Ioannidis, "Why most published research findings are false," &lt;em&gt;PLoS Medicine&lt;/em&gt;, vol. 2, no. 8, pp. 0696-0701, 2005, doi:&lt;a href="https://doi.org/10.1371/journal.pmed.0020124"&gt;10.1371/journal.pmed.0020124&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Grinstead2012"&gt;
&lt;p&gt;[49] C. M. Grinstead and J. L. Snell, &lt;em&gt;Introduction to Probability&lt;/em&gt;, 2nd ed. Providence, RI: American Mathematical Society, 1997.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Kemeny1960"&gt;
&lt;p&gt;[50] J. G. Kemeny and J. L. Snell, &lt;em&gt;Finite Markov Chains&lt;/em&gt;, First. Princeton, NJ: Van Nostrand, 1960.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Davis2004"&gt;
&lt;p&gt;[51] T. A. Davis, "Algorithm 832: UMFPACK V4.3—an Unsymmetric-pattern Multifrontal Method," &lt;em&gt;ACM Trans. Math. Softw.&lt;/em&gt;, vol. 30, no. 2, pp. 196-199, Jun. 2004, doi:&lt;a href="https://doi.org/10.1145/992200.992206"&gt;10.1145/992200.992206&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Just2014"&gt;
&lt;p&gt;[52] R. Just, D. Jalali, and M. D. Ernst, "Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs," in &lt;em&gt;Proceedings of the 2014 international symposium on software testing and analysis&lt;/em&gt;, 2014, pp. 437-440, doi:&lt;a href="https://doi.org/10.1145/2610384.2628055"&gt;10.1145/2610384.2628055&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Steimann2013"&gt;
&lt;p&gt;[53] F. Steimann, M. Frenkel, and R. Abreu, "Threats to the validity and value of empirical assessments of the accuracy of coverage-based fault locators," &lt;em&gt;Proceedings of the 2013 International Symposium on Software Testing and Analysis - ISSTA 2013&lt;/em&gt;, p. 314, 2013, doi:&lt;a href="https://doi.org/10.1145/2483760.2483767"&gt;10.1145/2483760.2483767&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Jones &lt;em&gt;et al.&lt;/em&gt; did not use the term "suspiciousness score" or "suspiciousness metric" in their 2002 paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt;. They introduced the term "suspiciousness score" in their 2005 paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;, in the context of ranking statements. Both terms are now in common use.&lt;a href="#fnref1"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;A dependence sphere is computed from the Program Dependence Graph (PDG) &lt;span class="citation"&gt;[&lt;a href="#ref-Horwitz1990"&gt;31&lt;/a&gt;], [&lt;a href="#ref-Ferrante1987"&gt;36&lt;/a&gt;]&lt;/span&gt;. In a PDG, program elements are nodes and their &lt;em&gt;control&lt;/em&gt; and &lt;em&gt;data&lt;/em&gt; dependencies are represented as edges. Given two nodes &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; a graph with a shortest path (ignoring edge directionality) &lt;span class="math inline"&gt;\(\pi\)&lt;/span&gt; between them, the sphere is all those nodes in the graph have have a path from &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; as short (or shorter) than &lt;span class="math inline"&gt;\(\pi\)&lt;/span&gt; (once again ignoring edge directionality).&lt;a href="#fnref2"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Implementations of these two computations maybe found in &lt;span&gt;&lt;code&gt;https://github.com/timtadh/expected-hitting-times&lt;/code&gt;&lt;/span&gt;.&lt;a href="#fnref3"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;See https://hackthology.com/pdfs/scam-2019-supplement.pdf&lt;a href="#fnref4"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Paper"></category></entry><entry><title>Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow Subgraphs</title><link href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html" rel="alternate"></link><published>2018-04-03T00:00:00-04:00</published><updated>2018-04-03T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt; and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2018-04-03:/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html</id><summary type="html">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow
Subgraphs&lt;/em&gt;.  &lt;a href="http://www.es.mdh.se/icst2018/"&gt;ICST 2018&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1109/ICST.2018.00019"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2018.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present a new algorithm, Score Weighted Random Walks (SWRW), for behavioral
fault localization. Behavioral fault localization localizes faults (bugs) in
programs to a group of interacting …&lt;/p&gt;</summary><content type="html">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow
Subgraphs&lt;/em&gt;.  &lt;a href="http://www.es.mdh.se/icst2018/"&gt;ICST 2018&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1109/ICST.2018.00019"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2018.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present a new algorithm, Score Weighted Random Walks (SWRW), for behavioral
fault localization. Behavioral fault localization localizes faults (bugs) in
programs to a group of interacting program elements such as basic blocks or
functions.  SWRW samples suspicious (or discriminative) subgraphs from
basic-block level dynamic control flow graphs collected during the execution of
passing and failing tests.  The suspiciousness of a subgraph may be measured by
any one of a family of new metrics adapted from probabilistic formulations of
existing coverage-based statistical fault localization metrics.  We conducted an
empirical evaluation of SWRW with nine subgraph-suspiciousness measures on five
real-world subject programs.  The results indicate that SWRW outperforms
previous fault localization techniques based on discriminative subgraph mining.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Automated fault localization&lt;/em&gt; techniques have been developed to help
programmers locate software faults (bugs) responsible for observed software
failures.  Many of these techniques are statistical in nature (e.g., {&lt;a href="https://dx.doi.org/10.1145/581339.581397"&gt;Jones
2002&lt;/a&gt;, &lt;a href="https://dx.doi.org/10.1145/1064978.1065014"&gt;Liblit
2005&lt;/a&gt;, &lt;a href="https://dx.doi.org/10.1137/1.9781611972757.26"&gt;Liu
2005&lt;/a&gt;}).  They employ statistical
measures of the &lt;em&gt;association&lt;/em&gt;, if any, between the occurrence of failures and
the execution of particular program elements like statements or conditional
branches.  The program elements that are most strongly associated with failures
are identified as "suspicious", so that developers can examine them to see if
they are faulty.  The association measures that are used are often called
&lt;em&gt;suspiciousness metrics&lt;/em&gt; {&lt;a href="https://dx.doi.org/10.1109/ICSE.2004.1317420"&gt;Jones
2004&lt;/a&gt;}.  Such &lt;em&gt;statistical fault
localization&lt;/em&gt; (SFL) techniques typically require &lt;em&gt;execution profiles&lt;/em&gt; (or
&lt;em&gt;spectra&lt;/em&gt;) and PASS/FAIL labels for a set of both passing and failing program
runs.  Each profile entry characterizes the execution of a
particular program element during a run.  For example, a statement-coverage
profile for a run indicates which statements were executed at least once.  The
profiles are collected with program instrumentation, while the labels are
typically supplied by software testers or end users.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Coverage Based Statistical Fault Localization&lt;/em&gt; (CBSFL) techniques compute a
suspiciousness score for each program element. The elements are presented to the
programmer as a ranked list with the most suspicious elements at the top
{&lt;a href="https://dx.doi.org/10.1145/1064978.1065014"&gt;Liblit 2005&lt;/a&gt;}.  Programmers
utilize the list to guide their debugging effort by starting at the most
suspicious element and moving down. The effectiveness of a CBSFL suspiciousness
metric is judged by how accurately it ranks a bug's (or bugs) location in a
program. The higher in the list the bug's location appears the better the metric
performs {&lt;a href="https://dx.doi.org/10.1002/smr.1616"&gt;Lucia 2014&lt;/a&gt;}.&lt;/p&gt;
&lt;p&gt;Kochhar &lt;em&gt;et al.&lt;/em&gt;  {&lt;a href="https://dx.doi.org/10.1145/2931037.2931051"&gt;Kochhar 2016&lt;/a&gt;}
recently surveyed 386 software engineering practitioners about their
expectations for automated fault localization. While practitioners indicated
their preference for very accurate algorithms, over 85% of respondents also
indicated their preference for tools which help them understand the output of
fault localization algorithms. This is an important finding as most statistical
approaches do not provide an explanation of their results. The SFL techniques
often simply compute suspiciousness measures and rank the program elements
accordingly.  These rankings may be helpful, but without more information
programmers could overlook the faulty element even when it is ranked highly.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Localization Process" src="/images/icst-localization-process.png"&gt;&lt;/p&gt;
&lt;div style="text-align: center; margin-top: -2em;"&gt;
&lt;strong&gt;Figure 1.&lt;/strong&gt; Process for localizing faults  with discriminative graph mining.
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Suspicious-Behavior Based Fault Localization&lt;/em&gt; (SBBFL) is a statistical fault
localization technique that aids the programmer in understanding suspiciousness
scores by providing a &lt;em&gt;context&lt;/em&gt; of interacting elements.
&lt;span style="color: gray"&gt;(&lt;em&gt;NB: Dynamic slicing {&lt;a href="https://www.franktip.org/pubs/jpl1995.pdf"&gt;Tip
1995&lt;/a&gt;} also provides such a context, but does
not in itself involve suspiciousness measures&lt;/em&gt;)&lt;/span&gt; Instead of implicating a
single element, SBBFL implicates a larger runtime behavior (see process in
Figure 1). The implicated control flow paths (or subgraphs) may help the
programmer understand the nature of a bug {&lt;a href="https://dx.doi.org/10.1145/1572272.1572290"&gt;Cheng
2009&lt;/a&gt;}.&lt;/p&gt;
&lt;p&gt;We present a new algorithm, &lt;em&gt;Score-Weighted Random Walks&lt;/em&gt; (SWRW), for
behavioral fault-localization.  SWRW belongs to a family of
&lt;em&gt;discriminative graph-mining algorithms&lt;/em&gt;  that have previously been used
for behavioral fault localization 
{
&lt;a href="https://dx.doi.org/10.1137/1.9781611972757.26"&gt;Liu 2005&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1145/1188895.1188910"&gt;DiFatta 2006&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1007/978-3-540-87479-9_40"&gt;Eichinger 2008&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1145/1572272.1572290"&gt;Cheng 2009&lt;/a&gt;,
&lt;a href="http://dl.acm.org/citation.cfm?id=1888258.1888293"&gt;Eichinger 2010&lt;/a&gt;,
&lt;a href="http://www.scopus.com/inward/record.url?eid=2-s2.0-84880082474&amp;amp;partnerID=tZOtx3y1"&gt;Eichinger 2011&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1145/1982595.1982599"&gt;Mousavian 2011&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1109/TSE.2010.93"&gt;Mariani 2011&lt;/a&gt;,
&lt;a href="http://dl.acm.org/citation.cfm?id=2029256.2029305"&gt;Parsa 2011&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1109/ICSTW.2013.17"&gt;Yousefi 2013&lt;/a&gt;
}. Graph mining is very powerful in principle but algorithms must
make trade-offs to address the challenging combinatorics of the graph mining
problem.  Our new algorithm, SWRW, mitigates the combinatorics by randomly
sampling "suspicious" subgraphs from dynamic control flow graphs.  During the
sampling process, the most suspicious subgraphs (as judged by a suitable
suspiciousness metric) are favored for selection. Unlike previous algorithms,
SWRW can be used with a wide variety of suspiciousness metrics --- which allows
it to use better metrics than available to previous work.  Even when using the
same metric as similar algorithms, SWRW localizes faults more accurately than
they do.&lt;/p&gt;
&lt;h2&gt;Summary of Contributions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A new behavioral fault localization algorithm, SWRW, that samples suspicious
   subgraphs from dynamic control flow graphs.  Unlike similar algorithms, SWRW
   can be used with a variety of suspiciousness metrics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New generalizations of existing suspiciousness metrics that allow them to be
   applied to behaviors represented by subgraphs of dynamic control flow graphs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An empirical study whose results suggest that SWRW is more accurate than
   similar algorithms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dynagrok, a new instrumentation, mutation, and analysis tool for the Go
   programming language.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Dynagrok: A New Profiling Tool&lt;/h1&gt;
&lt;p&gt;All &lt;em&gt;Coverage-Based Statistical Fault Localization&lt;/em&gt; (CBSFL) techniques use
&lt;em&gt;coverage profiles&lt;/em&gt; to gather information on how software behaved when
executed on a set of test inputs. A coverage profile typically contains an entry
for each program element of a given kind (e.g., statement, basic block, branch,
or function), which records whether (and possibly how many times) the element
was executed during the corresponding program run.  The profiles and PASS/FAIL
labels for all tests are then used to compute a statistical
&lt;em&gt;suspiciousness score&lt;/em&gt; for each program element.&lt;/p&gt;
&lt;p&gt;The process of gathering the coverage information from running programs is
called &lt;em&gt;profiling&lt;/em&gt; and there are many different varieties of profilers and
profiling techniques available. Coverage profiling is a simple and widely
implemented technique, which is why it has been widely used by the fault
localization community. Another technique is &lt;em&gt;tracing&lt;/em&gt;, which logs the sequence
of program locations as they are executed. The traces provide detailed
information on the behavior of the program but could grow to be very large for
long running programs. This paper uses &lt;em&gt;execution flow profiling&lt;/em&gt; which
computes the dynamic interprocedural control flow graph of a program's
execution.  This provides some of the benefits of tracing without recording an
excessive amount of data.&lt;/p&gt;
&lt;p&gt;To capture execution flow profiles we developed
&lt;a href="https://github.com/timtadh/dynagrok"&gt;Dynagrok&lt;/a&gt;, a new analysis, instrumentation
and mutation platform for the Go programming language. Go is a newer language
(2009) from Google that has been seeing increasing adoption in industry. It has
been adopted for web programming, systems programming, "DevOps," network
programing, and databases {&lt;a href="http://tiobe.com/tiobe-index/"&gt;http://tiobe.com/tiobe-index/&lt;/a&gt;,
&lt;a href="http://blog.golang.org/survey2017-results"&gt;http://blog.golang.org/survey2017-results&lt;/a&gt;}.  Dynagrok builds upon the
&lt;em&gt;abstract syntax tree&lt;/em&gt; (AST) representation provided by the Go standard
library.&lt;/p&gt;
&lt;p&gt;
Dynagrok collects profiles by inserting instrumentation into the AST of the
subject program.  The profiles currently collected are &lt;em&gt;dynamic control flow
graphs&lt;/em&gt; (DCFGs) whose vertices represent basic blocks. A &lt;em&gt;basic
block&lt;/em&gt; is a sequence of program operations that can only be entered at the
start of the sequence and can only be exited after the last operation in the
sequence 
{&lt;a href="https://www.worldcat.org/title/compilers-principles-techniques-and-tools/oclc/12285707"&gt;Aho 2007&lt;/a&gt;}.
A basic-block level &lt;em&gt;control flow graph&lt;/em&gt; (CFG) is a directed labeled
graph &lt;span class="math"&gt;\(g = (V, E, l)\)&lt;/span&gt; comprised of a finite set of
vertices &lt;span class="math"&gt;\(V\)&lt;/span&gt;, a set of edges &lt;span class="math"&gt;\(E
\subseteq V \times V\)&lt;/span&gt;, and a labeling function &lt;span
class="math"&gt;\(l\)&lt;/span&gt; mapping vertices and edges to labels.  Each vertex
&lt;span class="math"&gt;\(v \in V\)&lt;/span&gt; represents a basic block of the program.
Each edge &lt;span class="math"&gt;\((u, v) \in E\)&lt;/span&gt; represents a transition in
program execution from block &lt;span class="math"&gt;\(u\)&lt;/span&gt; to block &lt;span
class="math"&gt;\(v\)&lt;/span&gt;. The labeling function &lt;span class="math"&gt;\(l\)&lt;/span&gt;
labels the basic blocks with a unique identifier (e.g.
&lt;code&gt;function-name:block-id&lt;/code&gt;), which is consistently applied across
multiple executions but is never repeated in the same execution.
&lt;/p&gt;

&lt;p&gt;Figure 2 shows an example DCFG collected by Dynagrok for a simple program that
computes terms of the Fibonacci sequence. To collect such graphs Dynagrok parses
the program into an AST using Go's standard library.  Dynagrok then uses a
custom control flow analysis to build static control flow graphs. Each basic
block holds pointers to the statements inside of the AST. The blocks also have a
pointer to the enclosing &lt;em&gt;lexical block&lt;/em&gt; in the AST.  Using this information,
Dynagrok inserts profiling instructions into the AST at the beginning of each
basic block. The instructions inserted by Dynagrok use its &lt;code&gt;dgruntime&lt;/code&gt; library
to track the control flow of each thread (which is called a &lt;em&gt;goroutine&lt;/em&gt; in Go).
When the program shuts down (either normally or abnormally) the &lt;code&gt;dgruntime&lt;/code&gt;
library merges the flow graphs from all the threads together and writes out the
result.&lt;/p&gt;
&lt;p&gt;(&lt;a href="https://hackthology.com/pdfs/icst-2018.pdf"&gt;Read the rest of the paper as a pdf&lt;/a&gt;)&lt;/p&gt;
&lt;div style="text-align: center;"&gt;
&lt;img src="/images/icst-dcfg.png"
     text="Example Dynamic Control Flow Graph"
     style="width: 65%;"/&gt;
&lt;/div&gt;
&lt;div style="text-align: center; margin-top: -1em;"&gt;
&lt;strong&gt;Figure 2.&lt;/strong&gt; 
The dynamic control flow graph (DCFG) for the program &lt;br&gt;
in  Listing 1 (see below).
&lt;/div&gt;
&lt;div style="width: 90%; left:05%; position: relative;"&gt;
Each vertex is a basic block with a basic block identifer (e.g.
&lt;code&gt;b1&lt;/code&gt;) that, in conjunction with the name of the containing function,
serves as the label for the block (e.g. &lt;code&gt;main:b1&lt;/code&gt;). Each edge shows
the number of traversals taken during the execution of the program.  Note that
the loop update blocks (&lt;code&gt;main:b3&lt;/code&gt; and &lt;code&gt;fib:b7&lt;/code&gt;) will not
be in the profiles because Dynagrok instruments the Go source code and profiling
instructions cannot be syntactically inserted in those locations.  The
instrumented program is shown on the right.
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nb"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div style="text-align: center; margin-top: -1em;"&gt;
&lt;strong&gt;Listing 1.&lt;/strong&gt; 
An example Go program to compute the Fibonacci Sequence.
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dgruntime&amp;quot;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;defer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Shutdown&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}()&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;main&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;defer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ExitFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;main&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}()&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlkFromCond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nb"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fib&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;defer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ExitFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fib&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}()&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlkFromCond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div style="text-align: center; margin-top: -1em;"&gt;
&lt;strong&gt;Listing 2.&lt;/strong&gt; 
The program shown in Listing 1 after the instrumentation has been inserted by
dynagrok.
&lt;/div&gt;</content><category term="Paper"></category></entry><entry><title>Frequent Subgraph Analysis and its Software Engineering Applications</title><link href="https://hackthology.com/frequent-subgraph-analysis-and-its-software-engineering-applications.html" rel="alternate"></link><published>2017-08-18T00:00:00-04:00</published><updated>2017-08-18T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;</name></author><id>tag:hackthology.com,2017-08-18:/frequent-subgraph-analysis-and-its-software-engineering-applications.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;. &lt;em&gt;Frequent Subgraph Analysis and its Software Engineering Applications&lt;/em&gt;.
&lt;a href="http://case.edu/"&gt;Case Western Reserve University&lt;/a&gt;. Doctoral Dissertation. 2017.
&lt;br/&gt;
&lt;a href="https://hackthology.com/pdfs/dissertation.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-analysis-and-its-software-engineering-applications.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Frequent subgraph analysis is a class of techniques and algorithms to find
repeated sub-structures in graphs known as frequent subgraphs or graph
patterns. In the field of Software …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;. &lt;em&gt;Frequent Subgraph Analysis and its Software Engineering Applications&lt;/em&gt;.
&lt;a href="http://case.edu/"&gt;Case Western Reserve University&lt;/a&gt;. Doctoral Dissertation. 2017.
&lt;br/&gt;
&lt;a href="https://hackthology.com/pdfs/dissertation.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-analysis-and-its-software-engineering-applications.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Frequent subgraph analysis is a class of techniques and algorithms to find
repeated sub-structures in graphs known as frequent subgraphs or graph
patterns. In the field of Software Engineering, graph pattern discovery can
help detect semantic code duplication, locate the root cause of bugs, infer
program specifications, and even recommend intelligent auto-complete
suggestions.  Outside of Software Engineering, discovering graph patterns has
enabled important applications in personalized medicine, computer aided drug
design, computer vision, and multimedia.&lt;/p&gt;
&lt;p&gt;As promising as much of the previous work in areas such as semantic code
duplication detection has been, finding all of the patterns in graphs of a
large program's code has previously proven intractable.  Part of what makes
discovering all graphs patterns in a graph of a large program difficult is the
very large number of frequent subgraphs contained in graphs of large programs.
Another impediment arises when graphs contain frequent patterns with many
automorphisms and overlapping embeddings. Such patterns are pathologically
difficult to mine and are found in real programs.&lt;/p&gt;
&lt;p&gt;I present a family of algorithms and techniques for frequent subgraph analysis
with two specific aims. One, address pathological structures. Two, enable
important software engineering applications such as code clone detection and
fault localization without analyzing all frequent subgraphs. The first aim is
addressed by novel optimizations making the system faster and more scalable
than previously published work on both program graphs and other difficult to
mine graphs. The second aim is addressed by new algorithms for sampling,
ranking, and grouping frequent patterns.  Experiments and theoretical results
show the tractability of these new techniques.&lt;/p&gt;
&lt;p&gt;The power of frequent subgraph mining in Software Engineering is demonstrated
with studies on duplicate code (code clone) identification and fault
localization.  Identifying code clones from program dependence graphs allows
the identification of potential semantic clones. The proposed sampling
techniques enable tractable dependence clone identification and analysis.
Fault localization identifies potential locations for the root cause of bugs
in programs. Frequent substructures in dynamic program behavior graphs to
identify suspect behaviors which are further isolated with fully automatic
test case minimization and generation.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frequent subgraph analysis&lt;/em&gt; (FSA) is a family of techniques to discover
recurring subgraphs in graph databases. The databases can either be composed of
many individual graphs or a single large connected graph. This dissertation
discusses my contributions to frequent subgraph analysis and applies the
technique to address two pressing problems in software engineering: code clone
detection and automatic fault localization.&lt;/p&gt;
&lt;p&gt;The work on frequent subgraph analysis was motivated by the software engineering
problems. Large programs are composed of repeated patterns arising organically
through the process of program construction. Some regions of programs are
duplicated (intentionally or unintentionally). The duplicated regions are
referred to as &lt;em&gt;code clones&lt;/em&gt; (or just &lt;em&gt;clones&lt;/em&gt;). Other regions are
similar to each other because they perform similar tasks or share development
histories.&lt;/p&gt;
&lt;p&gt;Code clones may arise from programmers copying and pasting code, from
limitations of a programming language, from using certain APIs, from following
coding conventions, or from a variety of other causes.  Whatever their causes,
the existing clones in a code base need to be managed. When a programmer
modifies a region of code that is cloned in another location in the program
they should make an active decision whether or not to modify the other location.
Clearly, such decisions can only be made if the programmer is aware of the other
location.&lt;/p&gt;
&lt;p&gt;One type of duplication which is particularly difficult to detect is so called
&lt;em&gt;Type-4 clones&lt;/em&gt; or &lt;em&gt;semantic clones&lt;/em&gt;. Semantic clones are semantically
equivalent regions of code which may or may not be textually similar.
Differences could be small changes such as different variable names or large
changes such as a different algorithms which perform the same function. In
general identifying semantically equivalent regions is undecidable as a
reduction from the halting problem.&lt;/p&gt;
&lt;p&gt;Frequent subgraph analysis (FSA) can be used to identify some &lt;em&gt;Type-4&lt;/em&gt; clones
(as well as easier to identify clone classes). I use FSA to analyze a graphical
representation of the program called the &lt;em&gt;Program Dependence Graph&lt;/em&gt; (PDG)
{&lt;a href="https://doi.org/10.1145/24039.24041"&gt;Ferrante 1987&lt;/a&gt;}. Dependence graphs strip
away syntactic information and focus on the semantic relationships between
operations.  Non-semantic re-orderings of operations in a program do not effect
the structure of its dependence graph {&lt;a href="https://doi.org/10.1145/93548.93574"&gt;Horwitz
1990&lt;/a&gt;, &lt;a href="https://doi.org/10.1145/75309.75328"&gt;Podgurski
1989&lt;/a&gt;, &lt;a href="https://doi.org/10.1109/32.58784"&gt;Podgurski
1990&lt;/a&gt;}. Since PDGs are not sensitive to
unimportant syntactic changes some of the &lt;em&gt;Type-4&lt;/em&gt; clones in a program may
be identified with FSA.&lt;/p&gt;
&lt;p&gt;The other motivating application I applied FSA to is automatic fault
localization.  When programs have faults, defects, or bugs it is often time
consuming and sometimes difficult to find the cause of the bug. To address this
the software engineering community has been working on a variety of techniques
for &lt;em&gt;automatic fault localization&lt;/em&gt;.  The family of statistical fault
localization techniques analyzes the behavior of the program when the faults
manifest and when they do not. These techniques then identify statistical
associations between execution of particular program elements and the occurrence
of program failures.&lt;/p&gt;
&lt;p&gt;While statistical measures can identify suspicious elements of a program they
are blind to the relationships between the elements. If program behavior is
modeled through &lt;em&gt;Dynamic Control Flow Graphs&lt;/em&gt;, then execution relationships
between operations can be analyzed using FSA to identify suspicious
interactions. These suspicious interactions represent larger &lt;em&gt;behaviors&lt;/em&gt; of
the program which are statistically associated with program failure. The
behaviors serve as a context of interacting suspicious program elements which
potentially makes it easier for programmers to comprehend localization results.&lt;/p&gt;
&lt;p&gt;Much of the previous work in frequent subgraph analysis has focused on finding
all of the frequent subgraphs in a graph database (called &lt;em&gt;frequent subgraph
mining&lt;/em&gt; {&lt;a href="https://dx.doi.org/10.1007/3-540-45372-5_2"&gt;Inokuchi 2000&lt;/a&gt;}). I
have shown that finding all of the frequent subgraphs in a database of graphs is
not an efficient or effective way to either detect code clones or automatically
localize faults. Program dependence graphs of large programs have huge numbers
of recurring subgraphs.  Experiments on a number of open source projects (see
Chapter 4) showed that moderately sized Java programs (~70 KLOC) have more than
a hundred of million subgraphs that recur five or more times. Mining all
recurring subgraphs is an impractical way to either identify code clones or
localize faults.&lt;/p&gt;
&lt;p&gt;Furthermore, it turns out that program dependence graphs are particularly
difficult to analyze for recurring subgraphs. These graphs often have certain
structures which contain many &lt;em&gt;automorphisms&lt;/em&gt;. A structure with an automorphism
can be rotated upon itself. Each rotation appears to be a recurrence to
traditional frequent subgraph mining algorithms. However, because it is merely a
rotation, humans (e.g. programmers) do not perceive these rotations as instances
of duplication.&lt;/p&gt;
&lt;p&gt;To enable scalable frequent subgraph analysis of large programs new techniques
were needed. I developed novel optimizations for mining frequent subgraphs and
created a state of the art miner (REGRAX) for connected graphs (Chapter 3).  To
detect code clones from program dependence graphs, I developed an algorithm
(GRAPLE) to collect a representative sample of recurring subgraphs (Chapter 4).
Finally, a new algorithm (SWRW) was created for localizing faults from dynamic
control flow graphs, which outperforms previous algorithms (Chapter 6).&lt;/p&gt;
&lt;p&gt;REGRAX contains low level optimizations to the process of identifying frequent
subgraphs. Chapter 2 provides the necessary background on frequent subgraph
mining for understanding these optimizations. An extensive empirical study was
conducted on REGRAX to quantify the effect of each of the new optimizations on
databases from the SUBDUE corpus {&lt;a href="https://dl.acm.org/citation.cfm?id=1618595.1618605"&gt;Cook
1994&lt;/a&gt;}, on program
dependence graphs, and on random graphs.&lt;/p&gt;
&lt;p&gt;GRAPLE is a new algorithm to sample a representative set of frequent subgraphs
and estimate statistics characterizing properties of the set of all frequent
subgraphs. The sampling algorithm uses the theory of absorbing Markov chains to
model the process of extracting recurring subgraphs from a large connected
graph. By sampling a representative set of recurring subgraphs GRAPLE is able to
conduct frequent subgraph analysis on large programs which normally would not be
amenable to such analysis.&lt;/p&gt;
&lt;p&gt;One of the questions in code clone detection is: "are code clones detected from
program dependence graphs understandable to programmers?" GRAPLE was used to
answer this question, as it not only collects a sample of frequent subgraphs but
allows researchers to estimate the prevalence of features across the entire
population of frequent subgraphs (including those which were not sampled).
Chapter 4 details a case study which was conducted at a software company to
determine whether their programmers could make use of code clones detected from
program dependence graphs. The study would not have been possible without the
estimation framework in GRAPLE, as the software contained too many code clones
to be reviewed in the allocated budget.&lt;/p&gt;
&lt;p&gt;To apply frequent subgraph analysis to automatic fault localization, a new
algorithm named Score Weighted Random Walks (SWRW) was developed. SWRW samples
discriminative, suspicious, or significant subgraphs from a database of graphs.
The database is split into multiple classes where some graphs are labeled
"positive" and others "negative." In fault localization the "positive" graphs
were those dynamic control flow graphs collected from program executions which
exhibited a failure of some type. The "negative" graphs are from executions
which did not fail.&lt;/p&gt;
&lt;p&gt;SWRW, like GRAPLE, models the problem using the theory of absorbing Markov
chains. Unlike GRAPLE, it uses an &lt;em&gt;objective function&lt;/em&gt; (drawn from the
statistical fault localization literature {&lt;a href="https://dx.doi.org/10.1002/smr.1616"&gt;Lucia
2014&lt;/a&gt;}) to guide the sampling process. In
comparison to previous work in fault localization using graph mining, a much
wider variety of objective functions can applied. This allows for functions
better suited to statistical fault localization to be used as the objective
function. SWRW outperforms previous approaches which used discriminative mining
to localize faults in terms of fault localization accuracy.&lt;/p&gt;
&lt;h4&gt;Summary&lt;/h4&gt;
&lt;p&gt;This dissertation makes important and novel contributions to frequent subgraph
analysis which enable scalable semantic code clone detection and behavioral
fault localization. These advances can help programmers maintain their software
more efficiently leading to more stable and secure software for everyone. The
software engineering advances are built on new frequent subgraph analysis
algorithms. The new algorithms improve code clone detection time, fault
localization latency and accuracy, and enable analysis of larger and more
complex programs.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hackthology.com/pdfs/dissertation.pdf"&gt;Read the full dissertation&lt;/a&gt;.&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Rethinking Dependence Clones</title><link href="https://hackthology.com/rethinking-dependence-clones.html" rel="alternate"></link><published>2017-02-21T00:00:00-05:00</published><updated>2017-02-21T00:00:00-05:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt; and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2017-02-21:/rethinking-dependence-clones.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Rethinking Dependence Clones&lt;/em&gt;.
&lt;a href="https://iwsc2017.github.io/"&gt;IWSC 2017&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/IWSC.2017.7880512"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/iwsc-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/rethinking-dependence-clones.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Semantic code clones&lt;/em&gt; are regions of duplicated code that may appear dissimilar
but compute similar functions. Since in general it is algorithmically
undecidable whether two or more programs compute the same function, locating all …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Rethinking Dependence Clones&lt;/em&gt;.
&lt;a href="https://iwsc2017.github.io/"&gt;IWSC 2017&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/IWSC.2017.7880512"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/iwsc-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/rethinking-dependence-clones.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Semantic code clones&lt;/em&gt; are regions of duplicated code that may appear dissimilar
but compute similar functions. Since in general it is algorithmically
undecidable whether two or more programs compute the same function, locating all
semantic code clones is infeasible. One way to dodge the undecidability issue
and find potential semantic clones, using only static information, is to search
for recurring subgraphs of a &lt;em&gt;program dependence graph&lt;/em&gt; (PDG).  PDGs represent
control and data dependence relationships between statements or operations in a
program.  PDG-based clone detection techniques, unlike syntactically-based
techniques, do not distinguish between code fragments that differ only because
of dependence-preserving statement re-orderings, which also preserve semantics.
Consequently, they detect clones that are difficult to find by other means.
Despite this very desirable property, work on PDG-based clone detection has
largely stalled, apparently because of concerns about the scalability of the
approach.  We argue, however, that the time has come to reconsider PDG-based
clone detection, as a part of a holistic strategy for clone management.  We
present evidence that its scalability problems are not as severe as previously
thought.  This suggests the possibility of developing integrated clone
management systems that fuse information from multiple clone detection methods,
including PDG-based ones.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Fragments of similar code are typically scattered throughout large code bases.
These repeated fragments or &lt;em&gt;code clones&lt;/em&gt; often result from programmers copying
and pasting code.  Code clones (or just &lt;em&gt;clones&lt;/em&gt;) may also result from
limitations of a programming language, use of certain APIs or design patterns,
following coding conventions, or a variety of other causes.  Whatever their
causes, existing clones need to be managed. When a programmer modifies a region
of code that is cloned in another location in the program, they should make an
active decision whether or not to modify the other location.  Clearly, such
decisions can only be made if the programmer is aware of the other location.&lt;/p&gt;
&lt;p&gt;In general, there are 4 types of code clones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type-1 Clones -&lt;/strong&gt; Identical regions of code (excepting whitespace and
  comments).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type-2 Clones -&lt;/strong&gt; Syntactically equivalent regions (excepting names,
  literals, types, and comments).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type-3 Clones -&lt;/strong&gt; Syntactically similar regions (as in Type-2) but with
  minor differences such as statement additions or deletions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type-4 Clones -&lt;/strong&gt; Regions of code with functionally equivalent behavior but
  possibly with different syntactic structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Much of the research on code clone detection and maintenance has been geared
toward Type-1 and Type-2 clone, as they are easier to detect and validate than
Type-3 and Type-4 clones.  The two most popular detection methods involve
searching for clones in &lt;em&gt;token streams&lt;/em&gt; and &lt;em&gt;abstract syntax trees&lt;/em&gt; (ASTs).&lt;/p&gt;
&lt;p&gt;An alternative approach to clone detection is to search for them in a &lt;em&gt;Program
Dependence Graph&lt;/em&gt; (PDG), which represents the control and data dependences
between statements or operations in a program.  Recurring subgraphs in PDGs
represent potential &lt;em&gt;dependence clones&lt;/em&gt;.  Some of the previous work on PDG-based
clone detection used forward and backward path-slicing to find clones.  This
method can detect matching slices, but it cannot detect all recurring subgraphs.
The latter can be identified using &lt;em&gt;frequent subgraph mining&lt;/em&gt; (FSM).  However,
for low frequency thresholds, the number of PDG subgraphs discovered by FSM may
be enormous.  For example, we found that for a Java program with 70,000 lines of
code (LOC), over 700 million PDG subgraphs with 5 or more instances were
discovered by FSM.&lt;/p&gt;
&lt;p&gt;Since it is infeasible for developers to examine so many subgraphs, we
previously developed &lt;a href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html"&gt;GRAPLE&lt;/a&gt;, an algorithm to
select representative samples of maximal frequent subgraphs.  In this paper, the
core sampling process remains the same as in GRAPLE but we present a new
algorithm for traversing the &lt;em&gt;k&lt;/em&gt;-frequent subgraph lattice.  One tricky aspect
of FSM is how to define exactly what "frequency" means in a large connected
graph. In order to handle pathological cases that occur in real programs, we
introduce a new metric to measure subgraph frequency (or "support"), called
the &lt;em&gt;Greedy Independent Subgraphs&lt;/em&gt; (GIS) measure.  The results section details
the first empirical examination of the scalability and speed of sampling
dependence clones from large programs. The study showed that our new system can
quickly sample from programs with 500 KLOC of code and successfully sample from
programs with perhaps 2 MLOC. Finally, since at times the sampling algorithm may
return several potential clones, which are quite similar to each other, we
evaluate the performance of a density-based clustering algorithm on the samples
collected.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;p&gt;See the &lt;a href="https://hackthology.com/pdfs/iwsc-2017.pdf"&gt;PDF&lt;/a&gt; for the complete paper. IEEE has
the exclusive rights to publish this paper. Follow the
&lt;a href="https://doi.org/10.1109/IWSC.2017.7880512"&gt;DOI&lt;/a&gt; for the IEEE copy.&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Frequent Subgraph Mining of Personalized Signaling Pathway Networks Groups Patients with Frequently Dysregulated Disease Pathways and Predicts Prognosis</title><link href="https://hackthology.com/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html" rel="alternate"></link><published>2017-01-03T00:00:00-05:00</published><updated>2017-01-03T00:00:00-05:00</updated><author><name>Arda Durmaz, &lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, Doug Brubaker, and &lt;a href="http://gurkan.case.edu"&gt;Gurkan Bebek&lt;/a&gt;</name></author><id>tag:hackthology.com,2017-01-03:/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html</id><summary type="html">&lt;p&gt;A. Durmaz*, &lt;strong&gt;T. A. D. Henderson&lt;/strong&gt;*, D.  Brubaker, and G. Bebek. &lt;em&gt;Frequent
Subgraph Mining of Personalized Signaling Pathway Networks Groups Patients with
Frequently Dysregulated Disease Pathways and Predicts Prognosis.&lt;/em&gt;
&lt;a href="http://psb.stanford.edu/"&gt;PSB 2017&lt;/a&gt;.  * &lt;strong&gt;Co-First Author&lt;/strong&gt;
&lt;br/&gt;
&lt;a href="http://dx.doi.org/10.1142/9789813207813_0038"&gt;DOI&lt;/a&gt;.
&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/27896993"&gt;PUBMED&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;h4&gt;Motivation&lt;/h4&gt;
&lt;p&gt;Large scale genomics studies have generated comprehensive molecular
characterization of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A. Durmaz*, &lt;strong&gt;T. A. D. Henderson&lt;/strong&gt;*, D.  Brubaker, and G. Bebek. &lt;em&gt;Frequent
Subgraph Mining of Personalized Signaling Pathway Networks Groups Patients with
Frequently Dysregulated Disease Pathways and Predicts Prognosis.&lt;/em&gt;
&lt;a href="http://psb.stanford.edu/"&gt;PSB 2017&lt;/a&gt;.  * &lt;strong&gt;Co-First Author&lt;/strong&gt;
&lt;br/&gt;
&lt;a href="http://dx.doi.org/10.1142/9789813207813_0038"&gt;DOI&lt;/a&gt;.
&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/27896993"&gt;PUBMED&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;h4&gt;Motivation&lt;/h4&gt;
&lt;p&gt;Large scale genomics studies have generated comprehensive molecular
characterization of numerous cancer types. Subtypes for many tumor types have
been established; however, these classifications are based on molecular
characteristics of a small gene sets with limited power to detect dysregulation
at the patient level.  We hypothesize that frequent graph mining of pathways to
gather pathways functionally relevant to tumors can characterize tumor types and
provide opportunities for personalized therapies.&lt;/p&gt;
&lt;h4&gt;Results&lt;/h4&gt;
&lt;p&gt;In this study we present an integrative omics approach to group patients based
on their altered pathway characteristics and show prognostic differences within
breast cancer (p &amp;lt; 9.57e-10) and glioblastoma multiforme (p &amp;lt; 0.05) patients. We
were able validate this approach in secondary RNA-Seq datasets with p &amp;lt; 0.05 and
p &amp;lt; 0.01 respectively. We also performed pathway enrichment analysis to further
investigate the biological relevance of dysregulated pathways. We compared our
approach with network-based classifier algorithms and showed that our
unsupervised approach generates more robust and biologically relevant clustering
whereas previous approaches failed to report specific functions for similar
patient groups or classify patients into prognostic groups.&lt;/p&gt;
&lt;h4&gt;Conclusions&lt;/h4&gt;
&lt;p&gt;These results could serve as a means to improve
prognosis for future cancer patients, and to provide opportunities for
improved treatment options and personalized interventions.  The proposed novel 
graph mining approach is able to integrate PPI networks 
with gene expression in a biologically sound approach and cluster patients in to
clinically distinct groups. We have utilized breast cancer and glioblastoma
multiforme datasets from microarray and RNA-Seq platforms and identified
disease mechanisms differentiating samples.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;p&gt;See the &lt;a href="https://hackthology.com/pdfs/psb-2017.pdf"&gt;PDF&lt;/a&gt; for the complete paper. Follow the
&lt;a href="http://dx.doi.org/10.1142/9789813207813_0038"&gt;DOI&lt;/a&gt; for the authoritative
version.&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Sampling Code Clones from Program Dependence Graphs with GRAPLE</title><link href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html" rel="alternate"></link><published>2016-11-13T00:00:00-05:00</published><updated>2016-11-13T00:00:00-05:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt; and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2016-11-13:/sampling-code-clones-from-program-dependence-graphs-with-graple.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski. &lt;em&gt;Sampling Code Clones from Program
Dependence Graphs with GRAPLE&lt;/em&gt;.
&lt;a href="http://softwareanalytics.ca/swan16/Home.html"&gt;SWAN 2016&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1145/2989238.2989241"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://github.com/timtadh/graple"&gt;CODE&lt;/a&gt;.
&lt;a href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present &lt;a href="https://github.com/timtadh/graple"&gt;GRAPLE&lt;/a&gt;, a method to generate a
representative sample of recurring (frequent) subgraphs of any directed labeled
graph(s).  &lt;code&gt;GRAPLE&lt;/code&gt; is based on frequent subgraph …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski. &lt;em&gt;Sampling Code Clones from Program
Dependence Graphs with GRAPLE&lt;/em&gt;.
&lt;a href="http://softwareanalytics.ca/swan16/Home.html"&gt;SWAN 2016&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1145/2989238.2989241"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://github.com/timtadh/graple"&gt;CODE&lt;/a&gt;.
&lt;a href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present &lt;a href="https://github.com/timtadh/graple"&gt;GRAPLE&lt;/a&gt;, a method to generate a
representative sample of recurring (frequent) subgraphs of any directed labeled
graph(s).  &lt;code&gt;GRAPLE&lt;/code&gt; is based on frequent subgraph mining, absorbing Markov
chains, and Horvitz-Thompson estimation. It can be used to sample any kind of
graph representation for programs. One of many software engineering applications
for finding recurring subgraphs is detecting duplicated code (code clones) from
representations such as program dependence graphs (PDGs) and abstract syntax
trees.  To assess the usefulness of clones detected from PDGs, we conducted a
case study on a 73 KLOC commercial Android application developed over 5 years.
Nine of the application's developers participated. To our knowledge, it is the
first study to have professional developers examine code clones detected from
PDGs.  We describe a new PDG generation tool
&lt;a href="https://github.com/timtadh/jpdg"&gt;jpdg&lt;/a&gt; for JVM languages, which was used to
generate the dependence graphs used in the study.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Code clones&lt;/em&gt; are similar fragments of program code. They can arise from copying
and pasting, using certain design patterns or certain APIs, or adhering to
coding conventions, among other causes. Code clones create maintenance hazards,
because they often require subtle context-dependent adaptation and because other
changes must be applied to each member of a clone class. To manage clone
evolution the clones must first be found. Clones can be detected using any
program representation: source code text, tokens, abstract syntax trees (ASTs),
flow graphs, dependence graphs, etc. Each representation has advantages and
disadvantages for clone detection.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PDG-based clone detection&lt;/em&gt; finds &lt;em&gt;dependence clones&lt;/em&gt; corresponding to recurring
subgraphs of a program dependence graph (PDG).  Since PDGs are oblivious to
semantics preserving statement reorderings they are well suited to detect
&lt;em&gt;semantic&lt;/em&gt; (functionally equivalent) clones. A number of algorithms find clones
from PDGs.  However, as Bellon notes, "PDG based techniques are computationally
expensive and often report non-contiguous clones that may not be perceived as
clones by a human evaluator." Most PDG-based clone detection tools are biased,
detecting certain clones but not others.&lt;/p&gt;
&lt;p&gt;The root cause of scalability problems with PDG-based clone detection is the
number of dependence clones. The Background Section (in the
&lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;pdf&lt;/a&gt;) illustrates this with an example in which
we used an unbiased frequent subgraph mining algorithm to detect all dependence
clones in Java programs. In programs with about 70 KLOC it detected around 10
million clones before disk space was exhausted. Processing all dependence clones
is impractical even for modestly sized programs.&lt;/p&gt;
&lt;p&gt;Instead of exhaustively enumerating all dependence clones, an unbiased random
sample can be used to statistically estimate parameters of the whole
"population" of clones, such as the prevalence of clones exhibiting properties
of interest.  For these reasons, we developed a statistically unbiased method
for &lt;em&gt;sampling&lt;/em&gt; dependence clones and for &lt;em&gt;estimating&lt;/em&gt; parameters of the whole
clone population.&lt;/p&gt;
&lt;p&gt;We present &lt;a href="https://github.com/timtadh/graple"&gt;GRAPLE (GRAph samPLE)&lt;/a&gt;, a method
to generate a representative sample of recurring subgraphs of any directed
labeled graph(s). It can be used to sample subgraphs from any kind of program
graph representation.  &lt;code&gt;GRAPLE&lt;/code&gt; is not a general purpose clone detector but it
can answer questions about dependence clones that other PDG-based clone
detection tools cannot.  We conducted a preliminary case study on a commercial
application and had its developers evaluate whether the sampled subgraphs
represented code duplication.  To our knowledge, it is the first study to have
professional programmers examine dependence clones.  &lt;code&gt;GRAPLE&lt;/code&gt; has applications
in bug mining, test case selection, and bioinformatics. The sampling algorithm
also applies to frequent item sets, subsequences, and subtrees allowing code
clone sampling from tokens and ASTs.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;p&gt;See the &lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;PDF&lt;/a&gt; for the complete paper. ACM
has the exclusive rights to publish this paper. Tim Henderson and Andy Podgurski
own the copyright. Follow the &lt;a href="https://dx.doi.org/10.1145/2989238.2989241"&gt;DOI&lt;/a&gt;
for the ACM copy. This copy is posted here with the permission of ACM.&lt;/p&gt;</content><category term="Paper"></category></entry></feed>