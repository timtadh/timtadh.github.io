<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Hackthology - Paper</title><link href="https://hackthology.com/" rel="alternate"></link><link href="https://hackthology.com/feeds/paper.atom.xml" rel="self"></link><id>https://hackthology.com/</id><updated>2023-04-17T00:00:00-04:00</updated><entry><title>Flake Aware Culprit Finding</title><link href="https://hackthology.com/flake-aware-culprit-finding.html" rel="alternate"></link><published>2023-04-17T00:00:00-04:00</published><updated>2023-04-17T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, Bobby Dorward, Eric Nickell, Collin Johnston, Avi Kondareddy</name></author><id>tag:hackthology.com,2023-04-17:/flake-aware-culprit-finding.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy.
&lt;em&gt;Flake Aware Culprit Finding&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/home/icst-2023"&gt;ICST Industry Track 2023&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/flake-aware-culprit-finding.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;When a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Bobby Dorward, Eric Nickell, Collin Johnston, and Avi Kondareddy.
&lt;em&gt;Flake Aware Culprit Finding&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/home/icst-2023"&gt;ICST Industry Track 2023&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/flake-aware-culprit-finding.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/icst-2023.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;When a change introduces a bug into a large software repository, there is
often a delay between when the change is committed and when bug is detected.
This is true even when the bug causes an existing test to fail! These delays
are caused by resource constraints which prevent the organization from running
all of the tests on every change. Due to the delay, a Continuous Integration
system needs to locate buggy commits. Locating them is complicated by flaky
tests that pass and fail non-deterministically. The flaky tests introduce
noise into the CI system requiring costly reruns to determine if a failure was
caused by a bad code change or caused by non-deterministic test behavior. This
paper presents an algorithm, &lt;em&gt;Flake Aware Culprit Finding&lt;/em&gt;, that locates
buggy commits more accurately than a traditional bisection search. The
algorithm is based on Bayesian inference and noisy binary search, utilizing
prior information about which changes are most likely to contain the bug. A
large scale empirical study was conducted at Google on 13,000+ test breakages.
The study evaluates the accuracy and cost of the new algorithm versus a
traditional deflaked bisection search.&lt;/p&gt;
&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;h1 id="sec:introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Fast, collaborative, large scale development is enabled by the use of
Continuous Integration in a &lt;em&gt;mono-repository&lt;/em&gt; (monorepo)
environment where all of the source code is stored in a single shared
repository &lt;span class="citation" data-cites="Potvin2016"&gt;(&lt;a
href="#ref-Potvin2016" role="doc-biblioref"&gt;Potvin and Levenberg
2016&lt;/a&gt;)&lt;/span&gt;. In a monorepo, all developers share a single source of
truth for the state of the code and most builds are made from “head”
(the most recent commit to the repository) using the source code for all
libraries and dependencies rather than versioned pre-compiled archives.
This enables (among other things) unified versioning, atomic changes,
large scale refactorings, code re-use, cross team collaboration, and
flexible code ownership boundaries.&lt;/p&gt;
&lt;p&gt;Very large scale monorepos (such as those at Google &lt;span
class="citation" data-cites="Potvin2016 Memon2017"&gt;(&lt;a
href="#ref-Potvin2016" role="doc-biblioref"&gt;Potvin and Levenberg
2016&lt;/a&gt;; &lt;a href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al.
2017&lt;/a&gt;)&lt;/span&gt;, Microsoft &lt;span class="citation"
data-cites="Herzig2015a"&gt;(&lt;a href="#ref-Herzig2015a"
role="doc-biblioref"&gt;Herzig and Nagappan 2015&lt;/a&gt;)&lt;/span&gt;, and Facebook
&lt;span class="citation" data-cites="Machalica2019"&gt;(&lt;a
href="#ref-Machalica2019" role="doc-biblioref"&gt;Machalica et al.
2019&lt;/a&gt;)&lt;/span&gt;) require advanced systems for ensuring changes
submitted to the repository are properly tested and validated.
&lt;em&gt;Continuous Integration&lt;/em&gt; (CI) &lt;span class="citation"
data-cites="Fowler2006"&gt;(&lt;a href="#ref-Fowler2006"
role="doc-biblioref"&gt;Fowler 2006&lt;/a&gt;)&lt;/span&gt; is a system and practice of
automatically integrating changes into the code repository that serves
as the source of truth for the organization. In modern environments,
integration involves not only performing the textual merge required to
add the change but also verification tasks such as ensuring the software
compiles and the automated tests pass both on the new change before it
is integrated &lt;em&gt;and after&lt;/em&gt; it has been incorporated into the main
development branch.&lt;/p&gt;

&lt;p&gt;&lt;span id="fig:ci-timeline" label="fig:ci-timeline"&gt;
&lt;a href="images/icst-2023/fig1.png"&gt;&lt;img alt="Figure 1" src="images/icst-2023/fig1.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Due to resource constraints, tests are only run at specific versions
(called &lt;em&gt;milestones&lt;/em&gt; at Google) and many tests may be skipped
because they were not affected by the intervening changes (as determined
by static build dependencies) &lt;span class="citation"
data-cites="Gupta2011"&gt;(&lt;a href="#ref-Gupta2011"
role="doc-biblioref"&gt;Gupta, Ivey, and Penix 2011&lt;/a&gt;)&lt;/span&gt;. This
process is illustrated in Figure &lt;a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt;. A test is
considered passing at a version &lt;span class="math inline"&gt;\(b\)&lt;/span&gt;
if and only if it has a passing result at the most recent preceding
affecting version. Formally, if there exists a version &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt; where the test is passing and &lt;span
class="math inline"&gt;\(a \le b\)&lt;/span&gt; and there does not exist an
intervening version &lt;span class="math inline"&gt;\(c\)&lt;/span&gt;, &lt;span
class="math inline"&gt;\(a &amp;lt; c \le b\)&lt;/span&gt;, which affects the test
then the test is passing at version &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt;. A project’s status is considered
passing at a version &lt;span class="math inline"&gt;\(b\)&lt;/span&gt; if all tests
are passing (using the above definition) at version &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If a test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; is failing
(consistently) at a milestone version &lt;span
class="math inline"&gt;\(m\)&lt;/span&gt;, it may not necessarily mean that
version &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; introduced a change which
broke test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;. As shown in Figure &lt;a
href="#fig:ci-timeline" data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt; one or more
changes between &lt;span class="math inline"&gt;\(m\)&lt;/span&gt; and the previous
milestone at which test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; was run
may have introduced a breaking change. Figuring out which version
introduced the &lt;em&gt;breakage&lt;/em&gt; is called &lt;em&gt;culprit finding&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Many companies now employ automated culprit finding systems. These
systems typically implement a (possibly n-way) “bisect search” similar
to the one built into the Git version control system &lt;span
class="citation" data-cites="Couder2008"&gt;(&lt;a href="#ref-Couder2008"
role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;. Git’s bisect command runs
a binary search on the versions between the first known breaking version
and the last known passing version to identify the version which broke
the test.&lt;/p&gt;
&lt;p&gt;In the Google environment, there are two problems with a traditional
bisection algorithm &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;: (1)
it does not account for flaky (non-deterministic) tests, and (2) when
using &lt;span class="math inline"&gt;\(k\)&lt;/span&gt;-reruns to deflake the
search accuracy plateaus while build cost continues to increase
linearly. &lt;strong&gt;This paper proposes&lt;/strong&gt; a new method for culprit
finding which is both robust to non-deterministic (flaky) test behavior
and can identify the culprit with only logarithmic builds in the best
case by using prior information to accelerate the search.&lt;/p&gt;
&lt;h2 id="contributions"&gt;Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A Flake Aware Culprit Finding (FACF) algorithm.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A mathematical model for culprit finding adjusting for
non-deterministic test behavior.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A large scale empirical study of FACF on Google’s mono repository
comparing its performance against the traditional bisect
algorithm.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The study also evaluates the effectiveness: (a) optimizations for
FACF, and (b) adding deflaking runs to Bisect.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="background"&gt;Background&lt;/h1&gt;
&lt;p&gt;At Google, the Test Automation Platform (TAP) runs most tests that
can run on a single machine (and do not use the network except for the
loopback interface)&lt;a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and can run in under 15 minutes.&lt;a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Figure &lt;a href="#fig:ci-timeline"
data-reference-type="ref"
data-reference="fig:ci-timeline"&gt;[fig:ci-timeline]&lt;/a&gt; illustrates at a
high level of how TAP works. First, (not illustrated) tests are grouped
into logical projects by the teams that own them. Then, (for accounting
and quota allocation purposes) those projects are grouped into very high
level groupings called Quota Buckets. Each Quota Bucket is scheduled
independently when build resources are available. This means that TAP
does not run every test at every version. Instead it only runs tests
periodically at select versions called “Milestones” &lt;span
class="citation" data-cites="Memon2017 Micco2013 Leong2019"&gt;(&lt;a
href="#ref-Memon2017" role="doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;; &lt;a
href="#ref-Micco2013" role="doc-biblioref"&gt;Micco 2013&lt;/a&gt;; &lt;a
href="#ref-Leong2019" role="doc-biblioref"&gt;Leong et al.
2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;When scheduling a Milestone, all the tests that are built and run
with the same command line flags to the build system&lt;a href="#fn3"
class="footnote-ref" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; are
grouped into a set. That set is filtered to remove tests that haven’t
been affected by a change to themselves or a build level dependency
&lt;span class="citation" data-cites="Gupta2011 Micco2013 Micco2012"&gt;(&lt;a
href="#ref-Gupta2011" role="doc-biblioref"&gt;Gupta, Ivey, and Penix
2011&lt;/a&gt;; &lt;a href="#ref-Micco2013" role="doc-biblioref"&gt;Micco 2013&lt;/a&gt;,
&lt;a href="#ref-Micco2012" role="doc-biblioref"&gt;2012&lt;/a&gt;)&lt;/span&gt;. This is
a very coarse grained form of static dependence-based test selection
&lt;span class="citation" data-cites="Binkley1992 Horwitz1992 Tip1995"&gt;(&lt;a
href="#ref-Binkley1992" role="doc-biblioref"&gt;Binkley 1992&lt;/a&gt;; &lt;a
href="#ref-Horwitz1992" role="doc-biblioref"&gt;Horwitz and Reps 1992&lt;/a&gt;;
&lt;a href="#ref-Tip1995" role="doc-biblioref"&gt;Tip 1995&lt;/a&gt;)&lt;/span&gt;.
Google’s use of build dependence test selection has been influential and
it is now also used at a number of other corporations who have adopted
Bazel or similar tools &lt;span class="citation"
data-cites="Ananthanarayanan2019 Machalica2019"&gt;(&lt;a
href="#ref-Ananthanarayanan2019" role="doc-biblioref"&gt;Ananthanarayanan
et al. 2019&lt;/a&gt;; &lt;a href="#ref-Machalica2019"
role="doc-biblioref"&gt;Machalica et al. 2019&lt;/a&gt;)&lt;/span&gt;. Finally, that
(potentially very large) set of “affected” tests is sent to the Build
System &lt;span class="citation" data-cites="Wang2020"&gt;(&lt;a
href="#ref-Wang2020" role="doc-biblioref"&gt;Wang et al. 2020&lt;/a&gt;)&lt;/span&gt;
which batches them into efficiently sized groups and then builds and
runs them when there is capacity.&lt;/p&gt;
&lt;p&gt;Since tests are only run periodically even with accurate build level
dependence analysis to drive test selection, there are usually multiple
changes which may have introduced a fault in the software or a bug in
the test. Finding these “bug inducing changes” – which we call
&lt;strong&gt;culprits&lt;/strong&gt; – is the subject of this paper. The process of
finding these changes is aggravated by any non-deterministic behavior of
the tests and the build system running them.&lt;/p&gt;
&lt;h2 id="flaky-tests"&gt;Flaky Tests&lt;/h2&gt;
&lt;p&gt;When a test behaves non-deterministically, we refer to it, using the
standard term, as a &lt;em&gt;flaky test&lt;/em&gt;. A flaky test is one that passes
or fails non-deterministically with no changes to the code of either the
test or the system under test. At Google, we consider all tests to be at
least &lt;em&gt;slightly&lt;/em&gt; flaky as the machines that tests are running on
could fail in ways that get classified as a test failure (such as
resource exhaustion due to a noisy neighbor resulting in a test
exceeding its time limit). We encourage everyone to adopt this
conservative view for the purposes of identifying culprits.&lt;/p&gt;
&lt;p&gt;An important input to the culprit finding algorithm we are presenting
is an estimate of how flaky a test might be, as represented by an
estimated probability of the test failing due to a flaky failure. We
will denote the estimated probability of failure due to a flake for a
test &lt;span class="math inline"&gt;\(t\)&lt;/span&gt; as &lt;span
class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt;. The actual probability of
failure due to a flake will be denoted &lt;span
class="math inline"&gt;\(f_t\)&lt;/span&gt; (it is impossible in practice to know
&lt;span class="math inline"&gt;\(f_t\)&lt;/span&gt;). At Google, we estimate &lt;span
class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt; based on the historical failure
rate of the test. Apple and Facebook recently published on their work in
on estimating flake rates &lt;span class="citation"
data-cites="Kowalczyk2020 Machalica2020"&gt;(&lt;a href="#ref-Kowalczyk2020"
role="doc-biblioref"&gt;Kowalczyk et al. 2020&lt;/a&gt;; &lt;a
href="#ref-Machalica2020" role="doc-biblioref"&gt;Machalica et al.
2020&lt;/a&gt;)&lt;/span&gt;. One challenge with flake rate estimation is that a
single change to the code can completely change the flakiness
characteristics of a test in unpredictable ways. Making the flake rate
estimation sensitive to recent changes is an ongoing challenge and one
we partially solve in this work by doing live re-estimation of the flake
rate during the culprit finding process itself.&lt;/p&gt;
&lt;h2 id="sec:background:cf"&gt;Culprit Finding&lt;/h2&gt;
&lt;p&gt;Informally, Culprit Finding is the problem of identifying a version
&lt;span class="math inline"&gt;\(c\)&lt;/span&gt; between two versions &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt; such that &lt;span class="math inline"&gt;\(a
&amp;lt; c \le b\)&lt;/span&gt; where a given test &lt;span
class="math inline"&gt;\(t\)&lt;/span&gt; was passing at &lt;span
class="math inline"&gt;\(a\)&lt;/span&gt;, failing at &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;, and &lt;span
class="math inline"&gt;\(c\)&lt;/span&gt; is the version which introduced the
fault. At Google we consider the passing status of a test to be 100%
reliable and therefore version &lt;span class="math inline"&gt;\(a\)&lt;/span&gt;
cannot be a culprit but version &lt;span class="math inline"&gt;\(b\)&lt;/span&gt;
could be the culprit. For simplicity, this paper only considers
searching for the culprit for a single test &lt;span
class="math inline"&gt;\(t\)&lt;/span&gt; (which failed at &lt;span
class="math inline"&gt;\(b\)&lt;/span&gt;) but in principle it can be extended to
a set of tests &lt;span class="math inline"&gt;\(T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Google’s version control system, Piper &lt;span class="citation"
data-cites="Potvin2016"&gt;(&lt;a href="#ref-Potvin2016"
role="doc-biblioref"&gt;Potvin and Levenberg 2016&lt;/a&gt;)&lt;/span&gt;, stores the
mainline history as a linear list of versions. Each version gets a
monotonically, &lt;em&gt;but not sequentially&lt;/em&gt;, increasing number as its
version number called the &lt;em&gt;Changelist Number&lt;/em&gt; or CL number for
short. Thus, our implementation and experiments do not consider branches
and merges as would be necessary for a Distributed Version Control
System (DVCS) such as git. However, only trivial changes to the
mathematics are needed to apply our approach to such an environment
&lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Thus, the versions larger than &lt;span class="math inline"&gt;\(a\)&lt;/span&gt;
and smaller or equal to &lt;span class="math inline"&gt;\(b\)&lt;/span&gt; are an
ordered list of items we will call the &lt;em&gt;search range&lt;/em&gt; and denote
as &lt;span class="math inline"&gt;\(S =
\left\{s_1, ..., s_{n-1}, s_n\right\}\)&lt;/span&gt; where &lt;span
class="math inline"&gt;\(s_n = b\)&lt;/span&gt;. During the search, test
executions will be run (and may be run in parallel) at various versions.
The unordered set of executions at version &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt; will be denoted &lt;span
class="math inline"&gt;\(X_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For the purposes of culprit finding, based on the status of failure
detected, the build/test execution statuses are mapped to one of 2
possible states: &lt;code&gt;PASS&lt;/code&gt; or &lt;code&gt;FAIL&lt;/code&gt;. We denote (for
version &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt;): the number of
statuses that map to &lt;code&gt;PASS&lt;/code&gt; as &lt;span
class="math inline"&gt;\(\texttt{\small PASSES}(X_i)\)&lt;/span&gt; and the
number of statuses that map to &lt;code&gt;FAIL&lt;/code&gt; as &lt;span
class="math inline"&gt;\(\texttt{\small FAILS}(X_i)\)&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id="bisection"&gt;Bisection&lt;/h2&gt;

&lt;p&gt;&lt;span id="alg:bisect" label="alg:bisect"&gt;
&lt;a href="images/icst-2023/alg1.png"&gt;&lt;img alt="Algorithm Bisect" src="images/icst-2023/alg1.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Bisect algorithm is a specialized form of binary search. It runs
tests (or more generally any operation on the code that can return PASS
or FAIL) to attempt to identify which commit introduced a bug. Algorithm
&lt;a href="#alg:bisect" data-reference-type="ref"
data-reference="alg:bisect"&gt;[alg:bisect]&lt;/a&gt; gives the psuedo-code for
this procedure. Bisect is now commonly implemented in most version
control systems (see &lt;code&gt;git bisect&lt;/code&gt; and &lt;code&gt;hg bisect&lt;/code&gt;
as common examples). In the case of Distributed Version Control Systems
(DVCS) systems such as ‘git‘ and ‘hg‘ the implementation of bisect is
somewhat more complex as they model version history as a directed
acyclic graph &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The algorithm presented in Alg &lt;a href="#alg:bisect"
data-reference-type="ref" data-reference="alg:bisect"&gt;[alg:bisect]&lt;/a&gt;
has two important changes from the standard binary search. First, it
takes into account that a test can fail and pass at the same version and
the &lt;code&gt;firstFail&lt;/code&gt; function specifically looks for a failure
after the &lt;code&gt;lastPass&lt;/code&gt;. Second, it takes a parameter &lt;span
class="math inline"&gt;\(k\)&lt;/span&gt; which allows the user to specify that
each test execution be run &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; times.
This allows the user to “deflake” the test executions by rerunning
failures. Our empirical evaluation (Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;)
examines both the cost and effectiveness of this simple modification.
While this simple modification to bisect may be reasonably effective for
moderately flaky tests, it does not explicitly model the effect of
non-deterministic failures on the likelihood of a suspect to be the
culprit.&lt;/p&gt;
&lt;h2 id="guessing-games-and-noisy-searches"&gt;Guessing Games and Noisy
Searches&lt;/h2&gt;
&lt;div class="algorithm"&gt;

&lt;/div&gt;
&lt;p&gt;One can view the culprit finding scenario as a kind of guessing game.
The questioner guesses the test always fails at a given suspect version.
A test execution at that suspect acts as a kind of “responder” or
“oracle” which answers. The test execution is an unreliable oracle which
may answer FAIL when it should have answered PASS. But, the oracle never
answers PASS when it should have answered FAIL – note the asymmetry.&lt;/p&gt;
&lt;p&gt;This kind of guessing game is a Rényi-Ulam game, which has been well
characterized by the community &lt;span class="citation"
data-cites="Pelc2002"&gt;(&lt;a href="#ref-Pelc2002" role="doc-biblioref"&gt;Pelc
2002&lt;/a&gt;)&lt;/span&gt;. The main differences from the “classical” games
presented by Rényi and separately by Ulam is (a) the format of the
questions and (b) the limitations placed on the oracle responding
incorrectly. In a common formulation of the Rényi-Ulam game, the
questioner is trying to “guess the number” between (for instance) 1 and
1,000,000. Each question is allowed to be any subset of numbers in the
search range. The responder or oracle is allowed to lie at most a fixed
number of times (for instance once or twice). This formulation has
allowed the community to find analytic solutions to determine the number
of questions in the optimal strategy. We refer the reader to the survey
from Pelc for an overview of the field and its connection with error
correcting codes &lt;span class="citation" data-cites="Pelc2002"&gt;(&lt;a
href="#ref-Pelc2002" role="doc-biblioref"&gt;Pelc 2002&lt;/a&gt;)&lt;/span&gt;. The
culprit finding scenario is “noisy binary search” scenario where the
oracle only lies for one type of response. Rivest &lt;em&gt;et al.&lt;/em&gt; &lt;span
class="citation" data-cites="Rivest1978"&gt;(&lt;a href="#ref-Rivest1978"
role="doc-biblioref"&gt;Rivest, Meyer, and Kleitman 1978&lt;/a&gt;)&lt;/span&gt;
provide a theoretical treatment of this “half-lie” variant (as well as
the usual variation).&lt;/p&gt;
&lt;h2 id="bayesian-binary-search"&gt;Bayesian Binary Search&lt;/h2&gt;
&lt;p&gt;Ben Or and Hassidim &lt;span class="citation"
data-cites="Ben-Or2008"&gt;(&lt;a href="#ref-Ben-Or2008"
role="doc-biblioref"&gt;Ben-Or and Hassidim 2008&lt;/a&gt;)&lt;/span&gt; noted the
connection between these “Noisy Binary Search” problems and Bayesian
inference. Our work builds off of their formulation and applies it to
the culprit finding setting. In the Bayesian setting, consider the
probability each suspect is the culprit and the probability there is no
culprit (i.e. the original failure was caused by a flake). These
probabilities together form a distribution: &lt;span
class="math display"&gt;\[(\textrm{Pr}\left[{\text{there is no
culprit}}\right]) +
  \sum_{i=1}^{n} \textrm{Pr}\left[{s_i \text{ is the culprit}}\right] =
1\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We represent this distribution with &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right]\)&lt;/span&gt; with &lt;span
class="math inline"&gt;\(i=1..(n+1)\)&lt;/span&gt; where &lt;span
class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{C_i}\right] &amp;amp;= \textrm{Pr}\left[{s_i \text{ is
the culprit}}\right], \; \forall \; i \in [1, n]
   \\
  \textrm{Pr}\left[{C_{n+1}}\right] &amp;amp;= \textrm{Pr}\left[{\text{there
is no culprit}}\right] \\
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Initially when the original failure is first detected at &lt;span
class="math inline"&gt;\(s_n\)&lt;/span&gt; and the search range is created, the
probability distribution is uniform: &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right] =
\frac{1}{(n+1)}\)&lt;/span&gt; for all &lt;span class="math inline"&gt;\(i \in [1,
n+1]\)&lt;/span&gt;. Now consider new evidence &lt;span
class="math inline"&gt;\(E\)&lt;/span&gt; arriving. In the Bayesian model, this
would &lt;em&gt;update&lt;/em&gt; our probability given this new evidence: &lt;span
class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | E}\right] = \frac{\textrm{Pr}\left[{E |
C_i}\right] \cdot
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{E}\right]}
  \label{eq:bayes}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above equation, the &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right]\)&lt;/span&gt; is the
&lt;em&gt;prior&lt;/em&gt; probability and the probability &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_i | E}\right]\)&lt;/span&gt; is the
&lt;em&gt;posterior&lt;/em&gt; probability. In the culprit finding context, the new
evidence is typically newly observed test executions. However, there is
no reason we cannot take into account other information as well.&lt;/p&gt;
&lt;p&gt;In the Bayesian framework, new evidence is applied iteratively. In
each iteration, the prior probability is the posterior probability of
the previous iteration. Since multiplication is commutative, multiple
pieces of evidence can be applied in any order to arrive at the same
posterior probability: &lt;span class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{C_i | E_1, E_2}\right] &amp;amp;=
\frac{\textrm{Pr}\left[{E_2 |
C_i}\right]}{\textrm{Pr}\left[{E_2}\right]}
                         \frac{\textrm{Pr}\left[{E_1 |
C_i}\right]}{\textrm{Pr}\left[{E_1}\right]} \textrm{Pr}\left[{Ci}\right]
\\
                      &amp;amp;= \frac{\textrm{Pr}\left[{E_1 |
C_i}\right]}{\textrm{Pr}\left[{E_1}\right]}
                         \frac{\textrm{Pr}\left[{E_2 |
C_i}\right]}{\textrm{Pr}\left[{E_2}\right]} \textrm{Pr}\left[{Ci}\right]
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After the current posterior probability has been calculated, it is
used to select the next suspect to execute a test at. In the traditional
Bayes approach, the suspect to examine is the one with the maximum
posterior probability. However, in the context of a noisy binary search,
the suspects are ordered so a PASS at suspect &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt; indicates a PASS at all suspects
prior to &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt;. Following Ben Or and
Hassidim &lt;span class="citation" data-cites="Ben-Or2008"&gt;(&lt;a
href="#ref-Ben-Or2008" role="doc-biblioref"&gt;Ben-Or and Hassidim
2008&lt;/a&gt;)&lt;/span&gt;, we convert the posterior distribution to a cumulative
probability distribution and select the first suspect with a cumulative
probability &lt;span class="math inline"&gt;\(\geq 0.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id="flake-aware-culprit-finding"&gt;Flake Aware Culprit Finding&lt;/h1&gt;
&lt;p&gt;Now we introduce our algorithm, Flake Aware Culprit Finding (FACF),
in the Bayesian framework of Ben Or and Hassidim &lt;span class="citation"
data-cites="Ben-Or2008"&gt;(&lt;a href="#ref-Ben-Or2008"
role="doc-biblioref"&gt;Ben-Or and Hassidim 2008&lt;/a&gt;)&lt;/span&gt;. As above, let
&lt;span class="math inline"&gt;\(\textrm{Pr}\left[{C_i}\right]\)&lt;/span&gt; be
the probability that suspect &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; is
the culprit and &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{C_{n+1}}\right]\)&lt;/span&gt; be the
probability there is no culprit. These probabilities are initialized to
some suitable initial distribution such as the uniform distribution.&lt;/p&gt;
&lt;p&gt;To update the distribution with new evidence &lt;span
class="math inline"&gt;\(E\)&lt;/span&gt; in the form of a pass or fail for some
suspect &lt;span class="math inline"&gt;\(k\)&lt;/span&gt;, apply Bayes rule
(Equation &lt;a href="#eq:bayes" data-reference-type="ref"
data-reference="eq:bayes"&gt;[eq:bayes]&lt;/a&gt;). Note that &lt;span
class="math inline"&gt;\(\textrm{Pr}\left[{E}\right]\)&lt;/span&gt; can be
rewritten by Bayes rule in terms of a sum by marginalizing over the
likelihood that each suspect &lt;span class="math inline"&gt;\(j\)&lt;/span&gt;
could be the culprit: &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{E}\right] = \sum_{j=1}^{n} \textrm{Pr}\left[{E |
C_j}\right] \textrm{Pr}\left[{C_j}\right]\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are two types of evidence: test executions with state
&lt;code&gt;PASS&lt;/code&gt; or &lt;code&gt;FAIL&lt;/code&gt; at suspect &lt;span
class="math inline"&gt;\(i\)&lt;/span&gt;. We indicate these as the events &lt;span
class="math inline"&gt;\(P_i\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(F_i\)&lt;/span&gt; respectively. The marginalized
probability of a &lt;code&gt;PASS&lt;/code&gt; at &lt;span
class="math inline"&gt;\(i\)&lt;/span&gt; given there is a culprit at &lt;span
class="math inline"&gt;\(j\)&lt;/span&gt; is: &lt;span
class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{P_i | C_j}\right] =
    \begin{cases}
      1 - \hat{f}_t  &amp;amp; \text{if } i &amp;lt; j \\
      0              &amp;amp; \text{otherwise}
    \end{cases}
  \label{eq:pass}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The marginalized probability of a &lt;code&gt;FAIL&lt;/code&gt; at &lt;span
class="math inline"&gt;\(i\)&lt;/span&gt; given there is a culprit at &lt;span
class="math inline"&gt;\(j\)&lt;/span&gt; is: &lt;span
class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{F_i | C_j}\right] =
    \begin{cases}
      \hat{f}_t      &amp;amp; \text{if } i &amp;lt; j \\
      1              &amp;amp; \text{otherwise}
    \end{cases}
  \label{eq:fail}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With Equation &lt;a href="#eq:pass" data-reference-type="ref"
data-reference="eq:pass"&gt;[eq:pass]&lt;/a&gt;, we write a Bayesian update for
&lt;code&gt;PASS&lt;/code&gt; at suspect &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; as
the following. Given &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | P_k}\right] = \frac{\textrm{Pr}\left[{P_k |
C_i}\right] \textrm{Pr}\left[{C_i}\right]}{
\textrm{Pr}\left[{P_k}\right]
  }\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;let &lt;span class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{P_k}\right] &amp;amp;= \sum_{j=1}^{n}
\textrm{Pr}\left[{P_k | C_j}\right] \textrm{Pr}\left[{C_j}\right] \\
           &amp;amp;= \sum_{j=k+1}^{n} (1 - \hat{f}_t)
\textrm{Pr}\left[{C_j}\right]
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;div class="algorithm"&gt;

&lt;/div&gt;
&lt;div class="algorithm"&gt;
&lt;p&gt;cdf = cumulativeDistribution(dist) thresholds = &lt;span
class="math inline"&gt;\(\left\{ \frac{i}{(k+1)} \; | \; i=1..k
\right\}\)&lt;/span&gt; tidx = 0 runs = list() runs&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;then &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | P_k}\right] =
  \begin{cases}
    \frac{(1 - \hat{f}_t)
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{P_k}\right]} &amp;amp;
\text{if } k &amp;lt; i \\
    0                                         &amp;amp; \text{otherwise} \\
  \end{cases}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The update using Equation &lt;a href="#eq:fail"
data-reference-type="ref" data-reference="eq:fail"&gt;[eq:fail]&lt;/a&gt; for
&lt;code&gt;FAIL&lt;/code&gt; at suspect &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; is
similar. Given &lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{Pr}\left[{C_i | F_k}\right] = \frac{\textrm{Pr}\left[{F_k |
C_i}\right] \textrm{Pr}\left[{C_i}\right]}{
    \sum_{j=1}^{n} \textrm{Pr}\left[{F_k | C_j}\right]
\textrm{Pr}\left[{C_j}\right]
  }\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;let &lt;span class="math display"&gt;\[\begin{aligned}
\begin{split}
  \textrm{Pr}\left[{F_k}\right] &amp;amp;= \sum_{j=1}^{n}
\textrm{Pr}\left[{F_k | C_j}\right] \textrm{Pr}\left[{C_j}\right] \\
           &amp;amp;= \sum_{j=1}^{k} \textrm{Pr}\left[{C_j}\right] +
\sum_{j=k+1}^{n} \hat{f}_t \textrm{Pr}\left[{C_j}\right]
\end{split}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;then &lt;span class="math display"&gt;\[\begin{aligned}
\textrm{Pr}\left[{C_i | F_k}\right] =
  \begin{cases}
    \frac{\hat{f}_t
\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{F_k}\right]} &amp;amp;
\text{if } k &amp;lt; i \\
    \frac{\textrm{Pr}\left[{C_i}\right]}{\textrm{Pr}\left[{F_k}\right]}           &amp;amp;
\text{otherwise} \\
  \end{cases}\end{aligned}\]&lt;/span&gt;&lt;/p&gt;

&lt;h2 id="the-facf-algorithm"&gt;The FACF Algorithm&lt;/h2&gt;

&lt;p&gt;&lt;span id="alg:facf" label="alg:facf"&gt;
&lt;a href="images/icst-2023/alg2.png"&gt;&lt;img alt="Algorithm FACF" src="images/icst-2023/alg2.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id="alg:nextRuns" label="alg:nextRuns"&gt;
&lt;a href="images/icst-2023/alg3.png"&gt;&lt;img alt="Algorithm nextRuns" src="images/icst-2023/alg3.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Algorithm &lt;a href="#alg:facf" data-reference-type="ref"
data-reference="alg:facf"&gt;[alg:facf]&lt;/a&gt; presents the pseudo code for a
culprit finding algorithm based on the above Bayesian formulation. It
takes a prior distribution (the uniform distribution can be used), an
estimated flake rate &lt;span class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt;, and
the suspects. It returns the most likely culprit or &lt;code&gt;NONE&lt;/code&gt;
(in the case that it is most likely there is no culprit). The FACF
algorithm builds a set of evidence &lt;span
class="math inline"&gt;\(X\)&lt;/span&gt; containing the observed passes and
failures so far. At each step, that evidence is converted to a
distribution using the math presented above. Then, FACF calls Algorithm
&lt;a href="#alg:nextRuns" data-reference-type="ref"
data-reference="alg:nextRuns"&gt;[alg:nextRuns]&lt;/a&gt; &lt;code&gt;NextRuns&lt;/code&gt;
to compute the next suspect to run.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;NextRuns&lt;/code&gt; converts the probability distribution into a
cumulative distribution. It then selects the suspects with cumulative
probability over a set of thresholds and returns them. The version of
&lt;code&gt;NextRuns&lt;/code&gt; we present here has been generalized to allow for
&lt;span class="math inline"&gt;\(k\)&lt;/span&gt; parallel runs and also contains a
minor optimization on lines 10-11. If the suspect &lt;span
class="math inline"&gt;\(s_{i-1}\)&lt;/span&gt; prior to the selected suspect
&lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; hasn’t been selected to run and
its cumulative probability is above &lt;span
class="math inline"&gt;\(0\)&lt;/span&gt; then the prior suspect &lt;span
class="math inline"&gt;\(s_{i-1}\)&lt;/span&gt; is selected in favor of &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt;. This optimization ensures that we
look for the passing run preceding the most likely culprit before
looking for a failing run at the culprit.&lt;/p&gt;
&lt;h2 id="sec:thresholds"&gt;Choosing the Thresholds in NextRuns&lt;/h2&gt;
&lt;p&gt;Another optimization can be made to NextRuns by changing the
thresholds used to select suspects. Instead of simply dividing the
probability space up evenly it uses the observation that the information
gain from a PASS is more than the gain from a FAIL. Modeling that as
&lt;span class="math inline"&gt;\(I(P_i) = 1\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(I(F_i) = 1 - \hat{f}_t\)&lt;/span&gt;, we can compute
the expected information gain for new evidence at suspect &lt;span
class="math inline"&gt;\(s_i\)&lt;/span&gt; as &lt;span
class="math inline"&gt;\(\textrm{E}\left[{I(s_i)}\right] = I(P_i)P_i +
I(F_i)F_i\)&lt;/span&gt;. Expanding from the marginalized equations &lt;a
href="#eq:pass" data-reference-type="ref"
data-reference="eq:pass"&gt;[eq:pass]&lt;/a&gt; and &lt;a href="#eq:fail"
data-reference-type="ref" data-reference="eq:fail"&gt;[eq:fail]&lt;/a&gt; gives
&lt;span class="math display"&gt;\[\begin{aligned}
  \textrm{E}\left[{I(s_i)}\right] = (1 - \hat{f}_t)(n + i \hat{f}_t -
1)\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Computing the cumulative expectation over &lt;span
class="math inline"&gt;\(\{ \textrm{E}\left[{I(s_1)}\right], ...,
\textrm{E}\left[{I(s_{n+1})}\right] \}\)&lt;/span&gt; and normalizing allows
us to select the suspect that maximizes the information gain. The
empirical evaluation (Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;)
examines the effect of this optimization. The importance of the supplied
flake rate estimate (&lt;span class="math inline"&gt;\(\hat{f}_t\)&lt;/span&gt;) is
also examined in Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="sec:priors"&gt;Prior Probabilities for FACF&lt;/h2&gt;
&lt;p&gt;The prior distribution supplied to Algorithm &lt;a href="#alg:facf"
data-reference-type="ref" data-reference="alg:facf"&gt;[alg:facf]&lt;/a&gt; does
not have to be a uniform distribution. Instead, it can be informed by
any other data the culprit finding system might find useful. At Google,
we are exploring many such sources of data, but so far are only using
two primary sources in production.&lt;/p&gt;
&lt;p&gt;First, we have always used the static build dependence graph to
filter out non-affecting suspects before culprit finding &lt;span
class="citation" data-cites="Gupta2011"&gt;(&lt;a href="#ref-Gupta2011"
role="doc-biblioref"&gt;Gupta, Ivey, and Penix 2011&lt;/a&gt;)&lt;/span&gt;. In FACF,
we take that a step further and, using a stale snapshot for scalability,
we compute the minimum distance in the build graph between the files
changed in each suspect and the test being culprit-found &lt;span
class="citation" data-cites="Memon2017"&gt;(&lt;a href="#ref-Memon2017"
role="doc-biblioref"&gt;Memon et al. 2017&lt;/a&gt;)&lt;/span&gt;. Using these
distances, we form a probability distribution where the suspects with
files closer to the test have a higher probability of being the culprit
&lt;span class="citation" data-cites="Ziftci2013a Ziftci2017"&gt;(&lt;a
href="#ref-Ziftci2013a" role="doc-biblioref"&gt;Ziftci and Ramavajjala
2013&lt;/a&gt;; &lt;a href="#ref-Ziftci2017" role="doc-biblioref"&gt;Ziftci and
Reardon 2017&lt;/a&gt;)&lt;/span&gt;. Second, we look at the historical likelihood
of a true breakage (versus a flaky one) to determine the probability
that there is no culprit. Combining that distribution with the one from
the minimum build distance forms a prior distribution we use for culprit
finding.&lt;/p&gt;
&lt;p&gt;Using this scheme, flaky tests tend to get reruns at the &lt;span
class="math inline"&gt;\(s_n\)&lt;/span&gt; (the suspect where the failure was
detected) as their first test executions during culprit finding. This is
intentional as a large fraction of the search ranges we culprit find
tend to be flaky and this reduces the cost of proving that. In the
empirical evaluation (Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt;), we
examine the effect of the prior distribution on both cost and
accuracy.&lt;/p&gt;
&lt;h1 id="sec:verification"&gt;Culprit Verification for Accuracy
Evaluation&lt;/h1&gt;
&lt;p&gt;The previous sections detailed our culprit finding algorithm (FACF)
and the baseline algorithm we compare against (deflaked bisection). In
order to evaluate the accuracy of a culprit finding system for a
particular dataset, we need to know which commits are the actual culprit
commits. If the actual culprit commits are known, then we can compute
the usual measures (True Positive Rate, False Positive Rate, Accuracy,
Precision, Recall, etc...) &lt;span class="citation"
data-cites="Tharwat2021"&gt;(&lt;a href="#ref-Tharwat2021"
role="doc-biblioref"&gt;Tharwat 2021&lt;/a&gt;)&lt;/span&gt;. Previous work in culprit
finding, test case selection, fault localization and related research
areas have often used curated datasets (e.g. Defects4J &lt;span
class="citation" data-cites="Just2014"&gt;(&lt;a href="#ref-Just2014"
role="doc-biblioref"&gt;Just, Jalali, and Ernst 2014&lt;/a&gt;)&lt;/span&gt;), bug
databases &lt;span class="citation"
data-cites="Bhattacharya2010 Herzig2015 Najafi2019a"&gt;(&lt;a
href="#ref-Bhattacharya2010" role="doc-biblioref"&gt;Bhattacharya and
Neamtiu 2010&lt;/a&gt;; &lt;a href="#ref-Herzig2015" role="doc-biblioref"&gt;Herzig
et al. 2015&lt;/a&gt;; &lt;a href="#ref-Najafi2019a" role="doc-biblioref"&gt;Najafi,
Rigby, and Shang 2019&lt;/a&gt;)&lt;/span&gt;, and synthetic benchmarks with
injected bugs &lt;span class="citation"
data-cites="Henderson2018 Henderson2019"&gt;(&lt;a href="#ref-Henderson2018"
role="doc-biblioref"&gt;Henderson and Podgurski 2018&lt;/a&gt;; &lt;a
href="#ref-Henderson2019" role="doc-biblioref"&gt;Henderson, Podgurski, and
Kucuk 2019&lt;/a&gt;)&lt;/span&gt;. But our primary interest is in the accuracy of
the algorithms in the Google environment, not the accuracy on open
source bug databases, curated datasets, or synthetic bugs.&lt;/p&gt;
&lt;p&gt;At Google, we do not have a comprehensive database of developer
investigations of every continuous integration test failure – indeed
there are far too many failures per day for developers to examine and
evaluate every single one. We also didn’t want to completely rely on
human feedback to tell us when our culprit finding systems have
identified the wrong culprit – as had been previously done &lt;span
class="citation" data-cites="Ziftci2017"&gt;(&lt;a href="#ref-Ziftci2017"
role="doc-biblioref"&gt;Ziftci and Reardon 2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Instead, we wanted to be able to verify whether or not an identified
culprit for a particular test and search range is correct or not. The
verification needs to be automated such that the we can operationally
monitor the accuracy of the deployed system and catch any performance
regressions in the culprit finder. Continuous monitoring ensures a high
quality user experience by alerting our team to accuracy
degradations.&lt;/p&gt;
&lt;h2 id="culprit-finding-conclusions"&gt;Culprit Finding Conclusions&lt;/h2&gt;
&lt;p&gt;A culprit finder may draw one of two conclusions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;FI&lt;/strong&gt;: &lt;code&gt;FLAKE_IDENTIFIED&lt;/code&gt; &lt;em&gt;the
culprit finder determined the status transition was caused by
non-deterministic behavior.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;CVI&lt;/strong&gt;:
&lt;code&gt;CULPRIT_VERSION_IDENTIFIED(version)&lt;/code&gt; &lt;em&gt;the culprit finder
determined a specific &lt;code&gt;version&lt;/code&gt; was identified as the
culprit&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The verification system will label each conclusion examined with
either &lt;code&gt;CORRECT&lt;/code&gt; or &lt;code&gt;INCORRECT&lt;/code&gt; from which we can
define the standard counts for True and False Positives and Negatives.
Accuracy, Precision, and Recall can then be directly computed.&lt;/p&gt;
&lt;h2 id="when-is-a-culprit-incorrect"&gt;When is a Culprit Incorrect?&lt;/h2&gt;
&lt;p&gt;For ease of discussion, we will use the notation introduced in
Section &lt;a href="#sec:background:cf" data-reference-type="ref"
data-reference="sec:background:cf"&gt;2.2&lt;/a&gt; for the search range &lt;span
class="math inline"&gt;\(S\)&lt;/span&gt; with &lt;span
class="math inline"&gt;\(s_0\)&lt;/span&gt; indicating the previous passing
version, &lt;span class="math inline"&gt;\(s_n\)&lt;/span&gt; indicating the version
where the failure was first detected, and &lt;span
class="math inline"&gt;\(s_k\)&lt;/span&gt; indicating the identified culprit.
Now, consider the two cases&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;FLAKE_IDENTIFIED&lt;/code&gt; these are incorrect if no
subsequent runs pass at the version where the original failure was
detected (no flaky behavior can be reproduced).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;CULPRIT_VERSION_IDENTIFIED(version)&lt;/code&gt; these are
incorrect if we can prove any of the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A pass at &lt;span class="math inline"&gt;\(s_n\)&lt;/span&gt; indicates the
failure was a flake.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A pass at the culprit &lt;span class="math inline"&gt;\(s_k\)&lt;/span&gt;
indicates that the culprit was incorrectly identified.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No passes detected at the version prior to the culprit, &lt;span
class="math inline"&gt;\(s_{k-1}\)&lt;/span&gt;, indicates that the culprit was
incorrectly identified.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additionally the following conditions indicate the culprit may have
been misidentified due to a test dependency on either time or some
unknown external factor:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;No passes are identified at &lt;span
class="math inline"&gt;\(s_0\)&lt;/span&gt;, indicating the test failure may be
time or environment dependent (and the conclusion cannot be
trusted).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No pass at &lt;span class="math inline"&gt;\(s_0\)&lt;/span&gt; is detected
prior (in the time dimension) to a failure at &lt;span
class="math inline"&gt;\(s_n\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No failure at &lt;span class="math inline"&gt;\(s_n\)&lt;/span&gt; is
detected prior (in the time dimension) to a pass at &lt;span
class="math inline"&gt;\(s_0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No pass at &lt;span class="math inline"&gt;\(s_{k-1}\)&lt;/span&gt; is
detected prior (in the time dimension) to a failure at &lt;span
class="math inline"&gt;\(s_k\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;No failure at &lt;span class="math inline"&gt;\(s_k\)&lt;/span&gt; is
detected prior (in the time dimension) to a pass at &lt;span
class="math inline"&gt;\(s_{k-1}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Verification is not reproducible 17 hours later.&lt;a href="#fn4"
class="footnote-ref" id="fnref4"
role="doc-noteref"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Thus, our approach for determining whether or not a culprit is
correct is to do extensive reruns for a particular conclusion.&lt;/p&gt;
&lt;h2 id="how-many-reruns-to-conduct-to-verify-a-conclusion"&gt;How Many
Reruns to Conduct to Verify a Conclusion&lt;/h2&gt;
&lt;p&gt;A test run &lt;span class="math inline"&gt;\(x\)&lt;/span&gt; times fails every
time with the rate &lt;span class="math inline"&gt;\(\hat{f}_t^x\)&lt;/span&gt;. If
we want to achieve confidence level &lt;span
class="math inline"&gt;\(C\)&lt;/span&gt; (ex. &lt;span class="math inline"&gt;\(C =
.99999999\)&lt;/span&gt;) of verifier correctness, we can compute the number
of reruns to conduct for each check with &lt;span class="math inline"&gt;\(x =
\left\lceil{\frac{\,\textrm{log}\!\!\left({1 -
C}\right)}{\,\textrm{log}\!\!\left({\hat{f}_t}\right)}}\right\rceil\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="sampling-strategy"&gt;Sampling Strategy&lt;/h2&gt;
&lt;p&gt;While it would be ideal to verify every single culprit for every
single test, the expense, in practice, of such an operation is too high.
We conduct verification with a very high confidence level (&lt;span
class="math inline"&gt;\(C = 0.99999999\)&lt;/span&gt;) and a minimum on the
estimated flakiness rate of &lt;span class="math inline"&gt;\(0.1\)&lt;/span&gt; to
ensure a minimum of 8 reruns for each check even for targets that have
not been historically flaky. With such an expensive configuration, we
can only afford the following sampling strategy.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Randomly sample at &lt;span class="math inline"&gt;\(1\%\)&lt;/span&gt; of
culprit finding conclusions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sample the first conclusion for each unique culprit
version.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We tag which sampling strategy was used to sample a particular
conclusion allowing us to compute statistics for each sampling frame
independently.&lt;/p&gt;
&lt;h2 id="the-verified-culprits-dataset"&gt;The Verified Culprits
Dataset&lt;/h2&gt;
&lt;p&gt;Using our production culprit finders based on the FACF algorithm and
a verifier implementing the above verification strategy, we have
produced an internal dataset with 144,130 verified conclusions in the
last 60 days, which we use in Section &lt;a href="#sec:evaluation"
data-reference-type="ref" data-reference="sec:evaluation"&gt;5&lt;/a&gt; to
evaluate the performance of the algorithms presented in this paper.
Note, because the evaluation of an algorithm is also expensive, we only
conduct the evaluation on a subset of the whole dataset.&lt;/p&gt;
&lt;p&gt;While we are unable to make the dataset public, we encourage others
to use our verification approach on their own culprit finding systems
and when possible share these datasets with the external community for
use in future studies. Such datasets are not only useful for culprit
finding research but also for fault localization, test prioritization,
test selection, automatic program repair, and other topics.&lt;/p&gt;

&lt;h1 id="sec:evaluation"&gt;Empirical Evaluation&lt;/h1&gt;

&lt;p&gt;We empirically evaluated the behavior of the probabilistic Flake
Aware Culprit Finding (FACF) algorithm, compared it to the traditional
Bisect algorithm &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;,
evaluated the effectiveness of adding deflaking runs to Bisect, and
evaluated the effectiveness of two optimizations for FACF.&lt;/p&gt;
&lt;h2 id="research-questions"&gt;Research Questions&lt;/h2&gt;
&lt;p&gt;The following research questions were considered. All questions were
considered with respect to the Google environment (see Section &lt;a
href="#sec:dataset" data-reference-type="ref"
data-reference="sec:dataset"&gt;5.5&lt;/a&gt; for more details about the dataset
used).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Which algorithm (FACF or Bisect) was more accurate in identifying
culprit commits?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What difference do algorithmic tweaks (such as adding deflaking
runs to Bisect) make to accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How efficient is each algorithm as measured in number of test
executions?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What difference do algorithmic tweaks (such as adding deflaking
runs to Bisect) make to efficiency.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Does flakiness affect culprit finding accuracy and cost?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="measuring-culprit-finding-accuracy"&gt;Measuring Culprit Finding
Accuracy&lt;/h2&gt;
&lt;p&gt;To measure the accuracy of the culprit finders we produce a binary
variable (“correct”) which is true if the culprit finder identified
version matched the culprit in the dataset. Accuracy is then defined as
the sample proportion: &lt;span class="math inline"&gt;\(\textbf{ACC} =
\frac{\text{\# correct}}{\text{Total}}\)&lt;/span&gt;. Culprit finder “A” is
judged more accurate than culprit finder “B” if A’s &lt;strong&gt;ACC&lt;/strong&gt;
value is higher than B’s and a statistical test (described in Section &lt;a
href="#sec:stat-tests" data-reference-type="ref"
data-reference="sec:stat-tests"&gt;5.4&lt;/a&gt;) determines the difference is
significant.&lt;/p&gt;
&lt;h2 id="measuring-culprit-finding-cost"&gt;Measuring Culprit Finding
Cost&lt;/h2&gt;
&lt;p&gt;We measure cost as the number of test executions performed by the
algorithm. Culprit finder “A” is judged lower cost than culprit finder
“B” if the mean number of test executions is less than “B” and a
statistical test (described in Section &lt;a href="#sec:stat-tests"
data-reference-type="ref" data-reference="sec:stat-tests"&gt;5.4&lt;/a&gt;)
determines the difference is significant.&lt;/p&gt;
&lt;h2 id="sec:stat-tests"&gt;On Our Usage of Statistical Tests&lt;/h2&gt;
&lt;p&gt;For answering RQ1 and RQ2, we observe the distribution per
culprit-finding algorithm of whether or not the algorithm identified the
culprit (0 or 1). For answering RQ3 and RQ4, we observe the distribution
per culprit-finding algorithm of the number of test executions required
to complete culprit finding.&lt;/p&gt;
&lt;p&gt;We follow the recommendations of McCrum-Gardner &lt;span
class="citation" data-cites="McCrum-Gardner2008"&gt;(&lt;a
href="#ref-McCrum-Gardner2008" role="doc-biblioref"&gt;McCrum-Gardner
2008&lt;/a&gt;)&lt;/span&gt; as well as Walpole &lt;em&gt;et al.&lt;/em&gt; &lt;span
class="citation" data-cites="Walpole2007"&gt;(&lt;a href="#ref-Walpole2007"
role="doc-biblioref"&gt;Walpole 2007&lt;/a&gt;)&lt;/span&gt; as to the proper test
statistics for our analysis. Since data points for a specific culprit
range have a correspondence across algorithms, we perform paired
statistical tests when feasible. The analysis is complicated by the
observation that neither the dichotomous distribution of correctness nor
the numerical distribution of number of test executions are normally
distributed. Following McCrum-Gardner we opt to use non-parametric
tests. Additionally different tests are used for accuracy (dichotomous
nominal variable) and number of executions (numerical variable)
respectively.&lt;/p&gt;
&lt;p&gt;As we have more than 2 groups to consider (10 algorithms), we
initially perform omnibus tests over all algorithms to justify the
existence of statistical significance over all algorithms. In response
to the multiple comparisons problem &lt;span class="citation"
data-cites="Walpole2007"&gt;(&lt;a href="#ref-Walpole2007"
role="doc-biblioref"&gt;Walpole 2007&lt;/a&gt;)&lt;/span&gt;, we compute the corrected
significance bound using the formula for the family-wise error rate. Our
experimental design has us first conduct the ANOVA significance test to
determine if any of the algorithms differed in performance. Then we
conduct post-hoc testing to determine which algorithm’s performance
differed. For each considered question we conduct &lt;span
class="math inline"&gt;\(\binom{10}{2} + 1\)&lt;/span&gt; significance tests. We
have &lt;span class="math inline"&gt;\(6\)&lt;/span&gt; considered questions which
we use significance testing to answer, giving us a total &lt;span
class="math inline"&gt;\(r =
6\left(\binom{10}{2} + 1\right) = 276\)&lt;/span&gt; tests.&lt;/p&gt;
&lt;p&gt;Solving for the &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; in the
experiment-wise error rate formula from Walpole (pg. 529 &lt;span
class="citation" data-cites="Walpole2007"&gt;(&lt;a href="#ref-Walpole2007"
role="doc-biblioref"&gt;Walpole 2007&lt;/a&gt;)&lt;/span&gt;) with &lt;span
class="math inline"&gt;\(p\)&lt;/span&gt; standing for the probability of a false
rejection of at least one hypothesis in the experiment gives: &lt;span
class="math inline"&gt;\(\alpha = 1 - (1 - p)^{\frac{1}{r}}\)&lt;/span&gt;. Given
&lt;span class="math inline"&gt;\(r = 276\)&lt;/span&gt; tests and an intended
experiment-wise error rate of &lt;span class="math inline"&gt;\(p =
0.001\)&lt;/span&gt;, we conservatively arrive at &lt;span
class="math inline"&gt;\(\alpha =
10^{-6}\)&lt;/span&gt; after rounding down to control for family-wise error
rate. We use &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; as our
significance level in all tests.&lt;/p&gt;
&lt;p&gt;Cochran’s Q Test is used for significance testing of the accuracy
distributions and Friedman’s &lt;span
class="math inline"&gt;\(\chi^{2}\)&lt;/span&gt; Test for number of executions.
Post-hoc testing was conducted with McNemar’s test and Wilcoxon Signed
Rank Test respectively.&lt;/p&gt;
&lt;h2 id="sec:dataset"&gt;Evaluation Dataset&lt;/h2&gt;
&lt;p&gt;For the study we used a subset of the dataset produced by the
production Culprit Verifier (described in Section &lt;a
href="#sec:verification" data-reference-type="ref"
data-reference="sec:verification"&gt;4&lt;/a&gt;). The evaluation dataset
consists of 13600 test breakages with their required build flags, search
range of versions, and the verified culprit. The verified culprit may be
an indicator that there was no culprit and detected transition was
caused by a flake. Approximately 60% of the items had a culprit version
and 40% did not (as these were caused by a flaky failure).&lt;/p&gt;

&lt;p&gt;Multiple tests may be broken by the same version. When looking at
unique search ranges, we actually see approximately twice as many flaky
ranges as non-flaky ones. This hints towards having tighter bounds on
which ranges are worth culprit finding as all of the runs for two-thirds
of ranges would have been better utilized on more deflaking runs if
these two categories are accurately discernable.&lt;/p&gt;
&lt;p&gt;We therefore examine the overall dataset for discrepancies between
the two categories. For example, flaky ranges have, on average, 8 tests
failing while non-flaky ranges have 224. For the objective of finding
unique real culprits, we are interested in the minimum flake rate over
targets in a range for flaky and non-flaky ranges, respectively. Fig &lt;a
href="#fig:min-flake-rate" data-reference-type="ref"
data-reference="fig:min-flake-rate"&gt;[fig:min-flake-rate]&lt;/a&gt; shows us
that, in fact, 99% of real culprit ranges have targets below a flake
rate of 75%.&lt;/p&gt;

&lt;h2 id="results"&gt;Results&lt;/h2&gt;

&lt;p&gt;&lt;span id="fig:intervals" label="fig:intervals"&gt;
&lt;a href="images/icst-2023/fig2-4.png"&gt;&lt;img alt="Figure 2-4" src="images/icst-2023/fig2-4.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id="fig:min-flake-rate" label="fig:min-flake-rate"&gt;
&lt;a href="images/icst-2023/fig5.png"&gt;&lt;img alt="Figure 5" src="images/icst-2023/fig5.png"&gt;&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq1-which-algorithm-facf-or-bisect-was-more-accurate-in-identifying-culprit-commits"&gt;RQ1:
Which algorithm (FACF or Bisect) was more accurate in identifying
culprit commits?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;All versions of FACF were more accurate overall than all versions of
Bisect (Figure &lt;a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:acc"&gt;2&lt;/a&gt;). The difference was
significant with a p-value &lt;span class="math inline"&gt;\(&amp;lt;
10^{-12}\)&lt;/span&gt;, below the adjusted &lt;span
class="math inline"&gt;\(10^{-6}\)&lt;/span&gt; significance level required.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq2-what-difference-do-algorithmic-tweaks-such-as-adding-deflaking-runs-to-bisect-make-to-accuracy"&gt;RQ2:
What difference do algorithmic tweaks (such as adding deflaking runs to
Bisect) make to accuracy?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;The production version of FACF, FACF(all), was most accurate and
significantly different than all other algorithms (Figure &lt;a
href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:acc"&gt;2&lt;/a&gt;). This was followed by
FACF(fr=.1) which set the floor of 10% on the estimated flake rate for
tests. The other FACF variants were equally accurate.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq3-how-efficient-is-each-algorithm-as-measured-in-number-of-test-executions"&gt;RQ3:
How efficient is each algorithm as measured in number of test
executions?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;Figure &lt;a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals:builds"&gt;2&lt;/a&gt; shows the most efficient
algorithm was unsurprisingly Bisect(1) which does not attempt to do any
extra executions to deflake test failures. The next most efficient
algorithm was FACF(all) which was also the most accurate. As can be seen
in the boxplot in Figure &lt;a href="#fig:intervals"
data-reference-type="ref" data-reference="fig:intervals:builds"&gt;2&lt;/a&gt;,
the FACF variants tend to have many more extreme outliers in their cost
distribution than the Bisect variants. This is because they account for
the prior flake rate. An organization may control their cost by not
culprit finding tests which are more than (for instance) 50% flaky. No
such upper bound was established for this experiment.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq4-what-difference-do-algorithmic-tweaks-such-as-adding-deflaking-runs-to-bisect-make-to-efficiency"&gt;RQ4:
What difference do algorithmic tweaks (such as adding deflaking runs to
Bisect) make to efficiency?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;First, both FACF(all) and FACF(priors) had less cost than the
standard FACF. This indicates that while FACF(priors) does not improve
accuracy it does improve efficiency of the algorithm. In particular many
more culprits were found with just two builds.&lt;/p&gt;
&lt;p&gt;Second, FACF(heur-thresholds) had no significant effect on cost nor
did it show any improvement accuracy. We conclude that although the
mathematical motivation may be sound, in the Google environment, we
cannot validate a benefit to this optimization at this time.&lt;/p&gt;
&lt;p&gt;&lt;h3
id="rq5-does-flakiness-affect-culprit-finding-accuracy-and-cost"&gt;RQ5:
Does flakiness affect culprit finding accuracy and cost?&lt;/h3&gt;&lt;/p&gt;
&lt;p&gt;Figures &lt;a href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals"&gt;3&lt;/a&gt; and &lt;a
href="#fig:intervals" data-reference-type="ref"
data-reference="fig:intervals"&gt;4&lt;/a&gt; show the
accuracy and cost of culprit finding flaky and non-flaky culprits. For
search ranges without a culprit, FACF(all) clearly dominates Bisect in
accuracy and even has a small advantage in cost over Bisect(1).&lt;/p&gt;
&lt;p&gt;When there is a culprit, the FACF variants are all more accurate than
the Bisect variants according to the statistical tests. However, the
difference is much less pronounced with Bisect(1) accuracy &lt;span
class="math inline"&gt;\(\sim\)&lt;/span&gt;96.1% and FACF(all) accuracy &lt;span
class="math inline"&gt;\(\sim\)&lt;/span&gt;97.5%. In terms of cost, Bisect(1)
and Bisect(2) are the most efficient followed by the FACF variants.&lt;/p&gt;
&lt;p&gt;We will note, a production culprit finder cannot know a priori if a
breakage was caused by a flake. It would need to do deflaking runs to
establish that, costing approximately &lt;span
class="math inline"&gt;\(\left\lceil{\frac{\,\textrm{log}\!\!\left({p}\right)}{\,\textrm{log}\!\!\left({\hat{f}_t}\right)}}\right\rceil\)&lt;/span&gt;
where &lt;span class="math inline"&gt;\(p\)&lt;/span&gt; is your desired confidence
level. For &lt;span class="math inline"&gt;\(p=10^{-4}\)&lt;/span&gt; and &lt;span
class="math inline"&gt;\(\hat{f} = .2\)&lt;/span&gt;, this amounts to 6 extra
runs, which would significantly increase the cost of bisection. However,
as noted in Section &lt;a href="#sec:dataset" data-reference-type="ref"
data-reference="sec:dataset"&gt;5.5&lt;/a&gt;, prior flakiness rates could be
used to prune out a significant portion of flaky ranges (lowering cost)
with a minimal hit to accuracy.&lt;/p&gt;

&lt;h1 id="related-work"&gt;Related Work&lt;/h1&gt;
&lt;h2 id="culprit-finding"&gt;Culprit Finding&lt;/h2&gt;
&lt;p&gt;Despite its industrial importance, culprit finding has been
relatively less studied in the literature in comparison to topics such
as Fault Localization &lt;span class="citation"
data-cites="Agarwal2014 Wong2016"&gt;(&lt;a href="#ref-Agarwal2014"
role="doc-biblioref"&gt;Agarwal and Agrawal 2014&lt;/a&gt;; &lt;a
href="#ref-Wong2016" role="doc-biblioref"&gt;Wong et al. 2016&lt;/a&gt;)&lt;/span&gt;
(coverage based &lt;span class="citation"
data-cites="Jones2002 Jones2005 Lucia2014 Henderson2018 Henderson2019 Kucuk2021"&gt;(&lt;a
href="#ref-Jones2002" role="doc-biblioref"&gt;J. a. Jones, Harrold, and
Stasko 2002&lt;/a&gt;; &lt;a href="#ref-Jones2005" role="doc-biblioref"&gt;J. A.
Jones and Harrold 2005&lt;/a&gt;; &lt;a href="#ref-Lucia2014"
role="doc-biblioref"&gt;Lucia et al. 2014&lt;/a&gt;; &lt;a href="#ref-Henderson2018"
role="doc-biblioref"&gt;Henderson and Podgurski 2018&lt;/a&gt;; &lt;a
href="#ref-Henderson2019" role="doc-biblioref"&gt;Henderson, Podgurski, and
Kucuk 2019&lt;/a&gt;; &lt;a href="#ref-Kucuk2021" role="doc-biblioref"&gt;Kucuk,
Henderson, and Podgurski 2021&lt;/a&gt;)&lt;/span&gt;, information retrieval based
&lt;span class="citation" data-cites="Zhou2012 Youm2015 Ciborowska2022"&gt;(&lt;a
href="#ref-Zhou2012" role="doc-biblioref"&gt;J. Zhou, Zhang, and Lo
2012&lt;/a&gt;; &lt;a href="#ref-Youm2015" role="doc-biblioref"&gt;Youm et al.
2015&lt;/a&gt;; &lt;a href="#ref-Ciborowska2022" role="doc-biblioref"&gt;Ciborowska
and Damevski 2022&lt;/a&gt;)&lt;/span&gt;, or slicing based &lt;span class="citation"
data-cites="Podgurski1990 Horwitz1992 Ren2004"&gt;(&lt;a
href="#ref-Podgurski1990" role="doc-biblioref"&gt;Podgurski and Clarke
1990&lt;/a&gt;; &lt;a href="#ref-Horwitz1992" role="doc-biblioref"&gt;Horwitz and
Reps 1992&lt;/a&gt;; &lt;a href="#ref-Ren2004" role="doc-biblioref"&gt;Ren et al.
2004&lt;/a&gt;)&lt;/span&gt;), Bug Prediction &lt;span class="citation"
data-cites="Lewis2013 Punitha2013 Osman2017"&gt;(&lt;a href="#ref-Lewis2013"
role="doc-biblioref"&gt;Lewis et al. 2013&lt;/a&gt;; &lt;a href="#ref-Punitha2013"
role="doc-biblioref"&gt;Punitha and Chitra 2013&lt;/a&gt;; &lt;a
href="#ref-Osman2017" role="doc-biblioref"&gt;Osman et al.
2017&lt;/a&gt;)&lt;/span&gt;, Test Selection &lt;span class="citation"
data-cites="Engstrom2010 pan2022"&gt;(&lt;a href="#ref-Engstrom2010"
role="doc-biblioref"&gt;Engström, Runeson, and Skoglund 2010&lt;/a&gt;; &lt;a
href="#ref-pan2022" role="doc-biblioref"&gt;Pan et al. 2022&lt;/a&gt;)&lt;/span&gt;,
Test Prioritization &lt;span class="citation"
data-cites="DeS.CamposJunior2017 pan2022"&gt;(&lt;a
href="#ref-DeS.CamposJunior2017" role="doc-biblioref"&gt;de S. Campos
Junior et al. 2017&lt;/a&gt;; &lt;a href="#ref-pan2022" role="doc-biblioref"&gt;Pan
et al. 2022&lt;/a&gt;)&lt;/span&gt; and the SZZ algorithm for identifying bug
inducing commits in the context of research studies &lt;span
class="citation"
data-cites="Sliwerski2005 Rodriguez-Perez2018 Borg2019 Wen2019"&gt;(&lt;a
href="#ref-Sliwerski2005" role="doc-biblioref"&gt;Śliwerski, Zimmermann,
and Zeller 2005&lt;/a&gt;; &lt;a href="#ref-Rodriguez-Perez2018"
role="doc-biblioref"&gt;Rodríguez-Pérez, Robles, and González-Barahona
2018&lt;/a&gt;; &lt;a href="#ref-Borg2019" role="doc-biblioref"&gt;Borg et al.
2019&lt;/a&gt;; &lt;a href="#ref-Wen2019" role="doc-biblioref"&gt;Wen et al.
2019&lt;/a&gt;)&lt;/span&gt;. This is perhaps understandable as the problem only
emerges in environments were it becomes to costly to run every test at
every code submission.&lt;/p&gt;
&lt;p&gt;The idea of using a binary search to locate the culprit is common,
but it is not completely clear who invented it first. The Christian
Couder of &lt;code&gt;git bisect&lt;/code&gt; wrote a comprehensive paper on its
implementation &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt; but
indicates there are other similar tools: “So what are the tools used to
fight regressions? [...] The only specific tools are test suites and
tools similar to &lt;code&gt;git bisect&lt;/code&gt;.” The first version of the
&lt;code&gt;git bisect&lt;/code&gt; command appears to have been written by Linus
Torvalds in 2005.&lt;a href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt; Torvald’s commit message simply
states “This is useful for doing binary searching for problems.” The
source code for the mercurial bisect command &lt;code&gt;hg bisect&lt;/code&gt;
states that it was inspired by the git command.&lt;a href="#fn6"
class="footnote-ref" id="fnref6" role="doc-noteref"&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt; The
Subversion (SVN) command, &lt;code&gt;svn bisect&lt;/code&gt;, also cites the git
command as inspiration.&lt;a href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; Therefore, as far as the authors can
tell bisection for finding culprits appears to have been invented by
Torvalds.&lt;/p&gt;
&lt;p&gt;The Couder paper &lt;span class="citation" data-cites="Couder2008"&gt;(&lt;a
href="#ref-Couder2008" role="doc-biblioref"&gt;Couder 2008&lt;/a&gt;)&lt;/span&gt;
cites a GitHub repository for a program called “BBChop,” which is
lightly documented and appears not entirely completed by a user
enigmatically named Ealdwulf Wuffinga (likely a pseudonym, as that was a
name of the King of East Anglia from 664-713).&lt;a href="#fn8"
class="footnote-ref" id="fnref8" role="doc-noteref"&gt;&lt;sup&gt;8&lt;/sup&gt;&lt;/a&gt;
This program claims to have also implemented a Bayesian search for
culprits. We gladly cede the throne for first invention of applying
Bayes rule to culprit finding to Ealdwulf and note that our invention
was independent of the work in BBChop.&lt;/p&gt;
&lt;p&gt;More recently, work has been emerging on the industrial practice of
Culprit Finding. In 2013, Ziftci and Ramavajjala gave a public talk at
the Google Test Automation Conference in on Culprit Finding at Google
&lt;span class="citation" data-cites="Ziftci2013a"&gt;(&lt;a
href="#ref-Ziftci2013a" role="doc-biblioref"&gt;Ziftci and Ramavajjala
2013&lt;/a&gt;)&lt;/span&gt;. They note, that by 2013 Google has already been using
&lt;span class="math inline"&gt;\(m\)&lt;/span&gt;-ary bisection based culprit
finding for “small” and “medium” tests. They then give a preview of an
approach elaborated on in Ziftci and Reardon’s 2017 paper &lt;span
class="citation" data-cites="Ziftci2017"&gt;(&lt;a href="#ref-Ziftci2017"
role="doc-biblioref"&gt;Ziftci and Reardon 2017&lt;/a&gt;)&lt;/span&gt; for culprit
finding integration tests where test executions take a long time to run.
The authors propose a suspiciousness metric based on the minimum build
dependence distance and conduct a case study on the efficacy of using
such a metric to surface potential culprits to developers. We utilize a
similar build dependence distance as input to construct a prior
probability (see Section &lt;a href="#sec:priors" data-reference-type="ref"
data-reference="sec:priors"&gt;3.3&lt;/a&gt;) in this work but do not directly
surface the metric to our end users. Finally, for ground truth they rely
on the developers to report what the culprit version is. In contrast,
the empirical evaluation we present in this new work is fully automated
and does not rely on developers.&lt;/p&gt;
&lt;p&gt;In 2017, Saha and Gligoric &lt;span class="citation"
data-cites="Saha2017"&gt;(&lt;a href="#ref-Saha2017" role="doc-biblioref"&gt;Saha
and Gligoric 2017&lt;/a&gt;)&lt;/span&gt; proposed accelerating the traditional
bisect algorithm via Test Selection techniques. They use Coverage Based
Test Selection &lt;span class="citation" data-cites="Zhou2010 Musa2015"&gt;(&lt;a
href="#ref-Zhou2010" role="doc-biblioref"&gt;Z. Q. Zhou 2010&lt;/a&gt;; &lt;a
href="#ref-Musa2015" role="doc-biblioref"&gt;Musa et al. 2015&lt;/a&gt;)&lt;/span&gt;
to choose which commits are most likely to affect the test results at
the failing commit. This technique can be viewed as fine grained,
dynamic version of the static build dependence selection strategy we
employ at Google.&lt;/p&gt;
&lt;p&gt;In 2019, Najafi &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"
data-cites="Najafi2019a"&gt;(&lt;a href="#ref-Najafi2019a"
role="doc-biblioref"&gt;Najafi, Rigby, and Shang 2019&lt;/a&gt;)&lt;/span&gt; looked at
a similar problem to the traditional culprit finding problem we examine
here. In their scenario, there is a submission queue that queues up
commits to integrate into the main development branch. It batches these
waiting commits and tests them together. If a test fails, they need to
culprit find to determine the culprit commit. The authors evaluate using
a ML-driven risk based batching approach. In 2022, the same research
group re-evaluated the 2019 paper on an open source dataset in
Beheshtian &lt;em&gt;et al.&lt;/em&gt;’s 2022 paper &lt;span class="citation"
data-cites="Beheshtian2022"&gt;(&lt;a href="#ref-Beheshtian2022"
role="doc-biblioref"&gt;Beheshtian, Bavand, and Rigby 2022&lt;/a&gt;)&lt;/span&gt;.
They found that in the new environment the risk based batching didn’t do
as well as a new and improved combinatorics based batching scheme for
identifying the culprit commits.&lt;/p&gt;
&lt;p&gt;James Keenan gave a presentation in 2019 at the Perl Conference in
which he presented a concept, &lt;em&gt;multisection&lt;/em&gt;, for dealing with
multiple bugs in the same search range &lt;span class="citation"
data-cites="keenan2019"&gt;(&lt;a href="#ref-keenan2019"
role="doc-biblioref"&gt;Keenan 2019&lt;/a&gt;)&lt;/span&gt;. Ideally, industrial
culprit finders would automatically detect that there are multiple
distinct bug inducing commits.&lt;/p&gt;
&lt;p&gt;In 2021, two papers looked at using coverage based data for
accelerating bug localization. An and Yoo &lt;span class="citation"
data-cites="An2021"&gt;(&lt;a href="#ref-An2021" role="doc-biblioref"&gt;An and
Yoo 2021&lt;/a&gt;)&lt;/span&gt; used coverage data to accelerate both bisection and
SZZ &lt;span class="citation" data-cites="Sliwerski2005"&gt;(&lt;a
href="#ref-Sliwerski2005" role="doc-biblioref"&gt;Śliwerski, Zimmermann,
and Zeller 2005&lt;/a&gt;)&lt;/span&gt; and found significant speedups. Wen &lt;em&gt;et
al.&lt;/em&gt; &lt;span class="citation" data-cites="Wen2021"&gt;(&lt;a
href="#ref-Wen2021" role="doc-biblioref"&gt;Wen et al. 2021&lt;/a&gt;)&lt;/span&gt;
combined traditional Coverage Based Statistical Fault Localization &lt;span
class="citation" data-cites="Jones2002"&gt;(&lt;a href="#ref-Jones2002"
role="doc-biblioref"&gt;J. a. Jones, Harrold, and Stasko 2002&lt;/a&gt;)&lt;/span&gt;
with historical information to create a new suspiciousness score &lt;span
class="citation" data-cites="Sun2016"&gt;(&lt;a href="#ref-Sun2016"
role="doc-biblioref"&gt;Sun and Podgurski 2016&lt;/a&gt;)&lt;/span&gt;. This fault
localization method could be used in a culprit finding context by using
the line rankings to rank the commits.&lt;/p&gt;
&lt;p&gt;Finally, in 2022 Frolin Ocariza &lt;span class="citation"
data-cites="Ocariza2022"&gt;(&lt;a href="#ref-Ocariza2022"
role="doc-biblioref"&gt;Ocariza 2022&lt;/a&gt;)&lt;/span&gt; published a paper on
bisecting performance regressions. There are strong connections between
a flaky test and a performance regression as a performance regression
may not happen 100% of the time. Furthermore, the “baseline” version may
randomly have poor performance due to environmental or other factors.
Ocariza also arrives at a probabilistic approach, using a Bayesian model
to determine whether a run at a particular version is exhibiting the
performance regression or not. While mathematically related to our work
here and complementary, the approach is distinct. The authors note that
their technique can be combined with a Noisy Binary Search citing Karp
&lt;span class="citation" data-cites="Karp2007"&gt;(&lt;a href="#ref-Karp2007"
role="doc-biblioref"&gt;Karp and Kleinberg 2007&lt;/a&gt;)&lt;/span&gt; or with a
multisection search citing Keenan &lt;span class="citation"
data-cites="keenan2019"&gt;(&lt;a href="#ref-keenan2019"
role="doc-biblioref"&gt;Keenan 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="noisy-binary-search"&gt;Noisy Binary Search&lt;/h2&gt;
&lt;p&gt;There is a vast literature on Rényi-Ulam games and Noisy Searching
problems (doubly so when considering their connection to error
correcting codes)! We direct readers to the Pelc survey &lt;span
class="citation" data-cites="Pelc2002"&gt;(&lt;a href="#ref-Pelc2002"
role="doc-biblioref"&gt;Pelc 2002&lt;/a&gt;)&lt;/span&gt; as a starting point. We will
highlight a few articles here. First and foremost, the Ben Or and
Hassidim paper &lt;span class="citation" data-cites="Ben-Or2008"&gt;(&lt;a
href="#ref-Ben-Or2008" role="doc-biblioref"&gt;Ben-Or and Hassidim
2008&lt;/a&gt;)&lt;/span&gt; from 2008 most clearly explains the problem in terms of
a Bayesian inference, which is the formulation we use here. Second,
Waeber &lt;em&gt;et al.&lt;/em&gt; in 2013 &lt;span class="citation"
data-cites="Waeber2013"&gt;(&lt;a href="#ref-Waeber2013"
role="doc-biblioref"&gt;Waeber, Frazier, and Henderson 2013&lt;/a&gt;)&lt;/span&gt;
discuss the theoretical foundations of these &lt;em&gt;probabilistic bisection
algorithms&lt;/em&gt;. Finally, according to Waeber, Horstein first described
this algorithm in the context of error correcting codes in 1963 &lt;span
class="citation" data-cites="Horstein1963"&gt;(&lt;a href="#ref-Horstein1963"
role="doc-biblioref"&gt;Horstein 1963&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;h1 id="sec:conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Flake Aware Culprit Finding (FACF) is both accurate, efficient, and
able to flexibly incorporate prior information on the location of a
culprit. A large scale empirical study on real test and build breakages
found FACF to be significantly more effective than a traditional
bisection search while not increasing cost over a deflaked
bisection.&lt;/p&gt;

&lt;h1 id="sec:refereces"&gt;References&lt;/h1&gt;
&lt;div id="refs" class="references csl-bib-body hanging-indent"
role="doc-bibliography"&gt;
&lt;ol&gt;
&lt;li&gt;&lt;div id="ref-Agarwal2014" class="csl-entry" role="doc-biblioentry"&gt;
Agarwal, Pragya, and Arun Prakash Agrawal. 2014.
&lt;span&gt;“Fault-Localization &lt;span&gt;Techniques&lt;/span&gt; for &lt;span&gt;Software
Systems&lt;/span&gt;: &lt;span&gt;A Literature Review&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;SIGSOFT
Softw. Eng. Notes&lt;/em&gt; 39 (5): 1–8. &lt;a
href="https://doi.org/10.1145/2659118.2659125"&gt;https://doi.org/10.1145/2659118.2659125&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-An2021" class="csl-entry" role="doc-biblioentry"&gt;
An, Gabin, and Shin Yoo. 2021. &lt;span&gt;“Reducing the Search Space of Bug
Inducing Commits Using Failure Coverage.”&lt;/span&gt; In &lt;em&gt;Proceedings of
the 29th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on &lt;span&gt;European Software
Engineering Conference&lt;/span&gt; and &lt;span&gt;Symposium&lt;/span&gt; on the
&lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;,
1459–62. &lt;span&gt;Athens Greece&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3468264.3473129"&gt;https://doi.org/10.1145/3468264.3473129&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ananthanarayanan2019" class="csl-entry"
role="doc-biblioentry"&gt;
Ananthanarayanan, Sundaram, Masoud Saeida Ardekani, Denis Haenikel,
Balaji Varadarajan, Simon Soriano, Dhaval Patel, and Ali Reza
Adl-Tabatabai. 2019. &lt;span&gt;“Keeping Master Green at Scale.”&lt;/span&gt;
&lt;em&gt;Proceedings of the 14th EuroSys Conference 2019&lt;/em&gt;. &lt;a
href="https://doi.org/10.1145/3302424.3303970"&gt;https://doi.org/10.1145/3302424.3303970&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Beheshtian2022" class="csl-entry" role="doc-biblioentry"&gt;
Beheshtian, Mohammad Javad, Amir Hossein Bavand, and Peter C. Rigby.
2022. &lt;span&gt;“Software &lt;span&gt;Batch Testing&lt;/span&gt; to &lt;span&gt;Save Build
Test Resources&lt;/span&gt; and to &lt;span&gt;Reduce Feedback Time&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt; 48 (8): 2784–2801. &lt;a
href="https://doi.org/10.1109/TSE.2021.3070269"&gt;https://doi.org/10.1109/TSE.2021.3070269&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ben-Or2008" class="csl-entry" role="doc-biblioentry"&gt;
Ben-Or, Michael, and Avinatan Hassidim. 2008. &lt;span&gt;“The &lt;span&gt;Bayesian
Learner&lt;/span&gt; Is &lt;span&gt;Optimal&lt;/span&gt; for &lt;span&gt;Noisy Binary
Search&lt;/span&gt; (and &lt;span&gt;Pretty Good&lt;/span&gt; for &lt;span&gt;Quantum&lt;/span&gt; as
&lt;span&gt;Well&lt;/span&gt;).”&lt;/span&gt; In &lt;em&gt;2008 49th &lt;span&gt;Annual IEEE
Symposium&lt;/span&gt; on &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Computer
Science&lt;/span&gt;&lt;/em&gt;, 221–30. &lt;span&gt;Philadelphia, PA, USA&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/FOCS.2008.58"&gt;https://doi.org/10.1109/FOCS.2008.58&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Bhattacharya2010" class="csl-entry" role="doc-biblioentry"&gt;
Bhattacharya, Pamela, and Iulian Neamtiu. 2010. &lt;span&gt;“Fine-Grained
Incremental Learning and Multi-Feature Tossing Graphs to Improve Bug
Triaging.”&lt;/span&gt; In &lt;em&gt;2010 &lt;span&gt;IEEE International Conference&lt;/span&gt;
on &lt;span&gt;Software Maintenance&lt;/span&gt;&lt;/em&gt;, 1–10. &lt;span&gt;Timi oara,
Romania&lt;/span&gt;: &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSM.2010.5609736"&gt;https://doi.org/10.1109/ICSM.2010.5609736&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Binkley1992" class="csl-entry" role="doc-biblioentry"&gt;
Binkley, D. 1992. &lt;span&gt;“Using Semantic Differencing to Reduce the Cost
of Regression Testing.”&lt;/span&gt; In &lt;em&gt;Proceedings
&lt;span&gt;Conference&lt;/span&gt; on &lt;span&gt;Software Maintenance&lt;/span&gt; 1992&lt;/em&gt;,
41–50. &lt;span&gt;Orlando, FL, USA&lt;/span&gt;: &lt;span&gt;IEEE Comput. Soc.
Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSM.1992.242560"&gt;https://doi.org/10.1109/ICSM.1992.242560&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Borg2019" class="csl-entry" role="doc-biblioentry"&gt;
Borg, Markus, Oscar Svensson, Kristian Berg, and Daniel Hansson. 2019.
&lt;span&gt;“&lt;span&gt;SZZ&lt;/span&gt; Unleashed: An Open Implementation of the
&lt;span&gt;SZZ&lt;/span&gt; Algorithm - Featuring Example Usage in a Study of
Just-in-Time Bug Prediction for the &lt;span&gt;Jenkins&lt;/span&gt;
Project.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 3rd &lt;span&gt;ACM SIGSOFT
International Workshop&lt;/span&gt; on &lt;span&gt;Machine Learning
Techniques&lt;/span&gt; for &lt;span&gt;Software Quality Evaluation&lt;/span&gt; -
&lt;span&gt;MaLTeSQuE&lt;/span&gt; 2019&lt;/em&gt;, 7–12. &lt;span&gt;Tallinn, Estonia&lt;/span&gt;:
&lt;span&gt;ACM Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3340482.3342742"&gt;https://doi.org/10.1145/3340482.3342742&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ciborowska2022" class="csl-entry" role="doc-biblioentry"&gt;
Ciborowska, Agnieszka, and Kostadin Damevski. 2022. &lt;span&gt;“Fast
Changeset-Based Bug Localization with &lt;span&gt;BERT&lt;/span&gt;.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 44th &lt;span&gt;International Conference&lt;/span&gt; on
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 946–57. &lt;span&gt;Pittsburgh
Pennsylvania&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3510003.3510042"&gt;https://doi.org/10.1145/3510003.3510042&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Couder2008" class="csl-entry" role="doc-biblioentry"&gt;
Couder, Christian. 2008. &lt;span&gt;“Fighting Regressions with Git
Bisect.”&lt;/span&gt; &lt;em&gt;The Linux Kernel Archives&lt;/em&gt; 4 (5). &lt;a
href="https://www. kernel. org/pub/software/scm/git/doc s/git-
                  bisect-lk2009.html"&gt;https://www. kernel.
org/pub/software/scm/git/doc s/git- bisect-lk2009.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-DeS.CamposJunior2017" class="csl-entry"
role="doc-biblioentry"&gt;
de S. Campos Junior, Heleno, Marco Antônio P Araújo, José Maria N David,
Regina Braga, Fernanda Campos, and Victor Ströele. 2017. &lt;span&gt;“Test
&lt;span&gt;Case Prioritization&lt;/span&gt;: &lt;span&gt;A Systematic Review&lt;/span&gt; and
&lt;span&gt;Mapping&lt;/span&gt; of the &lt;span&gt;Literature&lt;/span&gt;.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 31st &lt;span&gt;Brazilian Symposium&lt;/span&gt; on
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 34–43. &lt;span&gt;New York, NY,
USA&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3131151.3131170"&gt;https://doi.org/10.1145/3131151.3131170&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Engstrom2010" class="csl-entry" role="doc-biblioentry"&gt;
Engström, Emelie, Per Runeson, and Mats Skoglund. 2010. &lt;span&gt;“A
Systematic Review on Regression Test Selection Techniques.”&lt;/span&gt;
&lt;em&gt;Information and Software Technology&lt;/em&gt; 52 (1): 14–30. &lt;a
href="https://doi.org/10.1016/j.infsof.2009.07.001"&gt;https://doi.org/10.1016/j.infsof.2009.07.001&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Fowler2006" class="csl-entry" role="doc-biblioentry"&gt;
Fowler, Martin. 2006. &lt;span&gt;“Continuous
&lt;span&gt;Integration&lt;/span&gt;.”&lt;/span&gt; &lt;a
href="https://martinfowler.com/articles/
                  continuousIntegration.html"&gt;https://martinfowler.com/articles/
continuousIntegration.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Gupta2011" class="csl-entry" role="doc-biblioentry"&gt;
Gupta, Pooja, Mark Ivey, and John Penix. 2011. &lt;span&gt;“Testing at the
Speed and Scale of &lt;span&gt;Google&lt;/span&gt;.”&lt;/span&gt; &lt;a
href="http://google-engtools.blogspot.com/2011/06/testing-at-
                  speed-and-scale-of-google.html"&gt;http://google-engtools.blogspot.com/2011/06/testing-at-
speed-and-scale-of-google.html&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Henderson2018" class="csl-entry" role="doc-biblioentry"&gt;
Henderson, Tim A. D., and Andy Podgurski. 2018. &lt;span&gt;“Behavioral
&lt;span&gt;Fault Localization&lt;/span&gt; by &lt;span&gt;Sampling Suspicious Dynamic
Control Flow Subgraphs&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2018 &lt;span&gt;IEEE&lt;/span&gt;
11th &lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Software
Testing&lt;/span&gt;, &lt;span&gt;Verification&lt;/span&gt; and &lt;span&gt;Validation&lt;/span&gt;
(&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;, 93–104. &lt;span&gt;Vasteras&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICST.2018.00019"&gt;https://doi.org/10.1109/ICST.2018.00019&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Henderson2019" class="csl-entry" role="doc-biblioentry"&gt;
Henderson, Tim A. D., Andy Podgurski, and Yigit Kucuk. 2019.
&lt;span&gt;“Evaluating &lt;span&gt;Automatic Fault Localization Using Markov
Processes&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019 19th &lt;span&gt;International Working
Conference&lt;/span&gt; on &lt;span&gt;Source Code Analysis&lt;/span&gt; and
&lt;span&gt;Manipulation&lt;/span&gt; (&lt;span&gt;SCAM&lt;/span&gt;)&lt;/em&gt;, 115–26.
&lt;span&gt;Cleveland, OH, USA&lt;/span&gt;: &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/SCAM.2019.00021"&gt;https://doi.org/10.1109/SCAM.2019.00021&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Herzig2015" class="csl-entry" role="doc-biblioentry"&gt;
Herzig, Kim, Michaela Greiler, Jacek Czerwonka, and Brendan Murphy.
2015. &lt;span&gt;“The &lt;span&gt;Art&lt;/span&gt; of &lt;span&gt;Testing Less&lt;/span&gt; Without
&lt;span&gt;Sacrificing Quality&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 37th &lt;span&gt;IEEE International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, 1:483–93.
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE.2015.66"&gt;https://doi.org/10.1109/ICSE.2015.66&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Herzig2015a" class="csl-entry" role="doc-biblioentry"&gt;
Herzig, Kim, and Nachiappan Nagappan. 2015. &lt;span&gt;“Empirically
&lt;span&gt;Detecting False Test Alarms Using Association
Rules&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt;
37th &lt;span&gt;IEEE International Conference&lt;/span&gt; on &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 39–48. &lt;span&gt;Florence, Italy&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE.2015.133"&gt;https://doi.org/10.1109/ICSE.2015.133&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Horstein1963" class="csl-entry" role="doc-biblioentry"&gt;
Horstein, M. 1963. &lt;span&gt;“Sequential Transmission Using Noiseless
Feedback.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Information Theory&lt;/em&gt; 9
(3): 136–43. &lt;a
href="https://doi.org/10.1109/TIT.1963.1057832"&gt;https://doi.org/10.1109/TIT.1963.1057832&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Horwitz1992" class="csl-entry" role="doc-biblioentry"&gt;
Horwitz, S, and T Reps. 1992. &lt;span&gt;“The Use of Program Dependence
Graphs in Software Engineering.”&lt;/span&gt; In &lt;em&gt;International
&lt;span&gt;Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;,
9:349. &lt;span&gt;Springer&lt;/span&gt;. &lt;a
href="http://portal.acm.org/citation.cfm?id=24041&amp;amp;amp;dl="&gt;http://portal.acm.org/citation.cfm?id=24041&amp;amp;amp;dl=&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Jones2002" class="csl-entry" role="doc-biblioentry"&gt;
Jones, J.a., M. J. Harrold, and J. Stasko. 2002. &lt;span&gt;“Visualization of
Test Information to Assist Fault Localization.”&lt;/span&gt; &lt;em&gt;Proceedings
of the 24th International Conference on Software Engineering. ICSE
2002&lt;/em&gt;. &lt;a
href="https://doi.org/10.1145/581339.581397"&gt;https://doi.org/10.1145/581339.581397&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Jones2005" class="csl-entry" role="doc-biblioentry"&gt;
Jones, James A., and Mary Jean Harrold. 2005. &lt;span&gt;“Empirical
&lt;span&gt;Evaluation&lt;/span&gt; of the &lt;span class="nocase"&gt;Tarantula Automatic
Fault-localization Technique&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the
20th &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM International Conference&lt;/span&gt; on
&lt;span&gt;Automated Software Engineering&lt;/span&gt;&lt;/em&gt;, 273–82. &lt;span&gt;New
York, NY, USA&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/1101908.1101949"&gt;https://doi.org/10.1145/1101908.1101949&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Just2014" class="csl-entry" role="doc-biblioentry"&gt;
Just, René, Darioush Jalali, and Michael D Ernst. 2014.
&lt;span&gt;“&lt;span&gt;Defects4J&lt;/span&gt;: &lt;span&gt;A Database&lt;/span&gt; of &lt;span&gt;Existing
Faults&lt;/span&gt; to &lt;span&gt;Enable Controlled Testing Studies&lt;/span&gt; for
&lt;span&gt;Java Programs&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 2014
&lt;span&gt;International Symposium&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;
and &lt;span&gt;Analysis&lt;/span&gt;&lt;/em&gt;, 437–40. &lt;span&gt;New York, NY, USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/2610384.2628055"&gt;https://doi.org/10.1145/2610384.2628055&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Karp2007" class="csl-entry" role="doc-biblioentry"&gt;
Karp, Richard M., and Robert Kleinberg. 2007. &lt;span&gt;“Noisy Binary Search
and Its Applications.”&lt;/span&gt; In &lt;em&gt;Proceedings of the Eighteenth
Annual &lt;span&gt;ACM-SIAM&lt;/span&gt; Symposium on Discrete Algorithms&lt;/em&gt;,
881–90. &lt;span&gt;SODA&lt;/span&gt; ’07. &lt;span&gt;USA&lt;/span&gt;: &lt;span&gt;Society for
Industrial and Applied Mathematics&lt;/span&gt;. &lt;a
href="https://dl.acm.org/doi/abs/10.5555/1283383.1283478"&gt;https://dl.acm.org/doi/abs/10.5555/1283383.1283478&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-keenan2019" class="csl-entry" role="doc-biblioentry"&gt;
Keenan, James. 2019. &lt;span&gt;“James &lt;span&gt;E&lt;/span&gt;. &lt;span&gt;Keenan&lt;/span&gt; -
"&lt;span&gt;Multisection&lt;/span&gt;: &lt;span&gt;When Bisection Isn&lt;/span&gt;’t
&lt;span&gt;Enough&lt;/span&gt; to &lt;span&gt;Debug&lt;/span&gt; a
&lt;span&gt;Problem&lt;/span&gt;".”&lt;/span&gt; &lt;a
href="https://www.youtube.com/watch?v=05CwdTRt6AM"&gt;https://www.youtube.com/watch?v=05CwdTRt6AM&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Kowalczyk2020" class="csl-entry" role="doc-biblioentry"&gt;
Kowalczyk, Emily, Karan Nair, Zebao Gao, Leo Silberstein, Teng Long, and
Atif Memon. 2020. &lt;span&gt;“Modeling and Ranking Flaky Tests at
&lt;span&gt;Apple&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the
&lt;span&gt;ACM&lt;/span&gt;/&lt;span&gt;IEEE&lt;/span&gt; 42nd &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;&lt;/em&gt;, 110–19. &lt;span&gt;Seoul
South Korea&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3377813.3381370"&gt;https://doi.org/10.1145/3377813.3381370&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Kucuk2021" class="csl-entry" role="doc-biblioentry"&gt;
Kucuk, Yigit, Tim A. D. Henderson, and Andy Podgurski. 2021.
&lt;span&gt;“Improving &lt;span&gt;Fault Localization&lt;/span&gt; by &lt;span&gt;Integrating
Value&lt;/span&gt; and &lt;span&gt;Predicate Based Causal Inference
Techniques&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2021
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 43rd &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;
(&lt;span&gt;ICSE&lt;/span&gt;)&lt;/em&gt;, 649–60. &lt;span&gt;Madrid, ES&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE43902.2021.00066"&gt;https://doi.org/10.1109/ICSE43902.2021.00066&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Leong2019" class="csl-entry" role="doc-biblioentry"&gt;
Leong, Claire, Abhayendra Singh, Mike Papadakis, Yves Le Traon, and John
Micco. 2019. &lt;span&gt;“Assessing &lt;span&gt;Transition-Based Test Selection
Algorithms&lt;/span&gt; at &lt;span&gt;Google&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 101–10. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00019"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00019&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Lewis2013" class="csl-entry" role="doc-biblioentry"&gt;
Lewis, Chris, Zhongpeng Lin, Caitlin Sadowski, and Xiaoyan Zhu. 2013.
&lt;span&gt;“Does Bug Prediction Support Human Developers? Findings from a
Google Case Study.”&lt;/span&gt; In &lt;em&gt;Proceedings of the …&lt;/em&gt;, 372–81. &lt;a
href="http://dl.acm.org/citation.cfm?id=2486838"&gt;http://dl.acm.org/citation.cfm?id=2486838&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Lucia2014" class="csl-entry" role="doc-biblioentry"&gt;
Lucia, David Lo, Lingxiao Jiang, Ferdian Thung, and Aditya Budi. 2014.
&lt;span&gt;“Extended Comprehensive Study of Association Measures for Fault
Localization.”&lt;/span&gt; &lt;em&gt;Journal of Software: Evolution and
Process&lt;/em&gt; 26 (2): 172–219. &lt;a
href="https://doi.org/10.1002/smr.1616"&gt;https://doi.org/10.1002/smr.1616&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Machalica2020" class="csl-entry" role="doc-biblioentry"&gt;
Machalica, Mateusz, Wojtek Chmiel, Stanislaw Swierc, and Ruslan
Sakevych. 2020. &lt;span&gt;“Probabilistic &lt;span&gt;Flakiness&lt;/span&gt;:
&lt;span&gt;How&lt;/span&gt; Do You Test Your Tests?”&lt;/span&gt; &lt;em&gt;DEVINFRA&lt;/em&gt;. &lt;a
href="https://engineering.fb.com/2020/12/10/developer-tools/
                  probabilistic-flakiness/"&gt;https://engineering.fb.com/2020/12/10/developer-tools/
probabilistic-flakiness/&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Machalica2019" class="csl-entry" role="doc-biblioentry"&gt;
Machalica, Mateusz, Alex Samylkin, Meredith Porth, and Satish Chandra.
2019. &lt;span&gt;“Predictive &lt;span&gt;Test Selection&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2019
&lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 41st &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 91–100. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2019.00018"&gt;https://doi.org/10.1109/ICSE-SEIP.2019.00018&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-McCrum-Gardner2008" class="csl-entry"
role="doc-biblioentry"&gt;
McCrum-Gardner, Evie. 2008. &lt;span&gt;“Which Is the Correct Statistical Test
to Use?”&lt;/span&gt; &lt;em&gt;British Journal of Oral and Maxillofacial
Surgery&lt;/em&gt; 46 (1): 38–41. &lt;a
href="https://doi.org/10.1016/j.bjoms.2007.09.002"&gt;https://doi.org/10.1016/j.bjoms.2007.09.002&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Memon2017" class="csl-entry" role="doc-biblioentry"&gt;
Memon, Atif, Zebao Gao, Bao Nguyen, Sanjeev Dhanda, Eric Nickell, Rob
Siemborski, and John Micco. 2017. &lt;span&gt;“Taming &lt;span
class="nocase"&gt;Google-scale&lt;/span&gt; Continuous Testing.”&lt;/span&gt; In
&lt;em&gt;2017 &lt;span&gt;IEEE&lt;/span&gt;/&lt;span&gt;ACM&lt;/span&gt; 39th &lt;span&gt;International
Conference&lt;/span&gt; on &lt;span&gt;Software Engineering&lt;/span&gt;: &lt;span&gt;Software
Engineering&lt;/span&gt; in &lt;span&gt;Practice Track&lt;/span&gt;
(&lt;span&gt;ICSE-SEIP&lt;/span&gt;)&lt;/em&gt;, 233–42. &lt;span&gt;Piscataway, NJ, USA&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2017.16"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.16&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Micco2012" class="csl-entry" role="doc-biblioentry"&gt;
Micco, John. 2012. &lt;span&gt;“Tools for &lt;span&gt;Continuous Integration&lt;/span&gt;
at &lt;span&gt;Google Scale&lt;/span&gt;.”&lt;/span&gt; Tech {{Talk}}. &lt;span&gt;Google
NYC&lt;/span&gt;. &lt;a
href="https://youtu.be/KH2_sB1A6lA"&gt;https://youtu.be/KH2_sB1A6lA&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Micco2013" class="csl-entry" role="doc-biblioentry"&gt;
———. 2013. &lt;span&gt;“Continuous &lt;span&gt;Integration&lt;/span&gt; at &lt;span&gt;Google
Scale&lt;/span&gt;.”&lt;/span&gt; Lecture. &lt;span&gt;EclipseCon 2013&lt;/span&gt;. &lt;a
href="https://web.archive.org/web/20140705215747/https://
                  www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
                  03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf"&gt;https://web.archive.org/web/20140705215747/https://
www.eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-
03-24%20Continuous%20Integration%20at%20Google%20Scale.pdf&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Musa2015" class="csl-entry" role="doc-biblioentry"&gt;
Musa, Samaila, Abu Bakar Md Sultan, Abdul Azim Bin Abd-Ghani, and Salmi
Baharom. 2015. &lt;span&gt;“Regression &lt;span&gt;Test Cases&lt;/span&gt; Selection for
&lt;span&gt;Object-Oriented Programs&lt;/span&gt; Based on &lt;span&gt;Affected
Statements&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;International Journal of Software
Engineering and Its Applications&lt;/em&gt; 9 (10): 91–108. &lt;a
href="https://doi.org/10.14257/ijseia.2015.9.10.10"&gt;https://doi.org/10.14257/ijseia.2015.9.10.10&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Najafi2019a" class="csl-entry" role="doc-biblioentry"&gt;
Najafi, Armin, Peter C. Rigby, and Weiyi Shang. 2019. &lt;span&gt;“Bisecting
Commits and Modeling Commit Risk During Testing.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on
&lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
&lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 279–89. &lt;span&gt;New York, NY, USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3338906.3338944"&gt;https://doi.org/10.1145/3338906.3338944&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ocariza2022" class="csl-entry" role="doc-biblioentry"&gt;
Ocariza, Frolin S. 2022. &lt;span&gt;“On the &lt;span&gt;Effectiveness&lt;/span&gt; of
&lt;span&gt;Bisection&lt;/span&gt; in &lt;span&gt;Performance Regression
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Empirical Software Engineering&lt;/em&gt; 27
(4): 95. &lt;a
href="https://doi.org/10.1007/s10664-022-10152-3"&gt;https://doi.org/10.1007/s10664-022-10152-3&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Osman2017" class="csl-entry" role="doc-biblioentry"&gt;
Osman, Haidar, Mohammad Ghafari, Oscar Nierstrasz, and Mircea Lungu.
2017. &lt;span&gt;“An &lt;span&gt;Extensive Analysis&lt;/span&gt; of &lt;span&gt;Efficient Bug
Prediction Configurations&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 13th
&lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Predictive Models&lt;/span&gt;
and &lt;span&gt;Data Analytics&lt;/span&gt; in &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 107–16. &lt;span&gt;Toronto Canada&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3127005.3127017"&gt;https://doi.org/10.1145/3127005.3127017&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-pan2022" class="csl-entry" role="doc-biblioentry"&gt;
Pan, Rongqi, Mojtaba Bagherzadeh, Taher A. Ghaleb, and Lionel Briand.
2022. &lt;span&gt;“Test Case Selection and Prioritization Using Machine
Learning: A Systematic Literature Review.”&lt;/span&gt; &lt;em&gt;Empirical Software
Engineering&lt;/em&gt; 27 (2): 29. &lt;a
href="https://doi.org/10.1007/s10664-021-10066-6"&gt;https://doi.org/10.1007/s10664-021-10066-6&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Pelc2002" class="csl-entry" role="doc-biblioentry"&gt;
Pelc, Andrzej. 2002. &lt;span&gt;“Searching Games with Errorsfifty Years of
Coping with Liars.”&lt;/span&gt; &lt;em&gt;Theoretical Computer Science&lt;/em&gt; 270
(1-2): 71–109. &lt;a
href="https://doi.org/10.1016/S0304-3975(01)00303-6"&gt;https://doi.org/10.1016/S0304-3975(01)00303-6&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Podgurski1990" class="csl-entry" role="doc-biblioentry"&gt;
Podgurski, A, and L A Clarke. 1990. &lt;span&gt;“A &lt;span&gt;Formal Model&lt;/span&gt;
of &lt;span&gt;Program Dependences&lt;/span&gt; and &lt;span&gt;Its Implications&lt;/span&gt;
for &lt;span&gt;Software Testing&lt;/span&gt;, &lt;span&gt;Debugging&lt;/span&gt;, and
&lt;span&gt;Maintenance&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions of Software
Engineering&lt;/em&gt; 16 (9): 965–79. &lt;a
href="https://doi.org/10.1109/32.58784"&gt;https://doi.org/10.1109/32.58784&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Potvin2016" class="csl-entry" role="doc-biblioentry"&gt;
Potvin, Rachel, and Josh Levenberg. 2016. &lt;span&gt;“Why Google Stores
Billions of Lines of Code in a Single Repository.”&lt;/span&gt;
&lt;em&gt;Communications of the ACM&lt;/em&gt; 59 (7): 78–87. &lt;a
href="https://doi.org/10.1145/2854146"&gt;https://doi.org/10.1145/2854146&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Punitha2013" class="csl-entry" role="doc-biblioentry"&gt;
Punitha, K., and S. Chitra. 2013. &lt;span&gt;“Software Defect Prediction
Using Software Metrics - &lt;span&gt;A&lt;/span&gt; Survey.”&lt;/span&gt; In &lt;em&gt;2013
&lt;span&gt;International Conference&lt;/span&gt; on &lt;span&gt;Information
Communication&lt;/span&gt; and &lt;span&gt;Embedded Systems&lt;/span&gt;
(&lt;span&gt;ICICES&lt;/span&gt;)&lt;/em&gt;, 555–58. &lt;span&gt;Chennai&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICICES.2013.6508369"&gt;https://doi.org/10.1109/ICICES.2013.6508369&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ren2004" class="csl-entry" role="doc-biblioentry"&gt;
Ren, Xiaoxia, Fenil Shah, Frank Tip, Barbara G. Ryder, and Ophelia
Chesley. 2004. &lt;span&gt;“Chianti: A Tool for Change Impact Analysis of Java
Programs.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 19th Annual &lt;span&gt;ACM
SIGPLAN&lt;/span&gt; Conference on &lt;span class="nocase"&gt;Object-oriented&lt;/span&gt;
Programming, Systems, Languages, and Applications&lt;/em&gt;, 432–48.
&lt;span&gt;Vancouver BC Canada&lt;/span&gt;: &lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/1028976.1029012"&gt;https://doi.org/10.1145/1028976.1029012&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Rivest1978" class="csl-entry" role="doc-biblioentry"&gt;
Rivest, R. L., A. R. Meyer, and D. J. Kleitman. 1978. &lt;span&gt;“Coping with
Errors in Binary Search Procedures (&lt;span&gt;Preliminary
Report&lt;/span&gt;).”&lt;/span&gt; In &lt;em&gt;Proceedings of the Tenth Annual
&lt;span&gt;ACM&lt;/span&gt; Symposium on &lt;span&gt;Theory&lt;/span&gt; of Computing -
&lt;span&gt;STOC&lt;/span&gt; ’78&lt;/em&gt;, 227–32. &lt;span&gt;San Diego, California, United
States&lt;/span&gt;: &lt;span&gt;ACM Press&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/800133.804351"&gt;https://doi.org/10.1145/800133.804351&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Rodriguez-Perez2018" class="csl-entry"
role="doc-biblioentry"&gt;
Rodríguez-Pérez, Gema, Gregorio Robles, and Jesús M. González-Barahona.
2018. &lt;span&gt;“Reproducibility and Credibility in Empirical Software
Engineering: &lt;span&gt;A&lt;/span&gt; Case Study Based on a Systematic Literature
Review of the Use of the &lt;span&gt;SZZ&lt;/span&gt; Algorithm.”&lt;/span&gt;
&lt;em&gt;Information and Software Technology&lt;/em&gt; 99 (July): 164–76. &lt;a
href="https://doi.org/10.1016/j.infsof.2018.03.009"&gt;https://doi.org/10.1016/j.infsof.2018.03.009&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Saha2017" class="csl-entry" role="doc-biblioentry"&gt;
Saha, Ripon, and Milos Gligoric. 2017. &lt;span&gt;“Selective &lt;span&gt;Bisection
Debugging&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Fundamental &lt;span&gt;Approaches&lt;/span&gt; to
&lt;span&gt;Software Engineering&lt;/span&gt;&lt;/em&gt;, edited by Marieke Huisman and
Julia Rubin, 10202:60–77. &lt;span&gt;Berlin, Heidelberg&lt;/span&gt;:
&lt;span&gt;Springer Berlin Heidelberg&lt;/span&gt;. &lt;a
href="https://doi.org/10.1007/978-3-662-54494-5_4"&gt;https://doi.org/10.1007/978-3-662-54494-5_4&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Sliwerski2005" class="csl-entry" role="doc-biblioentry"&gt;
Śliwerski, Jacek, Thomas Zimmermann, and Andreas Zeller. 2005.
&lt;span&gt;“When Do Changes Induce Fixes?”&lt;/span&gt; &lt;em&gt;ACM SIGSOFT Software
Engineering Notes&lt;/em&gt; 30 (4): 1. &lt;a
href="https://doi.org/10.1145/1082983.1083147"&gt;https://doi.org/10.1145/1082983.1083147&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Sun2016" class="csl-entry" role="doc-biblioentry"&gt;
Sun, Shih-Feng, and Andy Podgurski. 2016. &lt;span&gt;“Properties of
&lt;span&gt;Effective Metrics&lt;/span&gt; for &lt;span&gt;Coverage-Based Statistical
Fault Localization&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2016 &lt;span&gt;IEEE International
Conference&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;,
&lt;span&gt;Verification&lt;/span&gt; and &lt;span&gt;Validation&lt;/span&gt;
(&lt;span&gt;ICST&lt;/span&gt;)&lt;/em&gt;, 124–34. &lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/ICST.2016.31"&gt;https://doi.org/10.1109/ICST.2016.31&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Tharwat2021" class="csl-entry" role="doc-biblioentry"&gt;
Tharwat, Alaa. 2021. &lt;span&gt;“Classification Assessment Methods.”&lt;/span&gt;
&lt;em&gt;Applied Computing and Informatics&lt;/em&gt; 17 (1): 168–92. &lt;a
href="https://doi.org/10.1016/j.aci.2018.08.003"&gt;https://doi.org/10.1016/j.aci.2018.08.003&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Tip1995" class="csl-entry" role="doc-biblioentry"&gt;
Tip, Frank. 1995. &lt;span&gt;“A Survey of Program Slicing Techniques.”&lt;/span&gt;
&lt;em&gt;Journal of Programming Languages&lt;/em&gt; 3 (3): 121–89.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Waeber2013" class="csl-entry" role="doc-biblioentry"&gt;
Waeber, Rolf, Peter I. Frazier, and Shane G. Henderson. 2013.
&lt;span&gt;“Bisection &lt;span&gt;Search&lt;/span&gt; with &lt;span&gt;Noisy
Responses&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;SIAM Journal on Control and
Optimization&lt;/em&gt; 51 (3): 2261–79. &lt;a
href="https://doi.org/10.1137/120861898"&gt;https://doi.org/10.1137/120861898&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Walpole2007" class="csl-entry" role="doc-biblioentry"&gt;
Walpole, Ronald E., ed. 2007. &lt;em&gt;Probability &amp;amp; Statistics for
Engineers &amp;amp; Scientists&lt;/em&gt;. 8th ed. &lt;span&gt;Upper Saddle River,
NJ&lt;/span&gt;: &lt;span&gt;Pearson Prentice Hall&lt;/span&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wang2020" class="csl-entry" role="doc-biblioentry"&gt;
Wang, Kaiyuan, Greg Tener, Vijay Gullapalli, Xin Huang, Ahmed Gad, and
Daniel Rall. 2020. &lt;span&gt;“Scalable Build Service System with Smart
Scheduling Service.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 29th &lt;span&gt;ACM
SIGSOFT International Symposium&lt;/span&gt; on &lt;span&gt;Software Testing&lt;/span&gt;
and &lt;span&gt;Analysis&lt;/span&gt;&lt;/em&gt;, 452–62. &lt;span&gt;Virtual Event USA&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3395363.3397371"&gt;https://doi.org/10.1145/3395363.3397371&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wen2021" class="csl-entry" role="doc-biblioentry"&gt;
Wen, Ming, Junjie Chen, Yongqiang Tian, Rongxin Wu, Dan Hao, Shi Han,
and Shing-Chi Cheung. 2021. &lt;span&gt;“Historical &lt;span&gt;Spectrum Based Fault
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software
Engineering&lt;/em&gt; 47 (11): 2348–68. &lt;a
href="https://doi.org/10.1109/TSE.2019.2948158"&gt;https://doi.org/10.1109/TSE.2019.2948158&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wen2019" class="csl-entry" role="doc-biblioentry"&gt;
Wen, Ming, Rongxin Wu, Yepang Liu, Yongqiang Tian, Xuan Xie, Shing-Chi
Cheung, and Zhendong Su. 2019. &lt;span&gt;“Exploring and Exploiting the
Correlations Between Bug-Inducing and Bug-Fixing Commits.”&lt;/span&gt; In
&lt;em&gt;Proceedings of the 2019 27th &lt;span&gt;ACM Joint Meeting&lt;/span&gt; on
&lt;span&gt;European Software Engineering Conference&lt;/span&gt; and
&lt;span&gt;Symposium&lt;/span&gt; on the &lt;span&gt;Foundations&lt;/span&gt; of &lt;span&gt;Software
Engineering&lt;/span&gt;&lt;/em&gt;, 326–37. &lt;span&gt;Tallinn Estonia&lt;/span&gt;:
&lt;span&gt;ACM&lt;/span&gt;. &lt;a
href="https://doi.org/10.1145/3338906.3338962"&gt;https://doi.org/10.1145/3338906.3338962&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Wong2016" class="csl-entry" role="doc-biblioentry"&gt;
Wong, W. Eric, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa. 2016.
&lt;span&gt;“A &lt;span&gt;Survey&lt;/span&gt; on &lt;span&gt;Software Fault
Localization&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;IEEE Transactions on Software
Engineering&lt;/em&gt; 42 (8): 707–40. &lt;a
href="https://doi.org/10.1109/TSE.2016.2521368"&gt;https://doi.org/10.1109/TSE.2016.2521368&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Youm2015" class="csl-entry" role="doc-biblioentry"&gt;
Youm, Klaus Changsun, June Ahn, Jeongho Kim, and Eunseok Lee. 2015.
&lt;span&gt;“Bug &lt;span&gt;Localization Based&lt;/span&gt; on &lt;span&gt;Code Change
Histories&lt;/span&gt; and &lt;span&gt;Bug Reports&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;2015
&lt;span&gt;Asia-Pacific Software Engineering Conference&lt;/span&gt;
(&lt;span&gt;APSEC&lt;/span&gt;)&lt;/em&gt;, 190–97. &lt;span&gt;New Delhi&lt;/span&gt;:
&lt;span&gt;IEEE&lt;/span&gt;. &lt;a
href="https://doi.org/10.1109/APSEC.2015.23"&gt;https://doi.org/10.1109/APSEC.2015.23&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Zhou2012" class="csl-entry" role="doc-biblioentry"&gt;
Zhou, Jian, Hongyu Zhang, and David Lo. 2012. &lt;span&gt;“Where Should the
Bugs Be Fixed? &lt;span&gt;More&lt;/span&gt; Accurate Information Retrieval-Based
Bug Localization Based on Bug Reports.”&lt;/span&gt; &lt;em&gt;Proceedings -
International Conference on Software Engineering&lt;/em&gt;, 14–24. &lt;a
href="https://doi.org/10.1109/ICSE.2012.6227210"&gt;https://doi.org/10.1109/ICSE.2012.6227210&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Zhou2010" class="csl-entry" role="doc-biblioentry"&gt;
Zhou, Zhi Quan. 2010. &lt;span&gt;“Using Coverage Information to Guide Test
Case Selection in &lt;span&gt;Adaptive Random Testing&lt;/span&gt;.”&lt;/span&gt;
&lt;em&gt;Proceedings - International Computer Software and Applications
Conference&lt;/em&gt;, 208–13. &lt;a
href="https://doi.org/10.1109/COMPSACW.2010.43"&gt;https://doi.org/10.1109/COMPSACW.2010.43&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ziftci2013a" class="csl-entry" role="doc-biblioentry"&gt;
Ziftci, Celal, and Vivek Ramavajjala. 2013. &lt;span&gt;“Finding
&lt;span&gt;Culprits Automatically&lt;/span&gt; in &lt;span&gt;Failing Builds&lt;/span&gt; -
i.e. &lt;span&gt;Who Broke&lt;/span&gt; the &lt;span&gt;Build&lt;/span&gt;?”&lt;/span&gt; &lt;a
href="https://www.youtube.com/watch?v=SZLuBYlq3OM"&gt;https://www.youtube.com/watch?v=SZLuBYlq3OM&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;div id="ref-Ziftci2017" class="csl-entry" role="doc-biblioentry"&gt;
Ziftci, Celal, and Jim Reardon. 2017. &lt;span&gt;“Who Broke the Build?
&lt;span&gt;Automatically&lt;/span&gt; Identifying Changes That Induce Test Failures
in Continuous Integration at Google Scale.”&lt;/span&gt; &lt;em&gt;Proceedings -
2017 IEEE/ACM 39th International Conference on Software Engineering:
Software Engineering in Practice Track, ICSE-SEIP 2017&lt;/em&gt;, 113–22. &lt;a
href="https://doi.org/10.1109/ICSE-SEIP.2017.13"&gt;https://doi.org/10.1109/ICSE-SEIP.2017.13&lt;/a&gt;.
&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;section class="footnotes footnotes-end-of-document"
role="doc-endnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1" role="doc-endnote"&gt;&lt;p&gt;In practice the restriction on
network usage on TAP is somewhat loose. There are legacy tests that have
been tagged to allow them to utilize the network and there is an on
going effort to fully eliminate network usage on TAP.&lt;a href="#fnref1"
class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2" role="doc-endnote"&gt;&lt;p&gt;Other CI platforms exist for
integration tests that span multiple machines, use large amounts of RAM,
or take a very long time to run. The algorithm presented in this paper
has also been implemented for one of those CI systems but we will not
present an empirical evaluation of its performance in that environment
in this paper. In general, Culprit Finding is much more difficult in
such environments as tests may use a mixture of code from different
versions and network resources may be shared across tests.&lt;a
href="#fnref2" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3" role="doc-endnote"&gt;&lt;p&gt;An internal version of Bazel &lt;a
href="https://bazel.build/" class="uri"&gt;https://bazel.build/&lt;/a&gt;.&lt;a
href="#fnref3" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4" role="doc-endnote"&gt;&lt;p&gt;The exact number of hours isn’t so
important. The main thing is to repeat the verification at a suitable
interval from the original attempt.&lt;a href="#fnref4"
class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn5" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://github.com/git/git/commit/8b3a1e056f2107deedfdada86046971c9ad7bb87"
class="uri"&gt;https://github.com/git/git/commit/8b3a1e056f2107deedfdada86046971c9ad7bb87&lt;/a&gt;&lt;a
href="#fnref5" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn6" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://www.mercurial-scm.org/repo/hg/file/52464a20add0/mercurial/hbisect.py"
class="uri"&gt;https://www.mercurial-scm.org/repo/hg/file/52464a20add0/mercurial/hbisect.py&lt;/a&gt;,
&lt;a href="https://www.mercurial-scm.org/repo/hg/rev/a7678cbd7c28"
class="uri"&gt;https://www.mercurial-scm.org/repo/hg/rev/a7678cbd7c28&lt;/a&gt;&lt;a
href="#fnref6" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn7" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://metacpan.org/release/INFINOID/App-SVN-Bisect-1.1/source/README"
class="uri"&gt;https://metacpan.org/release/INFINOID/App-SVN-Bisect-1.1/source/README&lt;/a&gt;&lt;a
href="#fnref7" class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn8" role="doc-endnote"&gt;&lt;p&gt;&lt;a
href="https://github.com/Ealdwulf/bbchop"
class="uri"&gt;https://github.com/Ealdwulf/bbchop&lt;/a&gt;&lt;a href="#fnref8"
class="footnote-back" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="Paper"></category></entry><entry><title>Improving fault localization by integrating value and predicate based causal inference techniques</title><link href="https://hackthology.com/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html" rel="alternate"></link><published>2021-05-26T00:00:00-04:00</published><updated>2021-05-26T00:00:00-04:00</updated><author><name>Yiğit Küçük, &lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2021-05-26:/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html</id><summary type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;Improving fault localization by integrating value and predicate based causal inference techniques&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icse-2021/icse-2021-papers/72/Improving-Fault-Localization-by-Integrating-Value-and-Predicate-Based-Causal-Inferenc"&gt;ICSE 2021&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/ICSE43902.2021.00066"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://doi.org/10.5281/zenodo.4441439"&gt;ARTIFACT&lt;/a&gt;.
&lt;a href="https://hackthology.com/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical fault localization (SFL) techniques use execution profiles and
success/failure information from software executions, in conjunction …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;Improving fault localization by integrating value and predicate based causal inference techniques&lt;/em&gt;.  &lt;a href="https://conf.researchr.org/details/icse-2021/icse-2021-papers/72/Improving-Fault-Localization-by-Integrating-Value-and-Predicate-Based-Causal-Inferenc"&gt;ICSE 2021&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/ICSE43902.2021.00066"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icse-2021-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://doi.org/10.5281/zenodo.4441439"&gt;ARTIFACT&lt;/a&gt;.
&lt;a href="https://hackthology.com/improving-fault-localization-by-integrating-value-and-predicate-based-causal-inference-techniques.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical fault localization (SFL) techniques use execution profiles and
success/failure information from software executions, in conjunction with
statistical inference, to automatically score program elements based on how
likely they are to be faulty. SFL techniques typically employ one type of
profile data: either coverage data, predicate outcomes, or variable values. Most
SFL techniques actually measure correlation, not causation, between profile
values and success/failure, and so they are subject to confounding bias that
distorts the scores they produce. This paper presents a new SFL technique, named
UniVal, that uses causal inference techniques and machine learning to integrate
information about both predicate outcomes and variable values to more accurately
estimate the true failure-causing effect of program statements. UniVal was
empirically compared to several coverage-based, predicate-based, and value-based
SFL techniques on 800 program versions with real faults.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hackthology.com/pdfs/icse-2021.pdf"&gt;Read the paper&lt;/a&gt;&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>The Impact of Rare Failures on Statistical Fault Localization: the Case of the Defects4J Suite</title><link href="https://hackthology.com/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html" rel="alternate"></link><published>2019-10-03T00:00:00-04:00</published><updated>2019-10-03T00:00:00-04:00</updated><author><name>Yiğit Küçük, &lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2019-10-03:/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html</id><summary type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;The Impact of Rare Failures on Statistical Fault Localization: the Case of the Defects4J Suite&lt;/em&gt;.  &lt;a href="https://icsme2019.github.io/"&gt;ICSME 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icsme-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical Fault Localization (SFL) uses coverage profiles (or "spectra")
collected from passing and failing tests, together …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yiğit Küçük, &lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, and Andy Podgurski
&lt;em&gt;The Impact of Rare Failures on Statistical Fault Localization: the Case of the Defects4J Suite&lt;/em&gt;.  &lt;a href="https://icsme2019.github.io/"&gt;ICSME 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="http://tba"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icsme-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/the-impact-of-rare-failures-on-statistical-fault-localization-the-case-of-the-defects4j-suite.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Statistical Fault Localization (SFL) uses coverage profiles (or "spectra")
collected from passing and failing tests, together with statistical metrics,
which are typically composed of simple estimators, to identify which elements of
a program are most likely to have caused observed failures. Previous SFL
research has not thoroughly examined how the effectiveness of SFL metrics is
related to the proportion of failures in test suites and related quantities. To
address this issue, we studied the Defects4J benchmark suite of programs and
test suites and found that if a test suite has very few failures, SFL performs
poorly. To better understand this phenomenon, we investigated the precision of
some statistical estimators of which SFL metrics are composed, as measured by
their coefficients of variation.  The precision of an embedded estimator, which
depends on the dataset, was found to correlate with the effectiveness of a
metric containing it: low precision is associated with poor effectiveness.
Boosting precision by adding test cases was found to improve overall SFL
effectiveness. We present our findings and discuss their implications for the
evaluation and use of SFL metrics.&lt;/p&gt;
&lt;p&gt;(&lt;a href="https://hackthology.com/pdfs/icsme-2019.pdf"&gt;Read the rest of the paper as a pdf&lt;/a&gt;)&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Evaluating Automatic Fault Localization Using Markov Processes</title><link href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html" rel="alternate"></link><published>2019-09-30T00:00:00-04:00</published><updated>2019-09-30T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, Yiğit Küçük, and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2019-09-30:/evaluating-automatic-fault-localization-using-markov-processes.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Yiğit Küçük, and Andy Podgurski
&lt;em&gt;Evaluating Automatic Fault Localization Using Markov Processes&lt;/em&gt;.  &lt;a href="http://www.ieee-scam.org/2019/"&gt;SCAM 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/SCAM.2019.00021"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract …&lt;/h4&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;, Yiğit Küçük, and Andy Podgurski
&lt;em&gt;Evaluating Automatic Fault Localization Using Markov Processes&lt;/em&gt;.  &lt;a href="http://www.ieee-scam.org/2019/"&gt;SCAM 2019&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/SCAM.2019.00021"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/scam-2019-supplement.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/evaluating-automatic-fault-localization-using-markov-processes.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This is a conversion from a latex paper I wrote. If you want all formatting
correct you should read the
&lt;a href="https://hackthology.com/pdfs/scam-2019.pdf"&gt;pdf version&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;
Statistical fault localization (SFL) techniques are commonly compared and
evaluated using a measure known as "Rank Score" and its associated evaluation
process.  In the latter process each SFL technique under comparison is used to
produce a list of program locations, ranked by their suspiciousness scores.
Each technique then receives a Rank Score for each faulty program it is applied
to, which is equal to the rank of the first faulty location in the
corresponding list. The SFL technique whose average Rank Score is lowest is
judged the best overall, based on the assumption that a programmer will examine
each location in rank order until a fault is found.  However, this assumption
*oversimplifies* how an SFL technique would be used in practice.
Programmers are likely to regard suspiciousness ranks as just one source of
information among several that are relevant to locating faults. This paper
provides a new evaluation approach using first-order Markov models of debugging
processes, which can incorporate multiple additional kinds of information,
e.g., about code locality, dependences, or even intuition.  Our approach,
&lt;span&gt;&lt;span class="math inline"&gt;\( \textrm{HT}_{\textrm{Rank}} \)&lt;/span&gt;&lt;/span&gt;, scores SFL techniques based on the expected number of steps a
programmer would take through the Markov model before reaching a faulty
location. Unlike previous evaluation methods, &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; can compare techniques
even when they produce fault localization reports differing in structure or
information granularity.  To illustrate the approach, we present a case study
comparing two existing fault localization techniques that produce results
varying in form and granularity.
&lt;/p&gt;

&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;h1 id="introduction"&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Automatic fault localization is a software engineering technique to assist a programmer during the debugging process by suggesting "suspicious" locations that may contain or overlap with a fault (bug, defect) that is the root cause of observed failures. The big idea behind automatic fault localization (or just fault localization) is that pointing the programmer towards the right area of the program will enable them to find the relevant fault more quickly.&lt;/p&gt;
&lt;p&gt;A much-investigated approach to fault localization is &lt;em&gt;Coverage-Based Statistical Fault Localization&lt;/em&gt; (CBSFL), which is also known as &lt;em&gt;Spectrum-Based Fault Localization&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]-[&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;]&lt;/span&gt;. This approach uses code-coverage profiles and success/failure information from testing or field use of software to rank statements or other generic program locations (e.g., basic blocks, methods, or classes) from most "suspicious" (likely to be faulty) to least suspicious. To perform CBSFL, each test case is run using a version of the program being debugged that has been instrumented to record which potential fault locations were actually executed on that test. A human or automated &lt;em&gt;test oracle&lt;/em&gt; labels each test to indicate whether it passed or failed. The coverage profiles are also referred to as &lt;em&gt;coverage spectra&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the usage scenario typically envisioned for CBSFL, a programmer uses the ranked list of program locations to guide debugging. Starting at the top of the list and moving down, they examine each location to determine if it is faulty. If the location of a fault is near the top of the list, the programmer saves time by avoiding consideration of most of the non-faulty locations in the program. However, if there is no fault near the top of the list, the programmer instead wastes time examining many more locations than necessary. CBSFL techniques are typically evaluated empirically in terms of their ability to rank faulty locations near the top of the list &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;], [&lt;a href="#ref-Pearson2017"&gt;5&lt;/a&gt;]&lt;/span&gt;, as measured by each technique's "Rank Score", which is the location of the first faulty location in the list. A CBSFL technique that consistently ranks faulty statements from a wide range of faulty programs near the top of the corresponding lists is considered a good technique.&lt;/p&gt;
&lt;p&gt;One pitfall of using the ranked-list evaluation regime outlined above is that it can be applied fairly only when the techniques being compared provide results as a prioritized list of program elements of the same granularity. This means that if technique A produces a prioritized list of basic blocks, technique B produces an unordered set of sub-expressions, and technique C produces a prioritized list of classes then it is not valid to use the Standard Rank Score to compare them. The Standard Rank Score can only be applied to ordered lists and thus cannot be used directly to evaluate technique B, and it requires the techniques being compared to have the same granularity. Our new evaluation metric (called &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;) accounts for these differences in granularity and report construction, allowing a direct comparison between different styles of fault localization. We present a case study in Section 27 comparing &lt;em&gt;behavioral&lt;/em&gt; fault localization (which produces fragments of the dynamic control flow graph) to standard CBSFL.&lt;/p&gt;
&lt;p&gt;Second, it is evident that the imagined usage scenario for CBSFL, in which a programmer examines each possible fault location in rank order until a fault is found, is an oversimplification of programmer behavior &lt;span class="citation"&gt;[&lt;a href="#ref-Parnin2011"&gt;6&lt;/a&gt;]&lt;/span&gt;. Programmers are likely to deviate from this scenario, e.g.: by choosing not to re-examine locations that have already been carefully examined and have not changed; by examining the locations around a highly ranked one, regardless of their ranks; by examining locations that a highly ranked location is dependent upon or that are dependent upon it &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt;; by employing a conventional debugger (such as gdb, WinDB, or Visual Studio's debugger); or simply by using their knowledge and intuition about the program.&lt;/p&gt;
&lt;p&gt;To support more flexible and nuanced evaluation criteria for CBSFL and other fault localization techniques, we present a new approach to evaluating them that is based on constructing and analyzing first-order Markov models of debugging processes and that uses a new evaluation metric (&lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;) based on the "hitting time" of a Markov process. This approach allows researchers to directly compare different fault localization techniques by incorporating their results and other relevant information into an appropriate model. To illustrate the approach, we present models for two classes of fault localization techniques: CBSFL and Suspicious Behavior Based Fault Localization (SBBFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. The models we present are also easy to update, allowing researchers to incorporate results of future studies of programmers' behavior during debugging.&lt;/p&gt;
&lt;p&gt;Our new debugging model (and its &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; metric) can be thought of as a first-order simulation of the debugging process as conducted by a programmer. As such, we intend for it to be a practical alternative to conducting an expensive user study. The model is capable of incorporating a variety of behaviors a programmer may exhibit while debugging, allowing researchers to evaluate the performance of their tool against multiple debugging "styles."&lt;/p&gt;
&lt;h1 id="background-and-related-work"&gt;Background and Related Work&lt;/h1&gt;
&lt;p&gt;Coverage Based Statistical Fault Localization (CBSFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;], [&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt; techniques attempt to quantify the likelihood that individual program locations are faulty using sample statistics, called &lt;em&gt;suspiciousness metrics&lt;/em&gt; or &lt;em&gt;fault localization metrics&lt;/em&gt;, which are computed from PASS/FAIL labels assigned to test executions and from coverage profiles (coverage spectra) collected from those executions. A CBSFL suspiciousness metric (of which there are a great many &lt;span class="citation"&gt;[&lt;a href="#ref-Lucia2014"&gt;2&lt;/a&gt;], [&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;]&lt;/span&gt;) measures the statistical association between the occurrence of test failures and the coverage of individual program locations (program elements) of a certain kind.&lt;/p&gt;
&lt;p&gt;Some statistical fault localization techniques use additional information beyond basic coverage information to either improve accuracy or provide more explainable results. For instance, work on &lt;em&gt;Causal Statistical Fault Localization&lt;/em&gt; uses information about the execution of program dependence predecessors of a target statement to adjust for confounding bias that can distort suspiciousness scores &lt;span class="citation"&gt;[&lt;a href="#ref-Baah2010"&gt;10&lt;/a&gt;]&lt;/span&gt;. By contrast, &lt;em&gt;Suspicious-Behavior-Based Fault Localization&lt;/em&gt; (SBBFL) techniques use runtime control-flow information (the behavior) to identify groups of "collaborating" suspicious elements &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. These techniques typically leverage data mining techniques &lt;span class="citation"&gt;[&lt;a href="#ref-Aggarwal2014"&gt;11&lt;/a&gt;]&lt;/span&gt; such as frequent pattern mining &lt;span class="citation"&gt;[&lt;a href="#ref-Agrawal1993"&gt;12&lt;/a&gt;]-[&lt;a href="#ref-Aggarwal2014a"&gt;14&lt;/a&gt;]&lt;/span&gt; or significant pattern mining &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;], [&lt;a href="#ref-Yan2008"&gt;15&lt;/a&gt;]&lt;/span&gt;. Unlike CBSFL techniques, SBBFL techniques output a ranked list of &lt;em&gt;patterns&lt;/em&gt; (subgraphs, itemsets, trees) which each contain multiple program locations. This makes it difficult to directly compare SBBFL and CBSFL techniques using traditional evaluation methods.&lt;/p&gt;
&lt;p&gt;Finally, a variety of non-statistical (or hybrid) approaches to fault localization have been explored &lt;span class="citation"&gt;[&lt;a href="#ref-Abreu2006"&gt;16&lt;/a&gt;]-[&lt;a href="#ref-Wong2016"&gt;19&lt;/a&gt;]&lt;/span&gt;. These approaches range from delta debugging &lt;span class="citation"&gt;[&lt;a href="#ref-Zeller1999"&gt;20&lt;/a&gt;]&lt;/span&gt; to nearest neighbor queries &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt; to program slicing &lt;span class="citation"&gt;[&lt;a href="#ref-Tip1995"&gt;21&lt;/a&gt;], [&lt;a href="#ref-Mao2014"&gt;22&lt;/a&gt;]&lt;/span&gt; to information retrieval &lt;span class="citation"&gt;[&lt;a href="#ref-Marcus2004"&gt;23&lt;/a&gt;]-[&lt;a href="#ref-Le2015"&gt;25&lt;/a&gt;]&lt;/span&gt; to test case generation &lt;span class="citation"&gt;[&lt;a href="#ref-Artzi2010"&gt;26&lt;/a&gt;]-[&lt;a href="#ref-Perez2014"&gt;28&lt;/a&gt;]&lt;/span&gt;. Despite technical and theoretical differences in these approaches, they all suggest locations (or groups of locations) for programmers to consider when debugging.&lt;/p&gt;
&lt;h2 id="the-tarantula-evaluation"&gt;The Tarantula Evaluation&lt;/h2&gt;
&lt;p&gt;Some of the earliest papers on fault localization do not provide a quantitative method for evaluating performance (as is seen in later papers &lt;span class="citation"&gt;[&lt;a href="#ref-Pearson2017"&gt;5&lt;/a&gt;]&lt;/span&gt;). For instance, the earliest CBSFL paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt;, by Jones &lt;em&gt;et al.&lt;/em&gt;, proposes a technique and evaluates it qualitatively using data visualization. At the time, this was entirely appropriate as Jones was proposing a technique for visualizing the relative suspiciousness of different statements, as estimated with what is now called a suspiciousness metric (Tarantula). The visualization used for evaluating this technique aggregated the visualizations for all of the subject programs included in the study.&lt;/p&gt;
&lt;p&gt;While the evaluation method used in the Jones &lt;em&gt;et al.&lt;/em&gt; paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt; effectively communicated the potential of CBSFL (and interested many researchers in the idea) it was not good way to compare multiple fault localization techniques. In 2005 Jones and Harrold &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt; published a study that compared their Tarantula technique to three other techniques: Set Union and Intersection &lt;span class="citation"&gt;[&lt;a href="#ref-Agrawal1995"&gt;29&lt;/a&gt;]&lt;/span&gt;, Nearest Neighbor &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt;, and Cause-Transitions &lt;span class="citation"&gt;[&lt;a href="#ref-Cleve2005"&gt;30&lt;/a&gt;]&lt;/span&gt;. These techniques involved different approaches toward the fault localization problem and originally had been evaluated in different ways. Jones and Harrold re-evaluated all of the techniques under a new common evaluation framework.&lt;/p&gt;
&lt;p&gt;In their 2005 paper, Jones and Harrold evaluated the effectiveness of each fault localization technique by using it to rank the statements in each subject program version from most likely to be the root cause of observed program failures to least likely. For their technique Tarantula, the statements were ranked using the Tarantula suspiciousness score.&lt;a href="#fn1" class="footnoteRef" id="fnref1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; To compare the effectiveness of the techniques, another kind of score was assigned to each faulty version of each subject program. This score is based on the "rank score":&lt;/p&gt;
&lt;h4 id="definition.-tarantula-rank-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Tarantula Rank Score &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; with their suspiciousness scores &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; for &lt;span class="math inline"&gt;\(l
  \in L\)&lt;/span&gt; the Rank Score &lt;span class="math inline"&gt;\(r(l)\)&lt;/span&gt; for a faulty location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; is: &lt;span class="math display"&gt;\[\begin{aligned}
    {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) \ge s(l) \right\} }\right|}}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For Set Union and Intersection, Nearest Neighbor, and Cause-Transitions, Jones and Harrold used an idea of Renieres and Reiss &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;]&lt;/span&gt; and ranked a program's statements based on consideration of its System Dependence Graph (SDG) &lt;span class="citation"&gt;[&lt;a href="#ref-Horwitz1990"&gt;31&lt;/a&gt;]&lt;/span&gt;. The surrogate suspiciousness score of a program location &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; is the inverse of the size of the smallest dependence sphere around &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; that contains a faulty location. The surrogate scores are then used to calculate the Tarantula Rank Score (Def. 14).&lt;/p&gt;
&lt;p&gt;In Jones's and Harrold's evaluation the authors did not use the Tarantula Rank Score directly but instead used a version of it that is normalized by program size:&lt;/p&gt;
&lt;h4 id="definition.-tarantula-effectiveness-score-expense"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Tarantula Effectiveness Score (Expense) &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This score is the proportion of program locations that do &lt;strong&gt;not&lt;/strong&gt; need to be examined to find a fault when the locations are examined in rank order. Formally, let &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; be the total number of program locations, and let &lt;span class="math inline"&gt;\(r(f)\)&lt;/span&gt; be the Tarantula Rank Score (Def. 14) of the faulty location &lt;span class="math inline"&gt;\(f\)&lt;/span&gt;. Then the score is: &lt;span class="math display"&gt;\[\begin{aligned}
    \frac{n-r(f)}{n}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Using the normalized effectiveness score, Jones and Harrold directly compared the fault localization effectiveness of the techniques they considered. They did this in two ways. First, they presented a table that bucketed all the buggy versions of all the programs by their Tarantula Effectiveness Scores (given as percentages). Second, they presented a figure that showed the same data as a cumulative curve.&lt;/p&gt;
&lt;p&gt;The core ideas of Jones' and Harrold's Effectivness/Expense score now underlie most evaluations of CBSFL techniques. Faulty statements are scored, ranked, rank-scored, normalized, and then aggregated over all programs and versions to provide an overall representation of a fault localization method's performance (e.g., &lt;span class="citation"&gt;[&lt;a href="#ref-Steimann2013"&gt;53&lt;/a&gt;]&lt;/span&gt;, [&lt;a href="#ref-Lucia2014"&gt;2&lt;/a&gt;], [&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;], [&lt;a href="#ref-Wong2008"&gt;32&lt;/a&gt;]-[&lt;a href="#ref-Zheng2018"&gt;34&lt;/a&gt;]&lt;/span&gt;). However, some refinements have been made to both the Rank Score and the Effectiveness Score.&lt;/p&gt;
&lt;h2 id="the-implied-debugging-models"&gt;The Implied Debugging Models&lt;/h2&gt;
&lt;p&gt;It is worth (re)stating here the debugging model implied in the Jones and Harrold evaluation &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;. The programmer receives from the fault localization tool a ranked list of statements with the most suspicious statements at the top. The programmer then moves down the list examining each location in turn. If multiple statements have the same rank (the same suspiciousness score) all of those statements are examined before the programmer makes a determination on whether or not the bug has been located. This rule is captured in the mathematical definition of the Tarantula Rank Score (Definition 14).&lt;/p&gt;
&lt;p&gt;For the non-CBSFL methods which Jones compared CBSFL against, the ranks of the program locations were once again compared using the method of Renieres and Reiss &lt;span class="citation"&gt;[&lt;a href="#ref-Renieres2003"&gt;7&lt;/a&gt;], [&lt;a href="#ref-Cleve2005"&gt;30&lt;/a&gt;], [&lt;a href="#ref-ChaoLiu2006"&gt;35&lt;/a&gt;]&lt;/span&gt; which is sometimes called &lt;em&gt;T-Score&lt;/em&gt;. As a reminder, this method computes a surrogate suspiciousness score based on the size of smallest &lt;em&gt;dependence sphere&lt;/em&gt;&lt;a href="#fn2" class="footnoteRef" id="fnref2"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; centered around the locations indicated in the fault localization report that contain the faulty code. This implies a debugging model in which the programmer examines each "shell" of the dependence sphere in turn before moving onto the next larger shell (see Figure 7 in &lt;span class="citation"&gt;[&lt;a href="#ref-Cleve2005"&gt;30&lt;/a&gt;]&lt;/span&gt; for a visualization).&lt;/p&gt;
&lt;p&gt;Neither of these debugging models are realistic. Programmers may be reasonably expected to deviate from the ordering implied by the ranking. During the debugging process a programmer may use a variety of information sources — including intuition — to decide on the next element to examine. They may examine the same element multiple times. They may take a highly circuitous route to the buggy code or via intuition jump immediately to the fault. The models described above allow for none of these subtleties.&lt;/p&gt;
&lt;h2 id="refinements-to-the-evaluation-framework"&gt;Refinements to the Evaluation Framework&lt;/h2&gt;
&lt;p&gt;Wong &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Wong2008"&gt;32&lt;/a&gt;]&lt;/span&gt; introduced the most commonly used effectiveness score, which is called the &lt;span class="math inline"&gt;\(\mathcal{EXAM}\)&lt;/span&gt; score. This score is essentially the same as the Expense score (Def. 15) except that it gives the percentage of locations that need to be examined rather than those avoided.&lt;/p&gt;
&lt;h4 id="definition.-mathcalexam-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. &lt;span class="math inline"&gt;\(\mathcal{EXAM}\)&lt;/span&gt; Score &lt;span class="citation"&gt;[&lt;a href="#ref-Wong2008"&gt;32&lt;/a&gt;]&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{aligned}
    \frac{r(f)}{n}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ali &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Ali2009"&gt;37&lt;/a&gt;]&lt;/span&gt; identified an important problem with Jones' and Harrold's evaluation method: some fault localization techniques always assign different locations distinct suspiciousness scores, but others do not. Ali &lt;em&gt;et al.&lt;/em&gt; pointed out that when comparing techniques, the Tarantula Effectiveness Score may favor a technique that generates more distinct suspiciousness scores than the other techniques. The fix they propose is to assign to a location in a group of locations with the same suspiciousness score a rank score that reflects developers having to examine half the locations in the group on average.&lt;/p&gt;
&lt;h4 id="definition.-standard-rank-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Standard Rank Score&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This score is the expected number of locations a programmer would inspect before locating a fault. Formally, given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; with their suspiciousness scores &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; for &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt;, the Rank Score for a location &lt;span class="math inline"&gt;\(l
  \in L\)&lt;/span&gt; is &lt;span class="citation"&gt;[&lt;a href="#ref-Ali2009"&gt;37&lt;/a&gt;]&lt;/span&gt;: &lt;span class="math display"&gt;\[\begin{aligned}
    {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) &amp;gt; s(l) \right\} }\right|}} +
    \frac{
      {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \right\} }\right|}}
    }{
      2
    }
  \end{aligned}\]&lt;/span&gt; Note: when we refer to the "Standard Rank Score" this is the definition we are referring to.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Parnin and Orso &lt;span class="citation"&gt;[&lt;a href="#ref-Parnin2011"&gt;6&lt;/a&gt;]&lt;/span&gt; conducted a study of programmers' actual use of a statistical fault localization tool (Tarantula &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt;). One of their findings was that programmers did not look deeply through the ranked list of locations and would instead only consider the first few locations. Consequently, they encouraged researchers to no longer report effectiveness scores as percentages. Most CBSFL studies now report absolute (non-percentage) rank scores. This is desirable for another reason: larger programs can have much larger absolute ranks than small programs, for the same percentage rank. Consider, for instance a program with 100,000 lines. If a fault's Rank Score is 10,000 its percentage Exam Score would be 10%. A 10% Exam Score might look like a reasonably good localization (and would be if the program had 100 lines) but no programmer will be willing to look through 10,000 lines. By themselves, percentage evaluation metrics (like Exam Score) produce inherently misleading results for large programs.&lt;/p&gt;
&lt;p&gt;Steinmann &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Steimann2013"&gt;53&lt;/a&gt;]&lt;/span&gt; identified a number of threats to validity in CBSFL studies, including: heterogeneous subject programs, poor test suites, small sample sizes, unclear sample spaces, flaky tests, total number of faults, and masked faults. For evaluation they used the Standard Rank Score of Definition 17 modified to deal with &lt;span class="math inline"&gt;\(k\)&lt;/span&gt; faults tied at the same rank.&lt;/p&gt;
&lt;h4 id="definition.-steinmann-rank-score"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Steinmann Rank Score&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This score is the expected number of locations a programmer would inspect before finding a fault when multiple faulty statements may have the same rank. Formally, given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; with their suspiciousness scores &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; for &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt;, the Rank Score for a location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; is &lt;span class="citation"&gt;[&lt;a href="#ref-Steimann2013"&gt;53&lt;/a&gt;]&lt;/span&gt;: &lt;span class="math display"&gt;\[\begin{aligned}
    &amp;amp; {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) &amp;gt; s(l) \right\} }\right|}}\\
    &amp;amp; + \frac{
          {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \right\} }\right|}} + 1
        }{
          {{\left|{ \left\{ x ~:~ x \in L \wedge s(x) = s(l) \wedge x \text{ is a
          faulty location} \right\}}\right|}} + 1
        }
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Moon &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Moon2014"&gt;38&lt;/a&gt;]&lt;/span&gt; proposed Locality Information Loss (LIL) as an alternative evaluation framework. LIL models the localization result as a probability distribution constructed from the suspiciousness scores:&lt;/p&gt;
&lt;h4 id="definition.-lil-probability-distribution"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. LIL Probability Distribution&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt; be a suspicious metric normalized to the &lt;span class="math inline"&gt;\([0,1]\)&lt;/span&gt; range of reals. Let &lt;span class="math inline"&gt;\(n\)&lt;/span&gt; be the number of locations in the program and let &lt;span class="math inline"&gt;\(L = \{l_1,\ldots,
  l_n\}\)&lt;/span&gt; be the set of locations. The constructed probability distribution is given by: &lt;span class="math display"&gt;\[\begin{aligned}
    P_{\tau}(l_i) = \frac{\tau(l_i)}{\sum^{n}_{j=1} \tau(l_j)}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;LIL uses the Kullback-Leibler measure of divergence between distributions to compute a score indicating how different the distribution constructed for a suspiciousness metric of interest is from the distribution constructed from an "ideal" metric, which gives a score of 1 to the faulty location(s) and gives negligible scores to every other location. The advantage of the LIL framework is that it does not depend on a list of ranked statements and can be applied to non-statistical methods (using a synthetic &lt;span class="math inline"&gt;\(\tau\)&lt;/span&gt;). The disadvantage of LIL is that it does not reflect programmer effort (as the Rank Scores do). However, it may be a better metric to use when evaluating fault localization systems as components of automated fault repair systems.&lt;/p&gt;
&lt;p&gt;Pearson &lt;em&gt;et al.&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Pearson2017"&gt;5&lt;/a&gt;]&lt;/span&gt; re-evaluated a number of previous results using new real world subject programs with real defects and test suites. In contrast to previous work they made use of statistical hypothesis testing and confidence intervals to characterize the uncertainty of the results. To evaluate the performance of each technique under study they used the &lt;span class="math inline"&gt;\(\mathcal{EXAM}\)&lt;/span&gt; score, reporting best, average, and worst case results for multi-statement faults.&lt;/p&gt;
&lt;h1 id="a-new-approach-to-evaluation"&gt;A New Approach to Evaluation&lt;/h1&gt;
&lt;p&gt;Programmers consider multiple sources of information when performing debugging tasks and use them to guide their exploration of the source code. In our new approach to evaluating fault localization techniques, a model is constructed for each technique &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; and each program &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; of how a programmer using &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; might move from examining one location in &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; to examining another. The model for &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(P\)&lt;/span&gt; is used to compute a statistical estimate of the expected number of moves a programmer using &lt;span class="math inline"&gt;\(T\)&lt;/span&gt; would make before encountering a fault in &lt;span class="math inline"&gt;\(P\)&lt;/span&gt;. This estimate is used to compute a "hitting-time rank score" for technique &lt;span class="math inline"&gt;\(T\)&lt;/span&gt;. The scores for all the techniques can then be compared to determine which performed best on program &lt;span class="math inline"&gt;\(P\)&lt;/span&gt;. This section presents the general approach and specific example models. The models make use of CBSFL reports and information about static program structure and dynamic control flow.&lt;/p&gt;
&lt;p&gt;In order to support very flexible modeling and tractable analysis of debugging processes, we use first-order Markov chains (described below) to model them. Our first example models the debugging process assumed in previous work, in which a programmer follows a ranked list of suspicious program locations until a fault is found. Then we describe how to incorporate structural information about the program (which could influence a programmer's debugging behavior). Finally, we show how to model and compare CBSFL to a recent &lt;em&gt;Suspicious Behavioral Based Fault Localization&lt;/em&gt; (SBBFL) algorithm &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; that identifies suspicious subgraphs of dynamic control flow graphs. This third model demonstrates the flexibility of our approach and could be adapted to evaluate other SBBFL techniques &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Liu2005"&gt;39&lt;/a&gt;]-[&lt;a href="#ref-Yousefi2013"&gt;47&lt;/a&gt;]&lt;/span&gt;. It also demonstrates that our approach can be used to compare statistical and non-statistical fault localization techniques under the same assumptions about programmer debugging behavior.&lt;/p&gt;
&lt;p&gt;It is important to emphasize that the quality and value of the evaluation results obtained with our approach depend primarily on the appropriateness of the model of the debugging process that is created. This model represents the evaluators' knowledge about likely programmer behavior during debugging and the factors that influence it. To avoid biasing their evaluation, evaluators must commit to an evaluation model and refrain from "tweaking" it after applying it to the data. &lt;span class="citation"&gt;[&lt;a href="#ref-Ioannidis2005"&gt;48&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id="background-on-ergodic-markov-chains"&gt;Background on Ergodic Markov Chains&lt;/h2&gt;
&lt;p&gt;A finite state Markov chain consists of a set of &lt;em&gt;states&lt;/em&gt; &lt;span class="math inline"&gt;\(S = \{s_1, s_2,
..., s_n \}\)&lt;/span&gt; and an &lt;span class="math inline"&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt;, called the &lt;em&gt;transition matrix&lt;/em&gt; &lt;span class="citation"&gt;[&lt;a href="#ref-Grinstead2012"&gt;49&lt;/a&gt;]&lt;/span&gt;. Entry &lt;span class="math inline"&gt;\({\bf P}_{i,j}\)&lt;/span&gt; gives the probability for a &lt;em&gt;Markov process&lt;/em&gt; in state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; to move to state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt;. The probability of a Markov process moving from one state to another only depends on the state the process is currently in. This is known as the &lt;em&gt;Markov property&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A Markov chain is said to be &lt;em&gt;ergodic&lt;/em&gt; if, given enough steps, it can move from any state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; to any state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt;, i.e. &lt;span class="math inline"&gt;\({\textrm{Pr}\left[{s_i \xrightarrow{*} s_j}\right]} &amp;gt; 0\)&lt;/span&gt;. Thus, there are no states in an ergodic chain that the process can never leave.&lt;/p&gt;
&lt;p&gt;Ergodic Markov chains have &lt;em&gt;stationary distributions&lt;/em&gt;. Let &lt;span class="math inline"&gt;\({\bf v}\)&lt;/span&gt; be an arbitrary probability vector. The stationary distribution is a probability vector &lt;span class="math inline"&gt;\({\bf w}\)&lt;/span&gt; such that &lt;span class="math display"&gt;\[\lim_{n \rightarrow \infty} {\bf v}{\bf P}^{n} = {\bf w}\]&lt;/span&gt; The vector &lt;span class="math inline"&gt;\({\bf w}\)&lt;/span&gt; is a fixed point on &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; implying &lt;span class="math inline"&gt;\({\bf w}{\bf P} = {\bf
w}\)&lt;/span&gt;. Stationary distributions give the long term behavior of a Markov chain - meaning that after many steps the chance a Markov process ends in state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; is given by &lt;span class="math inline"&gt;\({\bf w}_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;expected hitting time&lt;/em&gt; of a state in a Markov chain is the expected number of steps (transitions) a Markov process will make before it encounters the state for the first time. Our new evaluation metric (&lt;span class="math inline"&gt;\(\text{HT}_{\text{Rank}}\)&lt;/span&gt;) uses the expected hitting time of the state representing a faulty program location to score a fault localization technique's performance. Lower expected hitting times yield better localization scores.&lt;/p&gt;
&lt;h4 id="definition.-expected-hitting-time"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Expected Hitting Time&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Consider a Markov chain with transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt;. Let &lt;span class="math inline"&gt;\(T_{i,j}\)&lt;/span&gt; be a random variable denoting the time at which a Markov process that starts at state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; reaches state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt;. The expected hitting time (or just hitting time) of state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt; for such a process is the expected value of &lt;span class="math inline"&gt;\(T_{i,j}\)&lt;/span&gt; &lt;span class="math display"&gt;\[\begin{aligned}
    {\textrm{E}\left[{T_{i,j}}\right]} = \sum_{k=1}^{\infty} k \cdot {\textrm{Pr}\left[{T_{i,j}=k}\right]}
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In general, a hitting time for a single state may be computed in &lt;span class="math inline"&gt;\(\mathcal{O}(n^3)\)&lt;/span&gt; steps &lt;span class="citation"&gt;[&lt;a href="#ref-Kemeny1960"&gt;50&lt;/a&gt;]&lt;/span&gt;. Somewhat less time is required for sparse transition matrices &lt;span class="citation"&gt;[&lt;a href="#ref-Davis2004"&gt;51&lt;/a&gt;]&lt;/span&gt;. Chapter 11 of Grinstead and Snell &lt;span class="citation"&gt;[&lt;a href="#ref-Grinstead2012"&gt;49&lt;/a&gt;]&lt;/span&gt; provides an accessible introduction to hitting time computation.&lt;/p&gt;
&lt;p&gt;Some programs may have too many elements for exact hitting time computations (our case study contains one such program). To deal with large programs the expected hitting time can also be estimated by taking repeated random walks through the Markov chain to obtain a sample of hitting times. The sample can then be used to estimate the expected hitting time by computing the sample mean.&lt;a href="#fn3" class="footnoteRef" id="fnref3"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="expected-hitting-time-rank-score-textrmht_textrmrank"&gt;Expected Hitting Time Rank Score (&lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;)&lt;/h2&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;Node&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;Has&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;bool&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;false&lt;/span&gt;
&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Key&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Key&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;true&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Key&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;left&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Has&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;            &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;right&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Has&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Listing 1.&lt;/strong&gt;
A bug in the implementation of the "Has" method in an AVL tree.
&lt;/div&gt;
&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;Table I: Reduced CBSFL Results for Listing 1&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;rank&lt;/th&gt;
&lt;th align="center"&gt;R. F1&lt;/th&gt;
&lt;th align="left"&gt;Function (Basic Block)&lt;/th&gt;
&lt;th align="left"&gt;rank&lt;/th&gt;
&lt;th align="center"&gt;R. F1&lt;/th&gt;
&lt;th align="left"&gt;Function (BB)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;1.5&lt;/td&gt;
&lt;td align="center"&gt;0.98&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (2)&lt;/td&gt;
&lt;td align="left"&gt;6&lt;/td&gt;
&lt;td align="center"&gt;0.95&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (3)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;1.5&lt;/td&gt;
&lt;td align="center"&gt;0.98&lt;/td&gt;
&lt;td align="left"&gt;Node.String (3)&lt;/td&gt;
&lt;td align="left"&gt;7.5&lt;/td&gt;
&lt;td align="center"&gt;0.94&lt;/td&gt;
&lt;td align="left"&gt;main (1)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;1.5&lt;/td&gt;
&lt;td align="center"&gt;0.98&lt;/td&gt;
&lt;td align="left"&gt;Node.Verify (4)&lt;/td&gt;
&lt;td align="left"&gt;7.5&lt;/td&gt;
&lt;td align="center"&gt;0.94&lt;/td&gt;
&lt;td align="left"&gt;Scanner.Scan (24)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;0.97&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (4)&lt;/td&gt;
&lt;td align="left"&gt;7.5&lt;/td&gt;
&lt;td align="center"&gt;0.94&lt;/td&gt;
&lt;td align="left"&gt;Node.Verify (0)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;4&lt;/td&gt;
&lt;td align="center"&gt;0.97&lt;/td&gt;
&lt;td align="left"&gt;Node.Has (6)&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;td align="center"&gt;&lt;/td&gt;
&lt;td align="left"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src="images/scam/markov-rank-list.png" alt="image" /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig. 1:&lt;/strong&gt; A simplified version of the Markov model for evaluating the ranked list of suspicious locations for the bug in Listing 1.
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;img src="images/scam/markov-rank-list-with-jumps.png" alt="image" /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig. 2:&lt;/strong&gt; An example Markov model showing how “jump” edges can be added to represent how a programmer might examine locations which are near the location they are currently reviewing. Compare to Figure 1.
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;This section introduces our new evaluation metric &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; produces a "rank score" similar to the score produced by the Standard Rank Score of Definition 17. In the standard score, program locations are ranked by their CBSFL suspiciousness scores. A location's position in the ordered list is that location's Rank Score (see the definition for details).&lt;/p&gt;
&lt;p&gt;The new &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; score is obtained as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;A Markov debugging model is supplied (as a Markov chain).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The expected hitting times (Def. 22) for each location in the program are computed.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The locations are ordered by their expected hitting times.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; for a location is its position in the ordered list.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="definition.-textrmht_textrmrank"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a set of locations &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; and a Markov chain &lt;span class="math inline"&gt;\((S, {\bf P})\)&lt;/span&gt; that represents the debugging process and has start state &lt;span class="math inline"&gt;\(0\)&lt;/span&gt;, the Hitting-Time Rank Score &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; for a location &lt;span class="math inline"&gt;\(l \in L\cap S\)&lt;/span&gt; is: &lt;span class="math display"&gt;\[\begin{aligned}
    &amp;amp; {{\left|{ \left\{ x ~:~
        x \in L \cap S \wedge
        {\textrm{E}\left[{T_{0,x}}\right]} &amp;lt; {\textrm{E}\left[{T_{0,l}}\right]}
        \right\}
    }\right|}} +
    \\
    &amp;amp; \frac{
        {{\left|{ \left\{ x ~:~
        x \in L \cap S \wedge
        {\textrm{E}\left[{T_{0,x}}\right]} = {\textrm{E}\left[{T_{0,l}}\right]} \right\}
        }\right|}}
    }{
      2
    }
  \end{aligned}\]&lt;/span&gt; Note: this is almost identical to Definition 17, but it replaces the suspiciousness score with the expected hitting time. Definition 18 can also be modified in a similar way for multi-fault programs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="markov-debugging-models"&gt;Markov Debugging Models&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; is parameterized by a "debugging model" expressed as a Markov chain. As noted above, a Markov chain is made up a of set of states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt; and transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt;. In a debugging model, there are two types of states: 1) textual locations in the source code of a program and 2) synthetic states. Figures 6 and 7 show examples of debugging models constructed for an implementation of an AVL tree. In the figures, the square nodes are Markov states representing basic blocks in the source code. (Note that CBSFL techniques typically compute the same suspiciousness score for all statements in a basic block.) The smaller, circular nodes are Markov states which are synthetic. They are there for structural reasons but do not represent particular locations in the source code. The edges in the graphs represent possible transitions between states, and they are annotated with transition probabilities. All outgoing edges from a node should sum to 1.&lt;/p&gt;
&lt;p&gt;In a Markov debugging model a programmer is represented by the Markov process. When the Markov process is simulated the debugging actions of a programmer are being simulated. This is a "first order" simulation, which means the actions of the simulated programmer (Markov process) only depend on the current location being examined. Thus, the Markov model provides a simple and easy-to-construct mathematical model of a programmer looking through the source code of a program to find the faulty statement(s). The simulated programmer begins at some starting state and moves from state to state until the faulty location is found. We require that all Markov models are &lt;em&gt;ergodic&lt;/em&gt;, ensuring that every state (program location) is eventually reachable in the model.&lt;/p&gt;
&lt;h2 id="an-extensible-markov-model-for-cbsfl"&gt;An Extensible Markov Model for CBSFL&lt;/h2&gt;
&lt;p&gt;As described in Section 12 a CBSFL report is made up of a ranked list of locations in the subject program. Our extensible Markov model includes a representation of the CBSFL ranked list. By itself, using &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; with a Markov model of a ranked list is just a mathematically complex way of restating Definition 17. However, with a Markov model of a CBSFL report in hand we can add further connections &lt;em&gt;between&lt;/em&gt; program locations (represented as Markov states) to represent other actions a programmer might take besides traversing down the ranked list. For instance, in Section 25 we note that programmers use graphical debuggers to traverse the dynamic control flow in a program - allowing them to visit statements in the order they are executed. We embed the dynamic control flow graph into the transition matrix of the Markov model to reflect this observation.&lt;/p&gt;
&lt;h3 id="a-ranked-list-as-a-markov-chain"&gt;A Ranked List as a Markov Chain&lt;/h3&gt;
&lt;p&gt;Figure 6 provides a graphical example of a Markov chain for a ranked list. Since the nodes in a graphical representation of a Markov chain represent states and the edges represent the transition matrix, the probabilities associated with outgoing edges of each node should sum to 1. In Figure 6, each circular node represents a rank in the list and the square nodes represent associated program locations, which are identified by their function names and static basic-block id number. The square nodes that are grouped together all have the same suspiciousness scores. We will provide here a brief, informal description of the structure of the transition matrix. A formal description of the chain is provided in Definition 28 in the Appendix. The exact choice of transition matrix in the formal chain was driven by a proof of equivalence between &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; with this Markov model (as defined in Definition 28) and the Standard Rank Score (Definition 17).&lt;a href="#fn4" class="footnoteRef" id="fnref4"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The transition matrix boils down to a couple of simple connections. The nodes representing groups form a doubly linked list (see the circular nodes in Figure 6). The ordering of the "group nodes" matches the ordering of the ranks in the ranked list. The links between the node are weighted so that a Markov process will tend to end up in a highly ranked node. More formally, the model was constructed so that if you ordered the Markov states by the probabilities in the &lt;em&gt;stationary distribution&lt;/em&gt; (which characterizes the long term behavior of the Markov process) from highest to lowest that ordering would match the order of the ranked list.&lt;/p&gt;
&lt;p&gt;The second type of connection is from a group node to its program location nodes. Each location node connects to exactly one group node. The transition probabilities (as shown in Figure 6) are set up such that there is an equal chance of moving to any of the locations in the group.&lt;/p&gt;
&lt;p&gt;The final connection is from a location node back to its group node. This is always assigned probability &lt;span class="math inline"&gt;\(1\)&lt;/span&gt; (see Figure 6). Again, see Definition 28 in the Appendix for the formal description.&lt;/p&gt;
&lt;h3 id="adding-local-jumps-to-the-cbsfl-chain"&gt;Adding Local Jumps to the CBSFL Chain&lt;/h3&gt;
&lt;p&gt;Figure 7 shows a modified version of the model in Figure 6. The modified model allows the Markov process to jump or "teleport" between program locations which are not adjacent in the CBSFL Ranked List. These "jump" connections are setup to model other ways a programmer might move through the source code of a program when debugging (see below). In the figure, these jumps are shown with added red and blue edges. For the Markov model depicted in the figure, the Markov process will with probability &lt;span class="math inline"&gt;\(\frac{1}{2}\)&lt;/span&gt; move back to the rank list and with probability &lt;span class="math inline"&gt;\(\frac{1}{2}\)&lt;/span&gt; teleport or jump to an alternative program location that is structurally adjacent (in the program's source code) to the current one.&lt;/p&gt;
&lt;p&gt;Informally, the modified model is set up so that if the Markov process is in a state &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; which represents program location &lt;span class="math inline"&gt;\(l_x\)&lt;/span&gt; it will with probability &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; move to a state &lt;span class="math inline"&gt;\(s_j\)&lt;/span&gt; which represents program location &lt;span class="math inline"&gt;\(l_y\)&lt;/span&gt; instead of returning to the rank list. The locations that the process can move to from &lt;span class="math inline"&gt;\(s_i\)&lt;/span&gt; are defined in a jump matrix &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; which is parameter to the Markov model. The matrix &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; encodes assumptions or observations about how programmers behave when they are debugging. A formal definition of the CBSFL chain with jumps is presented in the Appendix (see Definition 29).&lt;/p&gt;
&lt;p&gt;Definition 26 defines one general-purpose jump matrix &lt;span class="math inline"&gt;\({\bf
J}\)&lt;/span&gt;. It encodes two assumptions about programmer behavior. First, when a programmer considers a location inside a function they will also potentially examine other locations in that function. Second, when a programmer is examining a location they may examine locations preceding or succeeding it the dynamic control flow (e.g., with the assistance of a graphical debugger). Definition 26 encodes both assumptions by setting the relevant &lt;span class="math inline"&gt;\({\bf
J}_{i,j}\)&lt;/span&gt; entries to &lt;span class="math inline"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id="definition.-spacial-behavioral-jumps"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Spacial + Behavioral Jumps&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{aligned}
    {\bf J}_{i,j} &amp;amp;=
    \left\{
      \begin{array}{cll}
          1 &amp;amp;
          \text{if}
          &amp;amp;    \text{$s_i$ and $s_j$ represent locations} \\
          &amp;amp; &amp;amp;  \text{in the same function}
               \\
        \\
          1 &amp;amp;
          \text{if}
          &amp;amp;    \text{$s_i$ and $s_j$ are adjacent locations in the} \\
          &amp;amp; &amp;amp;  \text{program&amp;#39;s dynamic control flow graph} \\
        \\
          0
          &amp;amp; \text{otherwise}
      \end{array}
    \right.
  \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In addition to &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt;, the new chain is parameterized by the probability &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; of a jump occurring when the process visits a state representing a program location. As &lt;span class="math inline"&gt;\(p_{\text{jump}} \rightarrow 0\)&lt;/span&gt; the transition matrix of new chain approaches the transition matrix for the chain in Definition 28 (see the Appendix). We suggest setting &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; to &lt;span class="math inline"&gt;\(0.5\)&lt;/span&gt; in the absence of data from a user study.&lt;/p&gt;
&lt;h2 id="modeling-sbbfl"&gt;Modeling SBBFL&lt;/h2&gt;
&lt;p&gt;As noted earlier, Markov models can be constructed for alternative fault localization techniques. Suspicious Behavior Based Fault Localization (SBBFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; techniques return a report containing a ranked list of subgraphs or subtrees. Each subgraph contains multiple program locations usually drawn from a dynamic control flow graph of the program. Comparing this output to CBSFL using the Standard Rank Score can be difficult as a location may appear multiple times in graphs returned by the SBBFL algorithm. However, &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; can produce an accurate and comparable score by utilizing the expected hitting times of the states representing the faulty location. Definition 30 in the Appendix provides an example Markov chain which models a ranked list of suspicious subgraphs. It can be extended (not shown due to space constraints) to add a Jump matrix in the manner of Definition 29 (see the Appendix).&lt;/p&gt;
&lt;h1 id="case-study"&gt;Case Study&lt;/h1&gt;
&lt;p&gt;To illustrate our new approach to evaluation, we performed a case study in which it was used to evaluate several fault localization techniques of two different types: CBSFL &lt;span class="citation"&gt;[&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;], [&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt; and Suspicious-Behavior-Based Fault Localization (SBBFL) &lt;span class="citation"&gt;[&lt;a href="#ref-Cheng2009a"&gt;8&lt;/a&gt;], [&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. We investigated the following research questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RQ1&lt;/strong&gt;: How Accurate is &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Estimation? (Table III)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RQ2&lt;/strong&gt;: Does it make a difference whether &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; or the Standard Rank Score is used? (Table IV, Figs. 5 and 4)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We also considered an important general question about fault localization that a typical research study might investigate.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;RQ3&lt;/strong&gt;: Which kind of fault localization technique performs better, SBBFL or CBSFL? (Table IV)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to use an SBBFL technique, more complex profiling data (dynamic control flow graphs or DCFGs) are needed than for CBSFL (which requires only coverage data). Therefore, a more specialized profiler was required for this study. We used Dynagrok (https://github.com/timtadh/dynagrok) &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; - a profiling tool for the Go Programming Language. We also reused subject programs used in a previous study &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt; (see Table II). They are all real-world Go programs of various sizes that were injected with mutation faults. The test cases for the programs are all either real-world inputs or test cases from the system regression testing suites that are distributed with the programs. Six representative suspiciousness metrics were considered: Ochiai, F1, Jaccard, Relative Ochiai, Relative F1, and Relative Jaccard &lt;span class="citation"&gt;[&lt;a href="#ref-Sun2016"&gt;3&lt;/a&gt;]&lt;/span&gt;. These measures were all previously adapted to the SBBFL context &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;Table II: Datasets used in the evaluation&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;Program&lt;/th&gt;
&lt;th align="right"&gt;L.O.C.&lt;/th&gt;
&lt;th align="right"&gt;Mutants&lt;/th&gt;
&lt;th align="left"&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;AVL (github.com/timtadh/dynagrok)&lt;/td&gt;
&lt;td align="right"&gt;483&lt;/td&gt;
&lt;td align="right"&gt;19&lt;/td&gt;
&lt;td align="left"&gt;An AVL tree&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Blackfriday (github.com/russross/blackfriday)&lt;/td&gt;
&lt;td align="right"&gt;8,887&lt;/td&gt;
&lt;td align="right"&gt;19&lt;/td&gt;
&lt;td align="left"&gt;Markdown processor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;HTML (golang.org/x/net/html)&lt;/td&gt;
&lt;td align="right"&gt;9,540&lt;/td&gt;
&lt;td align="right"&gt;20&lt;/td&gt;
&lt;td align="left"&gt;An HTML parser&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Otto (github.com/robertkrimen/otto)&lt;/td&gt;
&lt;td align="right"&gt;39,426&lt;/td&gt;
&lt;td align="right"&gt;20&lt;/td&gt;
&lt;td align="left"&gt;Javascript interpreter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;&lt;span&gt;&lt;code&gt;gc&lt;/code&gt;&lt;/span&gt; (go.googlesource.com/go)&lt;/td&gt;
&lt;td align="right"&gt;51,873&lt;/td&gt;
&lt;td align="right"&gt;16&lt;/td&gt;
&lt;td align="left"&gt;The Go compiler&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;p&gt;Note: The AVL tree is in the examples directory.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;All applications of techniques were replicated to address random variation in the results as the SBBFL algorithm that we used employs sampling &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;. The results from the replications were averaged and, unless otherwise noted, the average Rank Scores are presented. Finally, the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; score was computed for 4 of the 5 programs. For the fifth program, the Go compiler, although &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; can be computed exactly, this requires so much time for each run to complete (&lt;span class="math inline"&gt;\(\approx\)&lt;/span&gt; 4 hours) that it precluded computing exact results for each program version (including all models for both SBBFL and CBSFL). Therefore, expected hitting times for the Go compiler were estimated using the method outlined in Section 21. Specifically, we collected 500 samples of hitting times of all states in the Markov debugging model by taking random walks (with a maximum walk length of 1,000,000 steps). The expected hitting times were estimated by taking the sample mean.&lt;/p&gt;
&lt;h2 id="the-chosen-textrmht_textrmrank-model"&gt;The Chosen &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Model&lt;/h2&gt;
&lt;p&gt;To avoid bias, it was important to choose the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; model before the case study was conducted, which we did. We used &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; with jumps (Definition 29) together with the jump matrix specified in Definition 26. We chose this matrix because we believe the order in which programmers examine program elements during debugging is driven in part by the structure of the program, even when they are debugging with the assistance of a fault localization report. The &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; probability was set to &lt;span class="math inline"&gt;\(0.5\)&lt;/span&gt; to indicate an equal chance of the programmer using or not using the fault localization report. A future large scale user study could empirically characterize the behavior of programmers while performing debugging to inform the choice of the &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; parameter. However, without such a study it is reasonable to use &lt;span class="math inline"&gt;\(0.5\)&lt;/span&gt;, which indicates "no information."&lt;/p&gt;
&lt;h2 id="rq1-how-accurate-is-textrmht_textrmrank-estimation"&gt;RQ1: How Accurate is &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Estimation?&lt;/h2&gt;
&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;&lt;span&gt;&lt;span class="math inline"&gt;Table III: \(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Estimation Error&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;Subject Program&lt;/th&gt;
&lt;th align="center"&gt;avl&lt;/th&gt;
&lt;th align="center"&gt;blackfriday&lt;/th&gt;
&lt;th align="center"&gt;html&lt;/th&gt;
&lt;th align="center"&gt;otto&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;mean   median   stdev   p-value&lt;/td&gt;
&lt;td align="center"&gt;0.781&lt;/td&gt;
&lt;td align="center"&gt;0.469&lt;/td&gt;
&lt;td align="center"&gt;0.473&lt;/td&gt;
&lt;td align="center"&gt;0.476&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;p&gt;Percentage error of the estimated &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; versus the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; (see note under Def 22). Letting &lt;span class="math inline"&gt;\(y\)&lt;/span&gt; be the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; and &lt;span class="math inline"&gt;\(\hat{y}\)&lt;/span&gt; be the estimated &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;, the percentage error is &lt;span class="math inline"&gt;\(\frac{|\hat{y} - y|}{y} * 100\)&lt;/span&gt;. &lt;span&gt;-2.0em&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For large programs the cost of computing the expected hitting times for &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; may be too high. Therefore, we have suggested estimating the expected hitting times (see note under Definition 22). To assess the accuracy of estimating the expected hitting time instead of computing it exactly, we did both for four subject programs, using the Relative F1 measure. For each program version, SFL technique, and Markov chain type the estimation error was computed as a percentage of the true value. Table III presents descriptive statistics characterizing these errors for each subject program. The last row of the table gives p-values from a independent two-sample T-test comparing the estimated &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; values and the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; values. The null hypothesis is that the expected estimate &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; is the same as the expected exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. All p-values are well above the &lt;span class="math inline"&gt;\(0.05\)&lt;/span&gt; significance level that would suggest rejecting the null hypothesis. Therefore, we accept the null hypotheses that the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; estimates are not significantly different from the exact &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; values. As indicated in the table, the maximum estimation error was around 3.7%. Therefore, if estimation is used we recommend considering &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; scores that are within 5% of each other to be equivalent.&lt;/p&gt;
&lt;h2 id="rq2-does-it-make-a-difference-whether-textrmht_textrmrank-or-the-standard-rank-score-is-used"&gt;RQ2: Does it make a difference whether &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; or the Standard Rank Score is used? &lt;/h2&gt;
&lt;table&gt;
&lt;caption&gt; &lt;span style="font-variant: small-caps;"&gt;Table IV: Fault Localization Performance&lt;/span&gt; &lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="left"&gt;&lt;/th&gt;
&lt;th align="center" colspan=3&gt;Standard Rank Score&lt;/th&gt;
&lt;th align="center" colspan=3&gt;HTRank Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;th align="left"&gt;score&lt;/td&gt;
&lt;th align="left"&gt;subject&lt;/td&gt;
&lt;th align="center"&gt;CBSFL&lt;/td&gt;
&lt;th align="center"&gt;SBBFL&lt;/td&gt;
&lt;th align="center"&gt; %&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt; &lt;/td&gt;
&lt;th align="center"&gt;CBSFL&lt;/td&gt;
&lt;th align="center"&gt;SBBFL&lt;/td&gt;
&lt;th align="center"&gt; %&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;9.1&lt;/td&gt;
&lt;td align="center"&gt;4.9&lt;/td&gt;
&lt;td align="center"&gt;-47 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;8.8&lt;/td&gt;
&lt;td align="center"&gt;4.4&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;td align="center"&gt;42.6&lt;/td&gt;
&lt;td align="center"&gt;11.0&lt;/td&gt;
&lt;td align="center"&gt;-74 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;40.7&lt;/td&gt;
&lt;td align="center"&gt;25.3&lt;/td&gt;
&lt;td align="center"&gt;-38 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.2&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;102.2&lt;/td&gt;
&lt;td align="center"&gt;21.5&lt;/td&gt;
&lt;td align="center"&gt;-79 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeF1&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;264.8&lt;/td&gt;
&lt;td align="center"&gt;98.9&lt;/td&gt;
&lt;td align="center"&gt;-63 %&lt;/td&gt;
&lt;td align="center"&gt;1148.9&lt;/td&gt;
&lt;td align="center"&gt;1475.1&lt;/td&gt;
&lt;td align="center"&gt;28 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;7.6&lt;/td&gt;
&lt;td align="center"&gt;6.3&lt;/td&gt;
&lt;td align="center"&gt;-16 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;8.8&lt;/td&gt;
&lt;td align="center"&gt;4.4&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;td align="center"&gt;43.6&lt;/td&gt;
&lt;td align="center"&gt;9.8&lt;/td&gt;
&lt;td align="center"&gt;-78 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;38.2&lt;/td&gt;
&lt;td align="center"&gt;22.4&lt;/td&gt;
&lt;td align="center"&gt;-41 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.7&lt;/td&gt;
&lt;td align="center"&gt;6.8&lt;/td&gt;
&lt;td align="center"&gt;-22 %&lt;/td&gt;
&lt;td align="center"&gt;99.2&lt;/td&gt;
&lt;td align="center"&gt;102.6&lt;/td&gt;
&lt;td align="center"&gt;3 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeOchiai&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;262.2&lt;/td&gt;
&lt;td align="center"&gt;101.6&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;2888.8&lt;/td&gt;
&lt;td align="center"&gt;2984.1&lt;/td&gt;
&lt;td align="center"&gt;3 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;10.1&lt;/td&gt;
&lt;td align="center"&gt;5.1&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;16.0&lt;/td&gt;
&lt;td align="center"&gt;12.1&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;80.0&lt;/td&gt;
&lt;td align="center"&gt;176.2&lt;/td&gt;
&lt;td align="center"&gt;120 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;33.3&lt;/td&gt;
&lt;td align="center"&gt;24.4&lt;/td&gt;
&lt;td align="center"&gt;-27 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.3&lt;/td&gt;
&lt;td align="center"&gt;-23 %&lt;/td&gt;
&lt;td align="center"&gt;75.2&lt;/td&gt;
&lt;td align="center"&gt;19.9&lt;/td&gt;
&lt;td align="center"&gt;-74 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;RelativeJaccard&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;747.9&lt;/td&gt;
&lt;td align="center"&gt;482.0&lt;/td&gt;
&lt;td align="center"&gt;-36 %&lt;/td&gt;
&lt;td align="center"&gt;1074.1&lt;/td&gt;
&lt;td align="center"&gt;1540.5&lt;/td&gt;
&lt;td align="center"&gt;43 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;10.4&lt;/td&gt;
&lt;td align="center"&gt;5.1&lt;/td&gt;
&lt;td align="center"&gt;-51 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;16.0&lt;/td&gt;
&lt;td align="center"&gt;12.1&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;81.9&lt;/td&gt;
&lt;td align="center"&gt;218.8&lt;/td&gt;
&lt;td align="center"&gt;167 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;33.9&lt;/td&gt;
&lt;td align="center"&gt;18.6&lt;/td&gt;
&lt;td align="center"&gt;-45 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.2&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;81.0&lt;/td&gt;
&lt;td align="center"&gt;28.5&lt;/td&gt;
&lt;td align="center"&gt;-65 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;F1&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;746.8&lt;/td&gt;
&lt;td align="center"&gt;470.4&lt;/td&gt;
&lt;td align="center"&gt;-37 %&lt;/td&gt;
&lt;td align="center"&gt;1047.0&lt;/td&gt;
&lt;td align="center"&gt;1537.9&lt;/td&gt;
&lt;td align="center"&gt;47 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;5.9&lt;/td&gt;
&lt;td align="center"&gt;6.3&lt;/td&gt;
&lt;td align="center"&gt;7 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;14.8&lt;/td&gt;
&lt;td align="center"&gt;10.3&lt;/td&gt;
&lt;td align="center"&gt;-30 %&lt;/td&gt;
&lt;td align="center"&gt;51.2&lt;/td&gt;
&lt;td align="center"&gt;108.7&lt;/td&gt;
&lt;td align="center"&gt;112 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;30.7&lt;/td&gt;
&lt;td align="center"&gt;25.3&lt;/td&gt;
&lt;td align="center"&gt;-18 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;10.3&lt;/td&gt;
&lt;td align="center"&gt;8.4&lt;/td&gt;
&lt;td align="center"&gt;-19 %&lt;/td&gt;
&lt;td align="center"&gt;312.8&lt;/td&gt;
&lt;td align="center"&gt;607.7&lt;/td&gt;
&lt;td align="center"&gt;94 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Ochiai&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;1091.6&lt;/td&gt;
&lt;td align="center"&gt;773.5&lt;/td&gt;
&lt;td align="center"&gt;-29 %&lt;/td&gt;
&lt;td align="center"&gt;3137.2&lt;/td&gt;
&lt;td align="center"&gt;2988.8&lt;/td&gt;
&lt;td align="center"&gt;-5 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;avl&lt;/td&gt;
&lt;td align="center"&gt;4.6&lt;/td&gt;
&lt;td align="center"&gt;2.0&lt;/td&gt;
&lt;td align="center"&gt;-56 %&lt;/td&gt;
&lt;td align="center"&gt;10.1&lt;/td&gt;
&lt;td align="center"&gt;5.1&lt;/td&gt;
&lt;td align="center"&gt;-50 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;blackfriday&lt;/td&gt;
&lt;td align="center"&gt;16.0&lt;/td&gt;
&lt;td align="center"&gt;12.1&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;77.8&lt;/td&gt;
&lt;td align="center"&gt;187.7&lt;/td&gt;
&lt;td align="center"&gt;141 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;html&lt;/td&gt;
&lt;td align="center"&gt;12.8&lt;/td&gt;
&lt;td align="center"&gt;5.0&lt;/td&gt;
&lt;td align="center"&gt;-61 %&lt;/td&gt;
&lt;td align="center"&gt;33.3&lt;/td&gt;
&lt;td align="center"&gt;24.6&lt;/td&gt;
&lt;td align="center"&gt;-26 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;otto&lt;/td&gt;
&lt;td align="center"&gt;8.2&lt;/td&gt;
&lt;td align="center"&gt;6.2&lt;/td&gt;
&lt;td align="center"&gt;-24 %&lt;/td&gt;
&lt;td align="center"&gt;75.3&lt;/td&gt;
&lt;td align="center"&gt;24.7&lt;/td&gt;
&lt;td align="center"&gt;-67 %&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td align="left"&gt;Jaccard&lt;/td&gt;
&lt;td align="left"&gt;compiler&lt;/td&gt;
&lt;td align="center"&gt;747.9&lt;/td&gt;
&lt;td align="center"&gt;485.4&lt;/td&gt;
&lt;td align="center"&gt;-35 %&lt;/td&gt;
&lt;td align="center"&gt;1078.8&lt;/td&gt;
&lt;td align="center"&gt;1435.6&lt;/td&gt;
&lt;td align="center"&gt;33 %&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div style="margin-right: 2em; margin-left: 2em;"&gt;
&lt;p&gt;Summarizes the fault localization performance for CBSFL and SBBFL. Each fault localization technique is evaluated using both the Standard Rank Score and the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score (Defs. 28 and 25). The mean rank scores are shown as well as the percentage changes from CBSFL to SBBFL. Lower ranks scores indicate better fault localization performance. A negative percentage difference (%&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt;) indicates SBBFL &lt;strong&gt;improved&lt;/strong&gt; on CBSFL. A positive %&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt; indicates CBSFL &lt;strong&gt;outperformed&lt;/strong&gt; SBBFL.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src="images/scam/model-cmp-otto.png" title="Fig: 3" alt="Empirical probability density plots for the subject program Otto - showing the distributions of both the Standard Rank Scores and the HT_Rank Scores across all runs of all versions of Otto. The CBSFL technique is shown in blue and the SBBFL technique is shown in orange. The only suspciousness score used in this plot is RelativeF1." /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig 3:&lt;/strong&gt;
Empirical probability density plots for the subject program Otto - showing the distributions of both the Standard Rank Scores and the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Scores across all runs of all versions of Otto. The CBSFL technique is shown in blue and the SBBFL technique is shown in orange. The only suspciousness score used in this plot is RelativeF1. 
&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;img src="images/scam/cbsfl-vs-sbbfl-by-rank-score.png" alt=" Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and HT_Rank (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1. " /&gt;
&lt;img style="margin-top: -2em;" src="images/scam/cbsfl-vs-sbbfl-by-htrank.png" alt=" Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and HT_Rank (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1. " /&gt;
&lt;div style="margin-top: -2em; margin-right: 2em; margin-left: 2em;"&gt;
&lt;strong&gt;Fig 4:&lt;/strong&gt;
Comparison of CBSFL vs SBBFL average ranks (on log scale), under the Standard Rank Score (top) and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; (bottom), for each version of the program Otto; suspiciousness metric is RelativeF1.
&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;Table IV details the fault localization performance we observed for CBSFL and SBBFL under the Standard Rank Score and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; using six different suspiciousness metrics. The results obtained with &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; were substantially different from those obtained with the Standard Rank Score, in terms of absolute ranks and percentage differences (%&lt;span class="math inline"&gt;\(\Delta\)&lt;/span&gt;). The mean Standard Rank Score for SBBFL is lower than that for CBSFL for &lt;em&gt;every&lt;/em&gt; suspiciousness metric and subject program. By contrast, for some programs and metrics, the mean &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; scores for CBSFL are lower than those for SBBFL. The mean &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; scores are also higher than the mean Standard Rank Scores overall.&lt;/p&gt;
&lt;p&gt;Another way to look at the same phenomenon is shown in Figure 5, which displays empirical probability density plots for the program Otto. Each plot compares the performance of CBSFL to SBBFL using a different evaluation method. The top plot uses the Standard Rank Score while the other plot uses &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. As shown in the top plot, the Standard Rank Scores for both CBSFL and SBBFL are concentrated mainly between 0 and 15, with no values over 60. In the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; plot, by contrast, the scores for both CBSFL and SBBFL are much more widely dispersed. Figure 4 compares CBSFL to SBBFL with respect to average ranks (on a log scale), under the Standard Rank Score (top) and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; (bottom), for each version of the program Otto. The suspiciousness metric is RelativeF1. The results for &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; are quite distict from those for the Standard Rank Score, with CBSFL showing much more variability under &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These results provide confirmation for the theoretical motivation for &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. Recall that one of the problems with the Standard Rank Score is that it does not account for either the differing structures of fault localization reports or differences in report granularity. SBBFL and CBSFL differ in both structure (ranked CFG fragments vs. ranked basic blocks) and granularity (multiple basic blocks vs. lone basic blocks). We expected the Standard Rank Score to unfairly favor SBBFL because it does not account for these differences and that is exactly what we see in Table IV. Under the Standard Rank Score SBBFL outperforms CBSFL on every program using every suspiciousness score.&lt;/p&gt;
&lt;p&gt;To be explicit, SBBFL reports ranked CFG fragments, each of which contains multiple basic blocks. Under the Standard Rank Score those fragments are ranked and the score is the rank of the first fragment in which the bug appears. CBSFL will, by contrast, rank each basic block independently. Now, consider the case where SBBFL reports a CFG fragment at rank 1 that contains 10 basic blocks, one of which contains the bug. Suppose CBSFL also reports each of those basic blocks as maximally suspicious. The Standard Rank Score for CBSFL will be 5 while for SBBFL it will be 1. Thus, the Standard Rank Score is unfairly biased toward SBBFL and against CBSFL. This is once again reflected in Table IV. The new metric &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; does not suffer from this problem. As shown in the table and discussed below, SBBFL often but not always outperforms CBSFL under &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;, suggesting that the theoretical correction we expect is indeed occurring. We therefore conclude that &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; provides a better metric for comparison when reports differ in structure and granularity.&lt;/p&gt;
&lt;h2 id="rq3-which-kind-of-fault-localization-technique-performs-better-sbbfl-or-cbsfl"&gt;RQ3: Which kind of fault localization technique performs better, SBBFL or CBSFL?&lt;/h2&gt;
&lt;p&gt;Referring once again to Table IV, the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; results indicate that SBBFL often but not always outperformed CBSFL. In particular, when the measure used was Relative F1 (which was found to be the best-performing measure for SBBFL in &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;), SBBFL performed better for all programs but the compiler. However, when Ochiai was used CBSFL outperformed SBBFL, although CBSFL with Relative F1 outperforms CBSFL with Ochiai. This indicates that SBBFL and Relative F1 may be the best combination tested with one caveat. For the compiler, SBBFL never outperforms CBSFL. However, CBSFL also performs very badly on this large program. This indicates that while CBSFL beats SBBFL on this program neither technique is effective at localizing faults in it.&lt;/p&gt;
&lt;h2 id="study-limitations"&gt;Study Limitations&lt;/h2&gt;
&lt;p&gt;The purpose of our case study is to illustrate the application of &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt;. It was not designed to settle controversies about the suspiciousness metrics we employed. Second, this study used real programs and tests but used synthetic bugs. In the future, we intend to provide an automated analysis system that builds upon the Defects4J dataset &lt;span class="citation"&gt;[&lt;a href="#ref-Just2014"&gt;52&lt;/a&gt;]&lt;/span&gt;. Third, our study included a representative but not exhaustive set of suspiciousness metrics &lt;span class="citation"&gt;[&lt;a href="#ref-Lucia2014"&gt;2&lt;/a&gt;]&lt;/span&gt; and may not generalize to other metrics.&lt;/p&gt;
&lt;p&gt;Finally, no user-study was performed to validate the chosen debugging model against actions an actual programmer would take. Although we would have liked to perform such a study at this time, our resources do not permit us to do so. To be conclusive, such a study would need to be large-scale and to utilize professional programmers who are skilled in traditional debugging and are able to spend significant time learning to use SFL techniques with comparable skill.&lt;/p&gt;
&lt;h1 id="discussion"&gt;Discussion&lt;/h1&gt;
&lt;p&gt;A new, flexible approach to evaluating automatic fault localization techniques was presented. The new &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score provides a principled and flexible way of comparing different fault localization techniques. It is robust to differences in granularity and allows complex fault localization reports (such as those produced by SBBFL) to be incorporated. Unlike previous attempts at cross-technique comparison (see Section 12) the final scores are based on the expected number of steps through a Markov model. The model can incorporate both information from the chosen fault localization technique as well as other information available to the programmer (such as the structure of the program). The &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score is sensitive to the model used (see Fig 5) and hence the choice of model is important. The choice should be made based on 1) the fault localization technique(s) being evaluated and 2) observations of programmer behavior. Choosing a model after viewing the results (and picking the model that gives the "best" results) leads to a biased outcome.&lt;/p&gt;
&lt;h2 id="recommendations-for-researchers" class="unnumbered"&gt;Recommendations for Researchers&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Report both Standard Rank Score and &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If evaluating multi-line faults the Steinmann Rank Score (Def. 18) should be used as the basis for defining the &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; Score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Report which model is being used and why it was chosen, and include a description of the model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In the absence of user study data, set &lt;span class="math inline"&gt;\(p_{\text{jump}} = .5\)&lt;/span&gt; and set the weights in &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; uniformly.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By evaluating fault localization methods with &lt;span&gt;&lt;span class="math inline"&gt;\(\textrm{HT}_{\textrm{Rank}}\)&lt;/span&gt;&lt;/span&gt; in addition to the Standard Rank Score researchers will be able to make valid cross-technique comparisons. This will enable the community to better understand the relationships between techniques and report-granularity while taking into account potential programmer behavior during debugging.&lt;/p&gt;
&lt;h1 id="acknowledgement" class="unnumbered"&gt;Acknowledgement&lt;/h1&gt;
&lt;p&gt;This work was partially supported by NSF award CCF-1525178 to Case Western Reserve University.&lt;/p&gt;
&lt;h1 id="markov-chain-definitions"&gt;Markov Chain Definitions&lt;/h1&gt;
&lt;h4 id="definition.-cbsfl-ranked-list-markov-chain"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. CBSFL Ranked List Markov Chain&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;To construct a Markov chain representing a list of program locations ranked in descending order by their suspiciousness scores:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; be the set of locations in the program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For a location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; let &lt;span class="math inline"&gt;\(s(l)\)&lt;/span&gt; be its CBSFL suspiciousness score.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Partition the locations in &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; into a list of groups &lt;span class="math inline"&gt;\(G = \left\{ g_1 \subseteq L, g_2 \subseteq L, ..., g_n \subseteq L \right\}\)&lt;/span&gt; such that for each group &lt;span class="math inline"&gt;\(g_i\)&lt;/span&gt; all of the locations it contains have the same score: &lt;span class="math inline"&gt;\( \forall ~ {g_i \in G}, ~ \forall ~ {l, l&amp;#39; \in g_i} \left[
        s(l) = s(l&amp;#39;) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The score of a group &lt;span class="math inline"&gt;\(s(g_i)\)&lt;/span&gt; is defined to be the common score of its members: &lt;span class="math inline"&gt;\( \forall ~ {l \in g_i} \left[ s(g_i) = s(l) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Order &lt;span class="math inline"&gt;\(G\)&lt;/span&gt; by the scores of its groups, such that &lt;span class="math inline"&gt;\(g_0\)&lt;/span&gt; has the highest score and &lt;span class="math inline"&gt;\(g_n\)&lt;/span&gt; has the lowest: &lt;span class="math inline"&gt;\(s(g_0) &amp;gt; s(g_1) &amp;gt; ... &amp;gt; s(g_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now construct the set of states. There is one state for each group &lt;span class="math inline"&gt;\(g
      \in G\)&lt;/span&gt; and for each location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt;. &lt;span class="math display"&gt;\[S =
          \left\{ g : g \in G \right\}
          \cup
          \left\{ l : l \in L \right\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally construct the transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; for the states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt;. &lt;span class="math display"&gt;\[{\bf P}_{i,j} =
      \left\{
        \begin{array}{lll}
           1 &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in G \wedge s_i \in s_j \\
          \\
             \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
             \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in L \wedge s_j \in s_i \\
          \\
            0
            &amp;amp; \text{otherwise}
        \end{array}
      \right.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h4 id="definition.-cbsfl-with-jumps-markov-chain"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. CBSFL with Jumps Markov Chain&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;This definition augments Definition 28.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt; be a "jump" matrix representing ways a programmer might move through the program during debugging.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class="math inline"&gt;\({\bf J}_{x,y} &amp;gt; 0\)&lt;/span&gt; then locations &lt;span class="math inline"&gt;\(x,y \in L\)&lt;/span&gt; are "connected" by &lt;span class="math inline"&gt;\({\bf J}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(p_{\text{jump}}\)&lt;/span&gt; be the probability that when visiting a location &lt;span class="math inline"&gt;\(l \in L\)&lt;/span&gt; the Markov process "jumps" to another location. Let &lt;span class="math inline"&gt;\(1 -
      p_{\text{jump}}\)&lt;/span&gt; be the probability that the process returns to the state which represents its group instead of jumping. As &lt;span class="math inline"&gt;\(p_\text{{jump}}
      \rightarrow 0\)&lt;/span&gt; the behavior of the chain approaches the behavior of the chain in Definition 28.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The new transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; for the states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt; is&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class="math display"&gt;\[\begin{aligned}
      {\bf P}_{i,j} &amp;amp;=
      \left\{
        \begin{array}{cll}
            1 - p_{\text{jump}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in G \wedge s_i \in s_j \\
            &amp;amp; &amp;amp;  \wedge \left( \sum_{k}{{\bf J}_{i,k}} \right) &amp;gt; 0 \\
          \\
              p_{\text{jump}}
              \left( \frac{{\bf J}_{i,j}}{\sum_{k}{{\bf J}_{i,k}}} \right)
            &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in L \wedge {\bf J}_{i,j} &amp;gt; 0 \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{L}\right|}} - 1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{L}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in L \wedge s_j \in s_i \\
          \\
            0
            &amp;amp; \text{otherwise}
        \end{array}
      \right.
    \end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id="definition.-suspicious-behavior-markov-chain"&gt;&lt;strong&gt;Definition&lt;/strong&gt;. Suspicious Behavior Markov Chain&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;A chain that models a ranked list of suspicious subgraphs:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(H\)&lt;/span&gt; be a set of suspicious subgraphs (behaviors).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For a subgraph &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt; let &lt;span class="math inline"&gt;\(\varsigma(h)\)&lt;/span&gt; be its suspiciousness score &lt;span class="citation"&gt;[&lt;a href="#ref-Henderson2018"&gt;9&lt;/a&gt;]&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(L\)&lt;/span&gt; be the set of locations in the program.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Partition the subgraphs in &lt;span class="math inline"&gt;\(H\)&lt;/span&gt; into a list of groups &lt;span class="math inline"&gt;\(G = \left\{ g_1 \subseteq H, g_2 \subseteq H, ..., g_n \subseteq H \right\}\)&lt;/span&gt; such that for each group &lt;span class="math inline"&gt;\(g_i\)&lt;/span&gt; all of the locations in &lt;span class="math inline"&gt;\(g_i\)&lt;/span&gt; have the same score: &lt;span class="math inline"&gt;\(\forall ~ {g_i \in G} ~ \forall ~ {a, b \in g_i} \left[
        \varsigma(a) = \varsigma(b) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let the score of a group &lt;span class="math inline"&gt;\(\varsigma(g_i)\)&lt;/span&gt; be the same as the scores of its members: &lt;span class="math inline"&gt;\( \forall ~ {g_i \in G} ~ \forall ~ {h \in g_i} \left[
        \varsigma(g_i) = \varsigma(h) \right]\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Order &lt;span class="math inline"&gt;\(G\)&lt;/span&gt; by the scores of its groups, such that &lt;span class="math inline"&gt;\(g_0\)&lt;/span&gt; has the highest score and &lt;span class="math inline"&gt;\(g_n\)&lt;/span&gt; has the lowest: &lt;span class="math inline"&gt;\( \varsigma(g_0) &amp;gt; \varsigma(g_1) &amp;gt; ... &amp;gt;
      \varsigma(g_n)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now construct the set of states. One state for each group &lt;span class="math inline"&gt;\(g \in G\)&lt;/span&gt;, one state for each subgraph &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt;, and one state for each location &lt;span class="math inline"&gt;\(l
      \in V_{h}\)&lt;/span&gt; for all &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt;. &lt;span class="math display"&gt;\[S =
          \left\{ g : g \in G \right\}
          \cup
          \left\{ h : h \in H \right\}
          \cup
          \left\{ l : l \in V_h,~ \forall~ h \in H \right\}\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let &lt;span class="math inline"&gt;\(c: L \rightarrow \mathbb{N}^{+}\)&lt;/span&gt; be a function that gives the number of subgraphs &lt;span class="math inline"&gt;\(h \in H\)&lt;/span&gt; which a location &lt;span class="math inline"&gt;\(l\)&lt;/span&gt; appears in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally construct the transition matrix &lt;span class="math inline"&gt;\({\bf P}\)&lt;/span&gt; for the states &lt;span class="math inline"&gt;\(S\)&lt;/span&gt;. &lt;span class="math display"&gt;\[{\bf P}_{i,j} =
      \left\{
        \begin{array}{cll}
            \frac{1}{c(s_i)} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in L \wedge s_j \in H \wedge s_i \in V_{s_j} \\
         \\
            \frac{1}{2}\frac{1}{{{\left|{V_{s_i}}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in H \wedge s_j \in L \wedge s_j \in V_{s_i} \\
          \\
            \frac{1}{2} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in H \wedge s_j \in G \wedge s_i \in s_j \\
          \\
            \frac{{{\left|{H}\right|}} - 1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_0 \wedge s_j = s_i \\
          \\
            \frac{1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i = g_n \wedge s_j = s_i \\
          \\
            \frac{{{\left|{H}\right|}} - 1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i - 1 = s_j \\
          \\
            \frac{1}{2{{\left|{H}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in G \wedge s_i + 1 = s_j \\
          \\
            \frac{1}{2{{\left|{s_i}\right|}}} &amp;amp;
            \text{if}
            &amp;amp;
                 s_i \in G \wedge s_j \in H \wedge s_j \in s_i \\
          \\
            0
            &amp;amp; \text{otherwise}
        \end{array}
      \right.\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;div id="refs" class="references"&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id="ref-Jones2002"&gt;
&lt;p&gt;[1] J. Jones, M. Harrold, and J. Stasko, "Visualization of test information to assist fault localization," &lt;em&gt;Proceedings of the 24th International Conference on Software Engineering. ICSE 2002&lt;/em&gt;, 2002, doi:&lt;a href="https://doi.org/10.1145/581339.581397"&gt;10.1145/581339.581397&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Lucia2014"&gt;
&lt;p&gt;[2] Lucia, D. Lo, L. Jiang, F. Thung, and A. Budi, "Extended comprehensive study of association measures for fault localization," &lt;em&gt;Journal of Software: Evolution and Process&lt;/em&gt;, vol. 26, no. 2, pp. 172-219, Feb. 2014, doi:&lt;a href="https://doi.org/10.1002/smr.1616"&gt;10.1002/smr.1616&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Sun2016"&gt;
&lt;p&gt;[3] S.-F. Sun and A. Podgurski, "Properties of Effective Metrics for Coverage-Based Statistical Fault Localization," in &lt;em&gt;2016 ieee international conference on software testing, verification and validation (icst)&lt;/em&gt;, 2016, pp. 124-134, doi:&lt;a href="https://doi.org/10.1109/ICST.2016.31"&gt;10.1109/ICST.2016.31&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Jones2005"&gt;
&lt;p&gt;[4] J. A. Jones and M. J. Harrold, "Empirical Evaluation of the Tarantula Automatic Fault-localization Technique," in &lt;em&gt;Proceedings of the 20th ieee/acm international conference on automated software engineering&lt;/em&gt;, 2005, pp. 273-282, doi:&lt;a href="https://doi.org/10.1145/1101908.1101949"&gt;10.1145/1101908.1101949&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Pearson2017"&gt;
&lt;p&gt;[5] S. Pearson, J. Campos, R. Just, G. Fraser, R. Abreu, M. D. Ernst, D. Pang, and B. Keller, "Evaluating and Improving Fault Localization," in &lt;em&gt;Proceedings of the 39th international conference on software engineering&lt;/em&gt;, 2017, pp. 609-620, doi:&lt;a href="https://doi.org/10.1109/ICSE.2017.62"&gt;10.1109/ICSE.2017.62&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Parnin2011"&gt;
&lt;p&gt;[6] C. Parnin and A. Orso, "Are Automated Debugging Techniques Actually Helping Programmers?" in &lt;em&gt;ISSTA&lt;/em&gt;, 2011, pp. 199-209.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Renieres2003"&gt;
&lt;p&gt;[7] M. Renieres and S. Reiss, "Fault localization with nearest neighbor queries," in &lt;em&gt;18th ieee international conference on automated software engineering, 2003. proceedings.&lt;/em&gt;, 2003, pp. 30-39, doi:&lt;a href="https://doi.org/10.1109/ASE.2003.1240292"&gt;10.1109/ASE.2003.1240292&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Cheng2009a"&gt;
&lt;p&gt;[8] H. Cheng, D. Lo, Y. Zhou, X. Wang, and X. Yan, "Identifying Bug Signatures Using Discriminative Graph Mining," in &lt;em&gt;Proceedings of the eighteenth international symposium on software testing and analysis&lt;/em&gt;, 2009, pp. 141-152, doi:&lt;a href="https://doi.org/10.1145/1572272.1572290"&gt;10.1145/1572272.1572290&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Henderson2018"&gt;
&lt;p&gt;[9] T. A. D. Henderson and A. Podgurski, "Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow Subgraphs," in &lt;em&gt;IEEE conference on software testing, validation and verification&lt;/em&gt;, 2018.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Baah2010"&gt;
&lt;p&gt;[10] G. G. K. Baah, A. Podgurski, and M. J. M. Harrold, "Causal inference for statistical fault localization," in &lt;em&gt;Proceedings of the 19th international symposium on software testing and analysis&lt;/em&gt;, 2010, pp. 73-84, doi:&lt;a href="https://doi.org/10.1145/1831708.1831717"&gt;10.1145/1831708.1831717&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Aggarwal2014"&gt;
&lt;p&gt;[11] C. C. Aggarwal and J. Han, Eds., &lt;em&gt;Frequent Pattern Mining&lt;/em&gt;. Cham: Springer International Publishing, 2014.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Agrawal1993"&gt;
&lt;p&gt;[12] R. Agrawal, T. Imieliński, and A. Swami, "Mining association rules between sets of items in large databases," &lt;em&gt;ACM SIGMOD Record&lt;/em&gt;, vol. 22, no. 2, pp. 207-216, Jun. 1993, doi:&lt;a href="https://doi.org/10.1145/170036.170072"&gt;10.1145/170036.170072&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Yan2002"&gt;
&lt;p&gt;[13] X. Yan and J. Han, "gSpan: graph-based substructure pattern mining," in &lt;em&gt;2002 ieee international conference on data mining, 2002. proceedings.&lt;/em&gt;, 2002, pp. 721-724, doi:&lt;a href="https://doi.org/10.1109/ICDM.2002.1184038"&gt;10.1109/ICDM.2002.1184038&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Aggarwal2014a"&gt;
&lt;p&gt;[14] C. C. Aggarwal, M. A. Bhuiyan, and M. A. Hasan, "Frequent Pattern Mining Algorithms: A Survey," in &lt;em&gt;Frequent pattern mining&lt;/em&gt;, Cham: Springer International Publishing, 2014, pp. 19-64.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Yan2008"&gt;
&lt;p&gt;[15] X. Yan, H. Cheng, J. Han, and P. S. Yu, "Mining Significant Graph Patterns by Leap Search," in &lt;em&gt;Proceedings of the 2008 acm sigmod international conference on management of data&lt;/em&gt;, 2008, pp. 433-444, doi:&lt;a href="https://doi.org/10.1145/1376616.1376662"&gt;10.1145/1376616.1376662&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Abreu2006"&gt;
&lt;p&gt;[16] R. Abreu, P. Zoeteweij, and A. Van Gemund, "An Evaluation of Similarity Coefficients for Software Fault Localization," in &lt;em&gt;2006 12th pacific rim international symposium on dependable computing (prdc'06)&lt;/em&gt;, 2006, pp. 39-46, doi:&lt;a href="https://doi.org/10.1109/PRDC.2006.18"&gt;10.1109/PRDC.2006.18&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Abreu2009"&gt;
&lt;p&gt;[17] R. Abreu, P. Zoeteweij, R. Golsteijn, and A. J. C. van Gemund, "A practical evaluation of spectrum-based fault localization," &lt;em&gt;Journal of Systems and Software&lt;/em&gt;, vol. 82, no. 11, pp. 1780-1792, 2009, doi:&lt;a href="https://doi.org/10.1016/j.jss.2009.06.035"&gt;10.1016/j.jss.2009.06.035&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Agarwal2014"&gt;
&lt;p&gt;[18] P. Agarwal and A. P. Agrawal, "Fault-localization Techniques for Software Systems: A Literature Review," &lt;em&gt;SIGSOFT Softw. Eng. Notes&lt;/em&gt;, vol. 39, no. 5, pp. 1-8, Sep. 2014, doi:&lt;a href="https://doi.org/10.1145/2659118.2659125"&gt;10.1145/2659118.2659125&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Wong2016"&gt;
&lt;p&gt;[19] W. E. Wong, R. Gao, Y. Li, R. Abreu, and F. Wotawa, "A Survey on Software Fault Localization," &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt;, vol. 42, no. 8, pp. 707-740, Aug. 2016, doi:&lt;a href="https://doi.org/10.1109/TSE.2016.2521368"&gt;10.1109/TSE.2016.2521368&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Zeller1999"&gt;
&lt;p&gt;[20] A. Zeller, "Yesterday, My Program Worked. Today, It Does Not. Why?" &lt;em&gt;SIGSOFT Softw. Eng. Notes&lt;/em&gt;, vol. 24, no. 6, pp. 253-267, Oct. 1999, doi:&lt;a href="https://doi.org/10.1145/318774.318946"&gt;10.1145/318774.318946&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Tip1995"&gt;
&lt;p&gt;[21] F. Tip, "A survey of program slicing techniques," &lt;em&gt;Journal of programming languages&lt;/em&gt;, vol. 3, no. 3, pp. 121-189, 1995.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Mao2014"&gt;
&lt;p&gt;[22] X. Mao, Y. Lei, Z. Dai, Y. Qi, and C. Wang, "Slice-based statistical fault localization," &lt;em&gt;Journal of Systems and Software&lt;/em&gt;, vol. 89, no. 1, pp. 51-62, 2014, doi:&lt;a href="https://doi.org/10.1016/j.jss.2013.08.031"&gt;10.1016/j.jss.2013.08.031&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Marcus2004"&gt;
&lt;p&gt;[23] A. Marcus, A. Sergeyev, V. Rajlieh, and J. I. Maletic, "An information retrieval approach to concept location in source code," &lt;em&gt;Proceedings - Working Conference on Reverse Engineering, WCRE&lt;/em&gt;, pp. 214-223, 2004, doi:&lt;a href="https://doi.org/10.1109/WCRE.2004.10"&gt;10.1109/WCRE.2004.10&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Zhou2012"&gt;
&lt;p&gt;[24] J. Zhou, H. Zhang, and D. Lo, "Where should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports," &lt;em&gt;Proceedings - International Conference on Software Engineering&lt;/em&gt;, pp. 14-24, 2012, doi:&lt;a href="https://doi.org/10.1109/ICSE.2012.6227210"&gt;10.1109/ICSE.2012.6227210&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Le2015"&gt;
&lt;p&gt;[25] T.-D. B. Le, R. J. Oentaryo, and D. Lo, "Information retrieval and spectrum based bug localization: better together," in &lt;em&gt;Proceedings of the 2015 10th joint meeting on foundations of software engineering - esec/fse 2015&lt;/em&gt;, 2015, pp. 579-590, doi:&lt;a href="https://doi.org/10.1145/2786805.2786880"&gt;10.1145/2786805.2786880&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Artzi2010"&gt;
&lt;p&gt;[26] S. Artzi, J. Dolby, F. Tip, and M. Pistoia, "Directed Test Generation for Effective Fault Localization," in &lt;em&gt;Proceedings of the 19th international symposium on software testing and analysis&lt;/em&gt;, 2010, pp. 49-60, doi:&lt;a href="https://doi.org/10.1145/1831708.1831715"&gt;10.1145/1831708.1831715&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Sahoo2013"&gt;
&lt;p&gt;[27] S. K. Sahoo, J. Criswell, C. Geigle, and V. Adve, "Using likely invariants for automated software fault localization," in &lt;em&gt;Proceedings of the eighteenth international conference on architectural support for programming languages and operating systems&lt;/em&gt;, 2013, vol. 41, p. 139, doi:&lt;a href="https://doi.org/10.1145/2451116.2451131"&gt;10.1145/2451116.2451131&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Perez2014"&gt;
&lt;p&gt;[28] A. Perez, R. Abreu, and A. Riboira, "A Dynamic Code Coverage Approach to Maximize Fault Localization Efficiency," &lt;em&gt;J. Syst. Softw.&lt;/em&gt;, vol. 90, pp. 18-28, Apr. 2014, doi:&lt;a href="https://doi.org/10.1016/j.jss.2013.12.036"&gt;10.1016/j.jss.2013.12.036&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Agrawal1995"&gt;
&lt;p&gt;[29] H. Agrawal, J. Horgan, S. London, and W. Wong, "Fault localization using execution slices and dataflow tests," in &lt;em&gt;Proceedings of sixth international symposium on software reliability engineering. issre'95&lt;/em&gt;, 1995, pp. 143-151, doi:&lt;a href="https://doi.org/10.1109/ISSRE.1995.497652"&gt;10.1109/ISSRE.1995.497652&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Cleve2005"&gt;
&lt;p&gt;[30] H. Cleve and A. Zeller, "Locating causes of program failures," &lt;em&gt;Proceedings of the 27th international conference on Software engineering - ICSE '05&lt;/em&gt;, p. 342, 2005, doi:&lt;a href="https://doi.org/10.1145/1062455.1062522"&gt;10.1145/1062455.1062522&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Horwitz1990"&gt;
&lt;p&gt;[31] S. Horwitz, "Identifying the Semantic and Textual Differences Between Two Versions of a Program," &lt;em&gt;SIGPLAN Not.&lt;/em&gt;, vol. 25, no. 6, pp. 234-245, Jun. 1990, doi:&lt;a href="https://doi.org/10.1145/93548.93574"&gt;10.1145/93548.93574&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Wong2008"&gt;
&lt;p&gt;[32] E. Wong, T. Wei, Y. Qi, and L. Zhao, "A Crosstab-based Statistical Method for Effective Fault Localization," in &lt;em&gt;2008 international conference on software testing, verification, and validation&lt;/em&gt;, 2008, pp. 42-51, doi:&lt;a href="https://doi.org/10.1109/ICST.2008.65"&gt;10.1109/ICST.2008.65&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Landsberg2015"&gt;
&lt;p&gt;[33] D. Landsberg, H. Chockler, D. Kroening, and M. Lewis, "Evaluation of Measures for Statistical Fault Localisation and an Optimising Scheme," in &lt;em&gt;International conference on fundamental approaches to software engineering&lt;/em&gt;, 2015, vol. 9033, pp. 115-129, doi:&lt;a href="https://doi.org/10.1007/978-3-662-46675-9"&gt;10.1007/978-3-662-46675-9&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Zheng2018"&gt;
&lt;p&gt;[34] Y. Zheng, Z. Wang, X. Fan, X. Chen, and Z. Yang, "Localizing multiple software faults based on evolution algorithm," &lt;em&gt;Journal of Systems and Software&lt;/em&gt;, vol. 139, pp. 107-123, 2018, doi:&lt;a href="https://doi.org/10.1016/j.jss.2018.02.001"&gt;10.1016/j.jss.2018.02.001&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-ChaoLiu2006"&gt;
&lt;p&gt;[35] C. Liu, L. Fei, X. Yan, J. Han, and S. P. Midkiff, "Statistical debugging: A hypothesis testing-based approach," &lt;em&gt;IEEE Transactions on Software Engineering&lt;/em&gt;, vol. 32, no. 10, pp. 831-847, Oct. 2006, doi:&lt;a href="https://doi.org/10.1109/TSE.2006.105"&gt;10.1109/TSE.2006.105&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Ferrante1987"&gt;
&lt;p&gt;[36] J. Ferrante, K. J. Ottenstein, and J. D. Warren, "The program dependence graph and its use in optimization," vol. 9. pp. 319-349, Jul-1987.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Ali2009"&gt;
&lt;p&gt;[37] S. Ali, J. H. Andrews, T. Dhandapani, and W. Wang, "Evaluating the Accuracy of Fault Localization Techniques," &lt;em&gt;2009 IEEE/ACM International Conference on Automated Software Engineering&lt;/em&gt;, pp. 76-87, 2009, doi:&lt;a href="https://doi.org/10.1109/ASE.2009.89"&gt;10.1109/ASE.2009.89&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Moon2014"&gt;
&lt;p&gt;[38] S. Moon, Y. Kim, M. Kim, and S. Yoo, "Ask the Mutants: Mutating faulty programs for fault localization," &lt;em&gt;Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation, ICST 2014&lt;/em&gt;, pp. 153-162, 2014, doi:&lt;a href="https://doi.org/10.1109/ICST.2014.28"&gt;10.1109/ICST.2014.28&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Liu2005"&gt;
&lt;p&gt;[39] C. Liu, H. Yu, P. S. Yu, X. Yan, H. Yu, J. Han, and P. S. Yu, "Mining Behavior Graphs for ‘Backtrace' of Noncrashing Bugs," in &lt;em&gt;Proceedings of the 2005 siam international conference on data mining&lt;/em&gt;, 2005, pp. 286-297, doi:&lt;a href="https://doi.org/10.1137/1.9781611972757.26"&gt;10.1137/1.9781611972757.26&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-DiFatta2006"&gt;
&lt;p&gt;[40] G. Di Fatta, S. Leue, and E. Stegantova, "Discriminative Pattern Mining in Software Fault Detection," in &lt;em&gt;Proceedings of the 3rd international workshop on software quality assurance&lt;/em&gt;, 2006, pp. 62-69, doi:&lt;a href="https://doi.org/10.1145/1188895.1188910"&gt;10.1145/1188895.1188910&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Eichinger2008"&gt;
&lt;p&gt;[41] F. Eichinger, K. Böhm, and M. Huber, "Mining Edge-Weighted Call Graphs to Localise Software Bugs," in &lt;em&gt;European conference machine learning and knowledge discovery in databases&lt;/em&gt;, 2008, pp. 333-348, doi:&lt;a href="https://doi.org/10.1007/978-3-540-87479-9_40"&gt;10.1007/978-3-540-87479-9_40&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Eichinger2010"&gt;
&lt;p&gt;[42] F. Eichinger, K. Krogmann, R. Klug, and K. Böhm, "Software-defect Localisation by Mining Dataflow-enabled Call Graphs," in &lt;em&gt;Proceedings of the 2010 european conference on machine learning and knowledge discovery in databases: Part i&lt;/em&gt;, 2010, pp. 425-441.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Mousavian2011"&gt;
&lt;p&gt;[43] Z. Mousavian, M. Vahidi-Asl, and S. Parsa, "Scalable Graph Analyzing Approach for Software Fault-localization," in &lt;em&gt;Proceedings of the 6th international workshop on automation of software test&lt;/em&gt;, 2011, pp. 15-21, doi:&lt;a href="https://doi.org/10.1145/1982595.1982599"&gt;10.1145/1982595.1982599&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Eichinger2011"&gt;
&lt;p&gt;[44] F. Eichinger, C. Oßner, and K. Böhm, "Scalable software-defect localisation by hierarchical mining of dynamic call graphs," &lt;em&gt;Proceedings of the 11th SIAM International Conference on Data Mining, SDM 2011&lt;/em&gt;, no. c, pp. 723-734, 2011.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Parsa2011"&gt;
&lt;p&gt;[45] S. Parsa, S. A. Naree, and N. E. Koopaei, "Software Fault Localization via Mining Execution Graphs," in &lt;em&gt;Proceedings of the 2011 international conference on computational science and its applications - volume part ii&lt;/em&gt;, 2011, pp. 610-623.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Mariani2011"&gt;
&lt;p&gt;[46] L. Mariani, F. Pastore, and M. Pezze, "Dynamic Analysis for Diagnosing Integration Faults," &lt;em&gt;IEEE Trans. Softw. Eng.&lt;/em&gt;, vol. 37, no. 4, pp. 486-508, Jul. 2011, doi:&lt;a href="https://doi.org/10.1109/TSE.2010.93"&gt;10.1109/TSE.2010.93&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Yousefi2013"&gt;
&lt;p&gt;[47] A. Yousefi and A. Wassyng, "A Call Graph Mining and Matching Based Defect Localization Technique," in &lt;em&gt;2013 ieee sixth international conference on software testing, verification and validation workshops&lt;/em&gt;, 2013, pp. 86-95, doi:&lt;a href="https://doi.org/10.1109/ICSTW.2013.17"&gt;10.1109/ICSTW.2013.17&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Ioannidis2005"&gt;
&lt;p&gt;[48] J. P. A. Ioannidis, "Why most published research findings are false," &lt;em&gt;PLoS Medicine&lt;/em&gt;, vol. 2, no. 8, pp. 0696-0701, 2005, doi:&lt;a href="https://doi.org/10.1371/journal.pmed.0020124"&gt;10.1371/journal.pmed.0020124&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Grinstead2012"&gt;
&lt;p&gt;[49] C. M. Grinstead and J. L. Snell, &lt;em&gt;Introduction to Probability&lt;/em&gt;, 2nd ed. Providence, RI: American Mathematical Society, 1997.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Kemeny1960"&gt;
&lt;p&gt;[50] J. G. Kemeny and J. L. Snell, &lt;em&gt;Finite Markov Chains&lt;/em&gt;, First. Princeton, NJ: Van Nostrand, 1960.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Davis2004"&gt;
&lt;p&gt;[51] T. A. Davis, "Algorithm 832: UMFPACK V4.3—an Unsymmetric-pattern Multifrontal Method," &lt;em&gt;ACM Trans. Math. Softw.&lt;/em&gt;, vol. 30, no. 2, pp. 196-199, Jun. 2004, doi:&lt;a href="https://doi.org/10.1145/992200.992206"&gt;10.1145/992200.992206&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Just2014"&gt;
&lt;p&gt;[52] R. Just, D. Jalali, and M. D. Ernst, "Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs," in &lt;em&gt;Proceedings of the 2014 international symposium on software testing and analysis&lt;/em&gt;, 2014, pp. 437-440, doi:&lt;a href="https://doi.org/10.1145/2610384.2628055"&gt;10.1145/2610384.2628055&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id="ref-Steimann2013"&gt;
&lt;p&gt;[53] F. Steimann, M. Frenkel, and R. Abreu, "Threats to the validity and value of empirical assessments of the accuracy of coverage-based fault locators," &lt;em&gt;Proceedings of the 2013 International Symposium on Software Testing and Analysis - ISSTA 2013&lt;/em&gt;, p. 314, 2013, doi:&lt;a href="https://doi.org/10.1145/2483760.2483767"&gt;10.1145/2483760.2483767&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="footnotes"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;Jones &lt;em&gt;et al.&lt;/em&gt; did not use the term "suspiciousness score" or "suspiciousness metric" in their 2002 paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2002"&gt;1&lt;/a&gt;]&lt;/span&gt;. They introduced the term "suspiciousness score" in their 2005 paper &lt;span class="citation"&gt;[&lt;a href="#ref-Jones2005"&gt;4&lt;/a&gt;]&lt;/span&gt;, in the context of ranking statements. Both terms are now in common use.&lt;a href="#fnref1"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;A dependence sphere is computed from the Program Dependence Graph (PDG) &lt;span class="citation"&gt;[&lt;a href="#ref-Horwitz1990"&gt;31&lt;/a&gt;], [&lt;a href="#ref-Ferrante1987"&gt;36&lt;/a&gt;]&lt;/span&gt;. In a PDG, program elements are nodes and their &lt;em&gt;control&lt;/em&gt; and &lt;em&gt;data&lt;/em&gt; dependencies are represented as edges. Given two nodes &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt; a graph with a shortest path (ignoring edge directionality) &lt;span class="math inline"&gt;\(\pi\)&lt;/span&gt; between them, the sphere is all those nodes in the graph have have a path from &lt;span class="math inline"&gt;\(\alpha\)&lt;/span&gt; as short (or shorter) than &lt;span class="math inline"&gt;\(\pi\)&lt;/span&gt; (once again ignoring edge directionality).&lt;a href="#fnref2"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Implementations of these two computations maybe found in &lt;span&gt;&lt;code&gt;https://github.com/timtadh/expected-hitting-times&lt;/code&gt;&lt;/span&gt;.&lt;a href="#fnref3"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;See https://hackthology.com/pdfs/scam-2019-supplement.pdf&lt;a href="#fnref4"&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Paper"></category></entry><entry><title>Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow Subgraphs</title><link href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html" rel="alternate"></link><published>2018-04-03T00:00:00-04:00</published><updated>2018-04-03T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt; and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2018-04-03:/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html</id><summary type="html">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow
Subgraphs&lt;/em&gt;.  &lt;a href="http://www.es.mdh.se/icst2018/"&gt;ICST 2018&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1109/ICST.2018.00019"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2018.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present a new algorithm, Score Weighted Random Walks (SWRW), for behavioral
fault localization. Behavioral fault localization localizes faults (bugs) in
programs to a group of interacting …&lt;/p&gt;</summary><content type="html">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow
Subgraphs&lt;/em&gt;.  &lt;a href="http://www.es.mdh.se/icst2018/"&gt;ICST 2018&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1109/ICST.2018.00019"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/icst-2018.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/behavioral-fault-localization-by-sampling-suspicious-dynamic-control-flow-subgraphs.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present a new algorithm, Score Weighted Random Walks (SWRW), for behavioral
fault localization. Behavioral fault localization localizes faults (bugs) in
programs to a group of interacting program elements such as basic blocks or
functions.  SWRW samples suspicious (or discriminative) subgraphs from
basic-block level dynamic control flow graphs collected during the execution of
passing and failing tests.  The suspiciousness of a subgraph may be measured by
any one of a family of new metrics adapted from probabilistic formulations of
existing coverage-based statistical fault localization metrics.  We conducted an
empirical evaluation of SWRW with nine subgraph-suspiciousness measures on five
real-world subject programs.  The results indicate that SWRW outperforms
previous fault localization techniques based on discriminative subgraph mining.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Automated fault localization&lt;/em&gt; techniques have been developed to help
programmers locate software faults (bugs) responsible for observed software
failures.  Many of these techniques are statistical in nature (e.g., {&lt;a href="https://dx.doi.org/10.1145/581339.581397"&gt;Jones
2002&lt;/a&gt;, &lt;a href="https://dx.doi.org/10.1145/1064978.1065014"&gt;Liblit
2005&lt;/a&gt;, &lt;a href="https://dx.doi.org/10.1137/1.9781611972757.26"&gt;Liu
2005&lt;/a&gt;}).  They employ statistical
measures of the &lt;em&gt;association&lt;/em&gt;, if any, between the occurrence of failures and
the execution of particular program elements like statements or conditional
branches.  The program elements that are most strongly associated with failures
are identified as "suspicious", so that developers can examine them to see if
they are faulty.  The association measures that are used are often called
&lt;em&gt;suspiciousness metrics&lt;/em&gt; {&lt;a href="https://dx.doi.org/10.1109/ICSE.2004.1317420"&gt;Jones
2004&lt;/a&gt;}.  Such &lt;em&gt;statistical fault
localization&lt;/em&gt; (SFL) techniques typically require &lt;em&gt;execution profiles&lt;/em&gt; (or
&lt;em&gt;spectra&lt;/em&gt;) and PASS/FAIL labels for a set of both passing and failing program
runs.  Each profile entry characterizes the execution of a
particular program element during a run.  For example, a statement-coverage
profile for a run indicates which statements were executed at least once.  The
profiles are collected with program instrumentation, while the labels are
typically supplied by software testers or end users.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Coverage Based Statistical Fault Localization&lt;/em&gt; (CBSFL) techniques compute a
suspiciousness score for each program element. The elements are presented to the
programmer as a ranked list with the most suspicious elements at the top
{&lt;a href="https://dx.doi.org/10.1145/1064978.1065014"&gt;Liblit 2005&lt;/a&gt;}.  Programmers
utilize the list to guide their debugging effort by starting at the most
suspicious element and moving down. The effectiveness of a CBSFL suspiciousness
metric is judged by how accurately it ranks a bug's (or bugs) location in a
program. The higher in the list the bug's location appears the better the metric
performs {&lt;a href="https://dx.doi.org/10.1002/smr.1616"&gt;Lucia 2014&lt;/a&gt;}.&lt;/p&gt;
&lt;p&gt;Kochhar &lt;em&gt;et al.&lt;/em&gt;  {&lt;a href="https://dx.doi.org/10.1145/2931037.2931051"&gt;Kochhar 2016&lt;/a&gt;}
recently surveyed 386 software engineering practitioners about their
expectations for automated fault localization. While practitioners indicated
their preference for very accurate algorithms, over 85% of respondents also
indicated their preference for tools which help them understand the output of
fault localization algorithms. This is an important finding as most statistical
approaches do not provide an explanation of their results. The SFL techniques
often simply compute suspiciousness measures and rank the program elements
accordingly.  These rankings may be helpful, but without more information
programmers could overlook the faulty element even when it is ranked highly.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Localization Process" src="/images/icst-localization-process.png"&gt;&lt;/p&gt;
&lt;div style="text-align: center; margin-top: -2em;"&gt;
&lt;strong&gt;Figure 1.&lt;/strong&gt; Process for localizing faults  with discriminative graph mining.
&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Suspicious-Behavior Based Fault Localization&lt;/em&gt; (SBBFL) is a statistical fault
localization technique that aids the programmer in understanding suspiciousness
scores by providing a &lt;em&gt;context&lt;/em&gt; of interacting elements.
&lt;span style="color: gray"&gt;(&lt;em&gt;NB: Dynamic slicing {&lt;a href="https://www.franktip.org/pubs/jpl1995.pdf"&gt;Tip
1995&lt;/a&gt;} also provides such a context, but does
not in itself involve suspiciousness measures&lt;/em&gt;)&lt;/span&gt; Instead of implicating a
single element, SBBFL implicates a larger runtime behavior (see process in
Figure 1). The implicated control flow paths (or subgraphs) may help the
programmer understand the nature of a bug {&lt;a href="https://dx.doi.org/10.1145/1572272.1572290"&gt;Cheng
2009&lt;/a&gt;}.&lt;/p&gt;
&lt;p&gt;We present a new algorithm, &lt;em&gt;Score-Weighted Random Walks&lt;/em&gt; (SWRW), for
behavioral fault-localization.  SWRW belongs to a family of
&lt;em&gt;discriminative graph-mining algorithms&lt;/em&gt;  that have previously been used
for behavioral fault localization 
{
&lt;a href="https://dx.doi.org/10.1137/1.9781611972757.26"&gt;Liu 2005&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1145/1188895.1188910"&gt;DiFatta 2006&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1007/978-3-540-87479-9_40"&gt;Eichinger 2008&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1145/1572272.1572290"&gt;Cheng 2009&lt;/a&gt;,
&lt;a href="http://dl.acm.org/citation.cfm?id=1888258.1888293"&gt;Eichinger 2010&lt;/a&gt;,
&lt;a href="http://www.scopus.com/inward/record.url?eid=2-s2.0-84880082474&amp;amp;partnerID=tZOtx3y1"&gt;Eichinger 2011&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1145/1982595.1982599"&gt;Mousavian 2011&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1109/TSE.2010.93"&gt;Mariani 2011&lt;/a&gt;,
&lt;a href="http://dl.acm.org/citation.cfm?id=2029256.2029305"&gt;Parsa 2011&lt;/a&gt;,
&lt;a href="https://dx.doi.org/10.1109/ICSTW.2013.17"&gt;Yousefi 2013&lt;/a&gt;
}. Graph mining is very powerful in principle but algorithms must
make trade-offs to address the challenging combinatorics of the graph mining
problem.  Our new algorithm, SWRW, mitigates the combinatorics by randomly
sampling "suspicious" subgraphs from dynamic control flow graphs.  During the
sampling process, the most suspicious subgraphs (as judged by a suitable
suspiciousness metric) are favored for selection. Unlike previous algorithms,
SWRW can be used with a wide variety of suspiciousness metrics --- which allows
it to use better metrics than available to previous work.  Even when using the
same metric as similar algorithms, SWRW localizes faults more accurately than
they do.&lt;/p&gt;
&lt;h2&gt;Summary of Contributions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A new behavioral fault localization algorithm, SWRW, that samples suspicious
   subgraphs from dynamic control flow graphs.  Unlike similar algorithms, SWRW
   can be used with a variety of suspiciousness metrics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New generalizations of existing suspiciousness metrics that allow them to be
   applied to behaviors represented by subgraphs of dynamic control flow graphs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An empirical study whose results suggest that SWRW is more accurate than
   similar algorithms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dynagrok, a new instrumentation, mutation, and analysis tool for the Go
   programming language.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Dynagrok: A New Profiling Tool&lt;/h1&gt;
&lt;p&gt;All &lt;em&gt;Coverage-Based Statistical Fault Localization&lt;/em&gt; (CBSFL) techniques use
&lt;em&gt;coverage profiles&lt;/em&gt; to gather information on how software behaved when
executed on a set of test inputs. A coverage profile typically contains an entry
for each program element of a given kind (e.g., statement, basic block, branch,
or function), which records whether (and possibly how many times) the element
was executed during the corresponding program run.  The profiles and PASS/FAIL
labels for all tests are then used to compute a statistical
&lt;em&gt;suspiciousness score&lt;/em&gt; for each program element.&lt;/p&gt;
&lt;p&gt;The process of gathering the coverage information from running programs is
called &lt;em&gt;profiling&lt;/em&gt; and there are many different varieties of profilers and
profiling techniques available. Coverage profiling is a simple and widely
implemented technique, which is why it has been widely used by the fault
localization community. Another technique is &lt;em&gt;tracing&lt;/em&gt;, which logs the sequence
of program locations as they are executed. The traces provide detailed
information on the behavior of the program but could grow to be very large for
long running programs. This paper uses &lt;em&gt;execution flow profiling&lt;/em&gt; which
computes the dynamic interprocedural control flow graph of a program's
execution.  This provides some of the benefits of tracing without recording an
excessive amount of data.&lt;/p&gt;
&lt;p&gt;To capture execution flow profiles we developed
&lt;a href="https://github.com/timtadh/dynagrok"&gt;Dynagrok&lt;/a&gt;, a new analysis, instrumentation
and mutation platform for the Go programming language. Go is a newer language
(2009) from Google that has been seeing increasing adoption in industry. It has
been adopted for web programming, systems programming, "DevOps," network
programing, and databases {&lt;a href="http://tiobe.com/tiobe-index/"&gt;http://tiobe.com/tiobe-index/&lt;/a&gt;,
&lt;a href="http://blog.golang.org/survey2017-results"&gt;http://blog.golang.org/survey2017-results&lt;/a&gt;}.  Dynagrok builds upon the
&lt;em&gt;abstract syntax tree&lt;/em&gt; (AST) representation provided by the Go standard
library.&lt;/p&gt;
&lt;p&gt;
Dynagrok collects profiles by inserting instrumentation into the AST of the
subject program.  The profiles currently collected are &lt;em&gt;dynamic control flow
graphs&lt;/em&gt; (DCFGs) whose vertices represent basic blocks. A &lt;em&gt;basic
block&lt;/em&gt; is a sequence of program operations that can only be entered at the
start of the sequence and can only be exited after the last operation in the
sequence 
{&lt;a href="https://www.worldcat.org/title/compilers-principles-techniques-and-tools/oclc/12285707"&gt;Aho 2007&lt;/a&gt;}.
A basic-block level &lt;em&gt;control flow graph&lt;/em&gt; (CFG) is a directed labeled
graph &lt;span class="math"&gt;\(g = (V, E, l)\)&lt;/span&gt; comprised of a finite set of
vertices &lt;span class="math"&gt;\(V\)&lt;/span&gt;, a set of edges &lt;span class="math"&gt;\(E
\subseteq V \times V\)&lt;/span&gt;, and a labeling function &lt;span
class="math"&gt;\(l\)&lt;/span&gt; mapping vertices and edges to labels.  Each vertex
&lt;span class="math"&gt;\(v \in V\)&lt;/span&gt; represents a basic block of the program.
Each edge &lt;span class="math"&gt;\((u, v) \in E\)&lt;/span&gt; represents a transition in
program execution from block &lt;span class="math"&gt;\(u\)&lt;/span&gt; to block &lt;span
class="math"&gt;\(v\)&lt;/span&gt;. The labeling function &lt;span class="math"&gt;\(l\)&lt;/span&gt;
labels the basic blocks with a unique identifier (e.g.
&lt;code&gt;function-name:block-id&lt;/code&gt;), which is consistently applied across
multiple executions but is never repeated in the same execution.
&lt;/p&gt;

&lt;p&gt;Figure 2 shows an example DCFG collected by Dynagrok for a simple program that
computes terms of the Fibonacci sequence. To collect such graphs Dynagrok parses
the program into an AST using Go's standard library.  Dynagrok then uses a
custom control flow analysis to build static control flow graphs. Each basic
block holds pointers to the statements inside of the AST. The blocks also have a
pointer to the enclosing &lt;em&gt;lexical block&lt;/em&gt; in the AST.  Using this information,
Dynagrok inserts profiling instructions into the AST at the beginning of each
basic block. The instructions inserted by Dynagrok use its &lt;code&gt;dgruntime&lt;/code&gt; library
to track the control flow of each thread (which is called a &lt;em&gt;goroutine&lt;/em&gt; in Go).
When the program shuts down (either normally or abnormally) the &lt;code&gt;dgruntime&lt;/code&gt;
library merges the flow graphs from all the threads together and writes out the
result.&lt;/p&gt;
&lt;p&gt;(&lt;a href="https://hackthology.com/pdfs/icst-2018.pdf"&gt;Read the rest of the paper as a pdf&lt;/a&gt;)&lt;/p&gt;
&lt;div style="text-align: center;"&gt;
&lt;img src="/images/icst-dcfg.png"
     text="Example Dynamic Control Flow Graph"
     style="width: 65%;"/&gt;
&lt;/div&gt;
&lt;div style="text-align: center; margin-top: -1em;"&gt;
&lt;strong&gt;Figure 2.&lt;/strong&gt; 
The dynamic control flow graph (DCFG) for the program &lt;br&gt;
in  Listing 1 (see below).
&lt;/div&gt;
&lt;div style="width: 90%; left:05%; position: relative;"&gt;
Each vertex is a basic block with a basic block identifer (e.g.
&lt;code&gt;b1&lt;/code&gt;) that, in conjunction with the name of the containing function,
serves as the label for the block (e.g. &lt;code&gt;main:b1&lt;/code&gt;). Each edge shows
the number of traversals taken during the execution of the program.  Note that
the loop update blocks (&lt;code&gt;main:b3&lt;/code&gt; and &lt;code&gt;fib:b7&lt;/code&gt;) will not
be in the profiles because Dynagrok instruments the Go source code and profiling
instructions cannot be syntactically inserted in those locations.  The
instrumented program is shown on the right.
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nb"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div style="text-align: center; margin-top: -1em;"&gt;
&lt;strong&gt;Listing 1.&lt;/strong&gt; 
An example Go program to compute the Fibonacci Sequence.
&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dgruntime&amp;quot;&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;defer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Shutdown&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}()&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;main&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;defer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ExitFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;main&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}()&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlkFromCond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nb"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fib&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;defer&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ExitFunc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;fib&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}()&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlkFromCond&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nx"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;n&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nx"&gt;dgruntime&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;EnterBlk&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;c&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div style="text-align: center; margin-top: -1em;"&gt;
&lt;strong&gt;Listing 2.&lt;/strong&gt; 
The program shown in Listing 1 after the instrumentation has been inserted by
dynagrok.
&lt;/div&gt;</content><category term="Paper"></category></entry><entry><title>Frequent Subgraph Analysis and its Software Engineering Applications</title><link href="https://hackthology.com/frequent-subgraph-analysis-and-its-software-engineering-applications.html" rel="alternate"></link><published>2017-08-18T00:00:00-04:00</published><updated>2017-08-18T00:00:00-04:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;</name></author><id>tag:hackthology.com,2017-08-18:/frequent-subgraph-analysis-and-its-software-engineering-applications.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;. &lt;em&gt;Frequent Subgraph Analysis and its Software Engineering Applications&lt;/em&gt;.
&lt;a href="http://case.edu/"&gt;Case Western Reserve University&lt;/a&gt;. Doctoral Dissertation. 2017.
&lt;br/&gt;
&lt;a href="https://hackthology.com/pdfs/dissertation.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-analysis-and-its-software-engineering-applications.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Frequent subgraph analysis is a class of techniques and algorithms to find
repeated sub-structures in graphs known as frequent subgraphs or graph
patterns. In the field of Software …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt;. &lt;em&gt;Frequent Subgraph Analysis and its Software Engineering Applications&lt;/em&gt;.
&lt;a href="http://case.edu/"&gt;Case Western Reserve University&lt;/a&gt;. Doctoral Dissertation. 2017.
&lt;br/&gt;
&lt;a href="https://hackthology.com/pdfs/dissertation.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-analysis-and-its-software-engineering-applications.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;Frequent subgraph analysis is a class of techniques and algorithms to find
repeated sub-structures in graphs known as frequent subgraphs or graph
patterns. In the field of Software Engineering, graph pattern discovery can
help detect semantic code duplication, locate the root cause of bugs, infer
program specifications, and even recommend intelligent auto-complete
suggestions.  Outside of Software Engineering, discovering graph patterns has
enabled important applications in personalized medicine, computer aided drug
design, computer vision, and multimedia.&lt;/p&gt;
&lt;p&gt;As promising as much of the previous work in areas such as semantic code
duplication detection has been, finding all of the patterns in graphs of a
large program's code has previously proven intractable.  Part of what makes
discovering all graphs patterns in a graph of a large program difficult is the
very large number of frequent subgraphs contained in graphs of large programs.
Another impediment arises when graphs contain frequent patterns with many
automorphisms and overlapping embeddings. Such patterns are pathologically
difficult to mine and are found in real programs.&lt;/p&gt;
&lt;p&gt;I present a family of algorithms and techniques for frequent subgraph analysis
with two specific aims. One, address pathological structures. Two, enable
important software engineering applications such as code clone detection and
fault localization without analyzing all frequent subgraphs. The first aim is
addressed by novel optimizations making the system faster and more scalable
than previously published work on both program graphs and other difficult to
mine graphs. The second aim is addressed by new algorithms for sampling,
ranking, and grouping frequent patterns.  Experiments and theoretical results
show the tractability of these new techniques.&lt;/p&gt;
&lt;p&gt;The power of frequent subgraph mining in Software Engineering is demonstrated
with studies on duplicate code (code clone) identification and fault
localization.  Identifying code clones from program dependence graphs allows
the identification of potential semantic clones. The proposed sampling
techniques enable tractable dependence clone identification and analysis.
Fault localization identifies potential locations for the root cause of bugs
in programs. Frequent substructures in dynamic program behavior graphs to
identify suspect behaviors which are further isolated with fully automatic
test case minimization and generation.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Frequent subgraph analysis&lt;/em&gt; (FSA) is a family of techniques to discover
recurring subgraphs in graph databases. The databases can either be composed of
many individual graphs or a single large connected graph. This dissertation
discusses my contributions to frequent subgraph analysis and applies the
technique to address two pressing problems in software engineering: code clone
detection and automatic fault localization.&lt;/p&gt;
&lt;p&gt;The work on frequent subgraph analysis was motivated by the software engineering
problems. Large programs are composed of repeated patterns arising organically
through the process of program construction. Some regions of programs are
duplicated (intentionally or unintentionally). The duplicated regions are
referred to as &lt;em&gt;code clones&lt;/em&gt; (or just &lt;em&gt;clones&lt;/em&gt;). Other regions are
similar to each other because they perform similar tasks or share development
histories.&lt;/p&gt;
&lt;p&gt;Code clones may arise from programmers copying and pasting code, from
limitations of a programming language, from using certain APIs, from following
coding conventions, or from a variety of other causes.  Whatever their causes,
the existing clones in a code base need to be managed. When a programmer
modifies a region of code that is cloned in another location in the program
they should make an active decision whether or not to modify the other location.
Clearly, such decisions can only be made if the programmer is aware of the other
location.&lt;/p&gt;
&lt;p&gt;One type of duplication which is particularly difficult to detect is so called
&lt;em&gt;Type-4 clones&lt;/em&gt; or &lt;em&gt;semantic clones&lt;/em&gt;. Semantic clones are semantically
equivalent regions of code which may or may not be textually similar.
Differences could be small changes such as different variable names or large
changes such as a different algorithms which perform the same function. In
general identifying semantically equivalent regions is undecidable as a
reduction from the halting problem.&lt;/p&gt;
&lt;p&gt;Frequent subgraph analysis (FSA) can be used to identify some &lt;em&gt;Type-4&lt;/em&gt; clones
(as well as easier to identify clone classes). I use FSA to analyze a graphical
representation of the program called the &lt;em&gt;Program Dependence Graph&lt;/em&gt; (PDG)
{&lt;a href="https://doi.org/10.1145/24039.24041"&gt;Ferrante 1987&lt;/a&gt;}. Dependence graphs strip
away syntactic information and focus on the semantic relationships between
operations.  Non-semantic re-orderings of operations in a program do not effect
the structure of its dependence graph {&lt;a href="https://doi.org/10.1145/93548.93574"&gt;Horwitz
1990&lt;/a&gt;, &lt;a href="https://doi.org/10.1145/75309.75328"&gt;Podgurski
1989&lt;/a&gt;, &lt;a href="https://doi.org/10.1109/32.58784"&gt;Podgurski
1990&lt;/a&gt;}. Since PDGs are not sensitive to
unimportant syntactic changes some of the &lt;em&gt;Type-4&lt;/em&gt; clones in a program may
be identified with FSA.&lt;/p&gt;
&lt;p&gt;The other motivating application I applied FSA to is automatic fault
localization.  When programs have faults, defects, or bugs it is often time
consuming and sometimes difficult to find the cause of the bug. To address this
the software engineering community has been working on a variety of techniques
for &lt;em&gt;automatic fault localization&lt;/em&gt;.  The family of statistical fault
localization techniques analyzes the behavior of the program when the faults
manifest and when they do not. These techniques then identify statistical
associations between execution of particular program elements and the occurrence
of program failures.&lt;/p&gt;
&lt;p&gt;While statistical measures can identify suspicious elements of a program they
are blind to the relationships between the elements. If program behavior is
modeled through &lt;em&gt;Dynamic Control Flow Graphs&lt;/em&gt;, then execution relationships
between operations can be analyzed using FSA to identify suspicious
interactions. These suspicious interactions represent larger &lt;em&gt;behaviors&lt;/em&gt; of
the program which are statistically associated with program failure. The
behaviors serve as a context of interacting suspicious program elements which
potentially makes it easier for programmers to comprehend localization results.&lt;/p&gt;
&lt;p&gt;Much of the previous work in frequent subgraph analysis has focused on finding
all of the frequent subgraphs in a graph database (called &lt;em&gt;frequent subgraph
mining&lt;/em&gt; {&lt;a href="https://dx.doi.org/10.1007/3-540-45372-5_2"&gt;Inokuchi 2000&lt;/a&gt;}). I
have shown that finding all of the frequent subgraphs in a database of graphs is
not an efficient or effective way to either detect code clones or automatically
localize faults. Program dependence graphs of large programs have huge numbers
of recurring subgraphs.  Experiments on a number of open source projects (see
Chapter 4) showed that moderately sized Java programs (~70 KLOC) have more than
a hundred of million subgraphs that recur five or more times. Mining all
recurring subgraphs is an impractical way to either identify code clones or
localize faults.&lt;/p&gt;
&lt;p&gt;Furthermore, it turns out that program dependence graphs are particularly
difficult to analyze for recurring subgraphs. These graphs often have certain
structures which contain many &lt;em&gt;automorphisms&lt;/em&gt;. A structure with an automorphism
can be rotated upon itself. Each rotation appears to be a recurrence to
traditional frequent subgraph mining algorithms. However, because it is merely a
rotation, humans (e.g. programmers) do not perceive these rotations as instances
of duplication.&lt;/p&gt;
&lt;p&gt;To enable scalable frequent subgraph analysis of large programs new techniques
were needed. I developed novel optimizations for mining frequent subgraphs and
created a state of the art miner (REGRAX) for connected graphs (Chapter 3).  To
detect code clones from program dependence graphs, I developed an algorithm
(GRAPLE) to collect a representative sample of recurring subgraphs (Chapter 4).
Finally, a new algorithm (SWRW) was created for localizing faults from dynamic
control flow graphs, which outperforms previous algorithms (Chapter 6).&lt;/p&gt;
&lt;p&gt;REGRAX contains low level optimizations to the process of identifying frequent
subgraphs. Chapter 2 provides the necessary background on frequent subgraph
mining for understanding these optimizations. An extensive empirical study was
conducted on REGRAX to quantify the effect of each of the new optimizations on
databases from the SUBDUE corpus {&lt;a href="https://dl.acm.org/citation.cfm?id=1618595.1618605"&gt;Cook
1994&lt;/a&gt;}, on program
dependence graphs, and on random graphs.&lt;/p&gt;
&lt;p&gt;GRAPLE is a new algorithm to sample a representative set of frequent subgraphs
and estimate statistics characterizing properties of the set of all frequent
subgraphs. The sampling algorithm uses the theory of absorbing Markov chains to
model the process of extracting recurring subgraphs from a large connected
graph. By sampling a representative set of recurring subgraphs GRAPLE is able to
conduct frequent subgraph analysis on large programs which normally would not be
amenable to such analysis.&lt;/p&gt;
&lt;p&gt;One of the questions in code clone detection is: "are code clones detected from
program dependence graphs understandable to programmers?" GRAPLE was used to
answer this question, as it not only collects a sample of frequent subgraphs but
allows researchers to estimate the prevalence of features across the entire
population of frequent subgraphs (including those which were not sampled).
Chapter 4 details a case study which was conducted at a software company to
determine whether their programmers could make use of code clones detected from
program dependence graphs. The study would not have been possible without the
estimation framework in GRAPLE, as the software contained too many code clones
to be reviewed in the allocated budget.&lt;/p&gt;
&lt;p&gt;To apply frequent subgraph analysis to automatic fault localization, a new
algorithm named Score Weighted Random Walks (SWRW) was developed. SWRW samples
discriminative, suspicious, or significant subgraphs from a database of graphs.
The database is split into multiple classes where some graphs are labeled
"positive" and others "negative." In fault localization the "positive" graphs
were those dynamic control flow graphs collected from program executions which
exhibited a failure of some type. The "negative" graphs are from executions
which did not fail.&lt;/p&gt;
&lt;p&gt;SWRW, like GRAPLE, models the problem using the theory of absorbing Markov
chains. Unlike GRAPLE, it uses an &lt;em&gt;objective function&lt;/em&gt; (drawn from the
statistical fault localization literature {&lt;a href="https://dx.doi.org/10.1002/smr.1616"&gt;Lucia
2014&lt;/a&gt;}) to guide the sampling process. In
comparison to previous work in fault localization using graph mining, a much
wider variety of objective functions can applied. This allows for functions
better suited to statistical fault localization to be used as the objective
function. SWRW outperforms previous approaches which used discriminative mining
to localize faults in terms of fault localization accuracy.&lt;/p&gt;
&lt;h4&gt;Summary&lt;/h4&gt;
&lt;p&gt;This dissertation makes important and novel contributions to frequent subgraph
analysis which enable scalable semantic code clone detection and behavioral
fault localization. These advances can help programmers maintain their software
more efficiently leading to more stable and secure software for everyone. The
software engineering advances are built on new frequent subgraph analysis
algorithms. The new algorithms improve code clone detection time, fault
localization latency and accuracy, and enable analysis of larger and more
complex programs.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hackthology.com/pdfs/dissertation.pdf"&gt;Read the full dissertation&lt;/a&gt;.&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Rethinking Dependence Clones</title><link href="https://hackthology.com/rethinking-dependence-clones.html" rel="alternate"></link><published>2017-02-21T00:00:00-05:00</published><updated>2017-02-21T00:00:00-05:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt; and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2017-02-21:/rethinking-dependence-clones.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Rethinking Dependence Clones&lt;/em&gt;.
&lt;a href="https://iwsc2017.github.io/"&gt;IWSC 2017&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/IWSC.2017.7880512"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/iwsc-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/rethinking-dependence-clones.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Semantic code clones&lt;/em&gt; are regions of duplicated code that may appear dissimilar
but compute similar functions. Since in general it is algorithmically
undecidable whether two or more programs compute the same function, locating all …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski.
&lt;em&gt;Rethinking Dependence Clones&lt;/em&gt;.
&lt;a href="https://iwsc2017.github.io/"&gt;IWSC 2017&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://doi.org/10.1109/IWSC.2017.7880512"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/iwsc-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/rethinking-dependence-clones.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Semantic code clones&lt;/em&gt; are regions of duplicated code that may appear dissimilar
but compute similar functions. Since in general it is algorithmically
undecidable whether two or more programs compute the same function, locating all
semantic code clones is infeasible. One way to dodge the undecidability issue
and find potential semantic clones, using only static information, is to search
for recurring subgraphs of a &lt;em&gt;program dependence graph&lt;/em&gt; (PDG).  PDGs represent
control and data dependence relationships between statements or operations in a
program.  PDG-based clone detection techniques, unlike syntactically-based
techniques, do not distinguish between code fragments that differ only because
of dependence-preserving statement re-orderings, which also preserve semantics.
Consequently, they detect clones that are difficult to find by other means.
Despite this very desirable property, work on PDG-based clone detection has
largely stalled, apparently because of concerns about the scalability of the
approach.  We argue, however, that the time has come to reconsider PDG-based
clone detection, as a part of a holistic strategy for clone management.  We
present evidence that its scalability problems are not as severe as previously
thought.  This suggests the possibility of developing integrated clone
management systems that fuse information from multiple clone detection methods,
including PDG-based ones.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Fragments of similar code are typically scattered throughout large code bases.
These repeated fragments or &lt;em&gt;code clones&lt;/em&gt; often result from programmers copying
and pasting code.  Code clones (or just &lt;em&gt;clones&lt;/em&gt;) may also result from
limitations of a programming language, use of certain APIs or design patterns,
following coding conventions, or a variety of other causes.  Whatever their
causes, existing clones need to be managed. When a programmer modifies a region
of code that is cloned in another location in the program, they should make an
active decision whether or not to modify the other location.  Clearly, such
decisions can only be made if the programmer is aware of the other location.&lt;/p&gt;
&lt;p&gt;In general, there are 4 types of code clones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Type-1 Clones -&lt;/strong&gt; Identical regions of code (excepting whitespace and
  comments).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type-2 Clones -&lt;/strong&gt; Syntactically equivalent regions (excepting names,
  literals, types, and comments).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type-3 Clones -&lt;/strong&gt; Syntactically similar regions (as in Type-2) but with
  minor differences such as statement additions or deletions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Type-4 Clones -&lt;/strong&gt; Regions of code with functionally equivalent behavior but
  possibly with different syntactic structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Much of the research on code clone detection and maintenance has been geared
toward Type-1 and Type-2 clone, as they are easier to detect and validate than
Type-3 and Type-4 clones.  The two most popular detection methods involve
searching for clones in &lt;em&gt;token streams&lt;/em&gt; and &lt;em&gt;abstract syntax trees&lt;/em&gt; (ASTs).&lt;/p&gt;
&lt;p&gt;An alternative approach to clone detection is to search for them in a &lt;em&gt;Program
Dependence Graph&lt;/em&gt; (PDG), which represents the control and data dependences
between statements or operations in a program.  Recurring subgraphs in PDGs
represent potential &lt;em&gt;dependence clones&lt;/em&gt;.  Some of the previous work on PDG-based
clone detection used forward and backward path-slicing to find clones.  This
method can detect matching slices, but it cannot detect all recurring subgraphs.
The latter can be identified using &lt;em&gt;frequent subgraph mining&lt;/em&gt; (FSM).  However,
for low frequency thresholds, the number of PDG subgraphs discovered by FSM may
be enormous.  For example, we found that for a Java program with 70,000 lines of
code (LOC), over 700 million PDG subgraphs with 5 or more instances were
discovered by FSM.&lt;/p&gt;
&lt;p&gt;Since it is infeasible for developers to examine so many subgraphs, we
previously developed &lt;a href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html"&gt;GRAPLE&lt;/a&gt;, an algorithm to
select representative samples of maximal frequent subgraphs.  In this paper, the
core sampling process remains the same as in GRAPLE but we present a new
algorithm for traversing the &lt;em&gt;k&lt;/em&gt;-frequent subgraph lattice.  One tricky aspect
of FSM is how to define exactly what "frequency" means in a large connected
graph. In order to handle pathological cases that occur in real programs, we
introduce a new metric to measure subgraph frequency (or "support"), called
the &lt;em&gt;Greedy Independent Subgraphs&lt;/em&gt; (GIS) measure.  The results section details
the first empirical examination of the scalability and speed of sampling
dependence clones from large programs. The study showed that our new system can
quickly sample from programs with 500 KLOC of code and successfully sample from
programs with perhaps 2 MLOC. Finally, since at times the sampling algorithm may
return several potential clones, which are quite similar to each other, we
evaluate the performance of a density-based clustering algorithm on the samples
collected.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;p&gt;See the &lt;a href="https://hackthology.com/pdfs/iwsc-2017.pdf"&gt;PDF&lt;/a&gt; for the complete paper. IEEE has
the exclusive rights to publish this paper. Follow the
&lt;a href="https://doi.org/10.1109/IWSC.2017.7880512"&gt;DOI&lt;/a&gt; for the IEEE copy.&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Frequent Subgraph Mining of Personalized Signaling Pathway Networks Groups Patients with Frequently Dysregulated Disease Pathways and Predicts Prognosis</title><link href="https://hackthology.com/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html" rel="alternate"></link><published>2017-01-03T00:00:00-05:00</published><updated>2017-01-03T00:00:00-05:00</updated><author><name>Arda Durmaz, &lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt;, Doug Brubaker, and &lt;a href="http://gurkan.case.edu"&gt;Gurkan Bebek&lt;/a&gt;</name></author><id>tag:hackthology.com,2017-01-03:/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html</id><summary type="html">&lt;p&gt;A. Durmaz*, &lt;strong&gt;T. A. D. Henderson&lt;/strong&gt;*, D.  Brubaker, and G. Bebek. &lt;em&gt;Frequent
Subgraph Mining of Personalized Signaling Pathway Networks Groups Patients with
Frequently Dysregulated Disease Pathways and Predicts Prognosis.&lt;/em&gt;
&lt;a href="http://psb.stanford.edu/"&gt;PSB 2017&lt;/a&gt;.  * &lt;strong&gt;Co-First Author&lt;/strong&gt;
&lt;br/&gt;
&lt;a href="http://dx.doi.org/10.1142/9789813207813_0038"&gt;DOI&lt;/a&gt;.
&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/27896993"&gt;PUBMED&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;h4&gt;Motivation&lt;/h4&gt;
&lt;p&gt;Large scale genomics studies have generated comprehensive molecular
characterization of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A. Durmaz*, &lt;strong&gt;T. A. D. Henderson&lt;/strong&gt;*, D.  Brubaker, and G. Bebek. &lt;em&gt;Frequent
Subgraph Mining of Personalized Signaling Pathway Networks Groups Patients with
Frequently Dysregulated Disease Pathways and Predicts Prognosis.&lt;/em&gt;
&lt;a href="http://psb.stanford.edu/"&gt;PSB 2017&lt;/a&gt;.  * &lt;strong&gt;Co-First Author&lt;/strong&gt;
&lt;br/&gt;
&lt;a href="http://dx.doi.org/10.1142/9789813207813_0038"&gt;DOI&lt;/a&gt;.
&lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/27896993"&gt;PUBMED&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/psb-2017-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://hackthology.com/frequent-subgraph-mining-of-personalized-signaling-pathway-networks-groups-patients-with-frequently-dysregulated-disease-pathways-and-predicts-prognosis.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;h4&gt;Motivation&lt;/h4&gt;
&lt;p&gt;Large scale genomics studies have generated comprehensive molecular
characterization of numerous cancer types. Subtypes for many tumor types have
been established; however, these classifications are based on molecular
characteristics of a small gene sets with limited power to detect dysregulation
at the patient level.  We hypothesize that frequent graph mining of pathways to
gather pathways functionally relevant to tumors can characterize tumor types and
provide opportunities for personalized therapies.&lt;/p&gt;
&lt;h4&gt;Results&lt;/h4&gt;
&lt;p&gt;In this study we present an integrative omics approach to group patients based
on their altered pathway characteristics and show prognostic differences within
breast cancer (p &amp;lt; 9.57e-10) and glioblastoma multiforme (p &amp;lt; 0.05) patients. We
were able validate this approach in secondary RNA-Seq datasets with p &amp;lt; 0.05 and
p &amp;lt; 0.01 respectively. We also performed pathway enrichment analysis to further
investigate the biological relevance of dysregulated pathways. We compared our
approach with network-based classifier algorithms and showed that our
unsupervised approach generates more robust and biologically relevant clustering
whereas previous approaches failed to report specific functions for similar
patient groups or classify patients into prognostic groups.&lt;/p&gt;
&lt;h4&gt;Conclusions&lt;/h4&gt;
&lt;p&gt;These results could serve as a means to improve
prognosis for future cancer patients, and to provide opportunities for
improved treatment options and personalized interventions.  The proposed novel 
graph mining approach is able to integrate PPI networks 
with gene expression in a biologically sound approach and cluster patients in to
clinically distinct groups. We have utilized breast cancer and glioblastoma
multiforme datasets from microarray and RNA-Seq platforms and identified
disease mechanisms differentiating samples.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;p&gt;See the &lt;a href="https://hackthology.com/pdfs/psb-2017.pdf"&gt;PDF&lt;/a&gt; for the complete paper. Follow the
&lt;a href="http://dx.doi.org/10.1142/9789813207813_0038"&gt;DOI&lt;/a&gt; for the authoritative
version.&lt;/p&gt;</content><category term="Paper"></category></entry><entry><title>Sampling Code Clones from Program Dependence Graphs with GRAPLE</title><link href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html" rel="alternate"></link><published>2016-11-13T00:00:00-05:00</published><updated>2016-11-13T00:00:00-05:00</updated><author><name>&lt;a href="http://hackthology.com"&gt;Tim Henderson&lt;/a&gt; and &lt;a href="http://engineering.case.edu/profiles/hap"&gt;Andy Podgurski&lt;/a&gt;</name></author><id>tag:hackthology.com,2016-11-13:/sampling-code-clones-from-program-dependence-graphs-with-graple.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski. &lt;em&gt;Sampling Code Clones from Program
Dependence Graphs with GRAPLE&lt;/em&gt;.
&lt;a href="http://softwareanalytics.ca/swan16/Home.html"&gt;SWAN 2016&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1145/2989238.2989241"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://github.com/timtadh/graple"&gt;CODE&lt;/a&gt;.
&lt;a href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present &lt;a href="https://github.com/timtadh/graple"&gt;GRAPLE&lt;/a&gt;, a method to generate a
representative sample of recurring (frequent) subgraphs of any directed labeled
graph(s).  &lt;code&gt;GRAPLE&lt;/code&gt; is based on frequent subgraph …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Tim A. D. Henderson&lt;/strong&gt; and Andy Podgurski. &lt;em&gt;Sampling Code Clones from Program
Dependence Graphs with GRAPLE&lt;/em&gt;.
&lt;a href="http://softwareanalytics.ca/swan16/Home.html"&gt;SWAN 2016&lt;/a&gt;.
&lt;br/&gt;
&lt;a href="https://dx.doi.org/10.1145/2989238.2989241"&gt;DOI&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;PDF&lt;/a&gt;.
&lt;a href="https://hackthology.com/pdfs/swan-2016-supplemental.pdf"&gt;SUPPLEMENT&lt;/a&gt;.
&lt;a href="https://github.com/timtadh/graple"&gt;CODE&lt;/a&gt;.
&lt;a href="https://hackthology.com/sampling-code-clones-from-program-dependence-graphs-with-graple.html"&gt;WEB&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Abstract&lt;/h4&gt;
&lt;p&gt;We present &lt;a href="https://github.com/timtadh/graple"&gt;GRAPLE&lt;/a&gt;, a method to generate a
representative sample of recurring (frequent) subgraphs of any directed labeled
graph(s).  &lt;code&gt;GRAPLE&lt;/code&gt; is based on frequent subgraph mining, absorbing Markov
chains, and Horvitz-Thompson estimation. It can be used to sample any kind of
graph representation for programs. One of many software engineering applications
for finding recurring subgraphs is detecting duplicated code (code clones) from
representations such as program dependence graphs (PDGs) and abstract syntax
trees.  To assess the usefulness of clones detected from PDGs, we conducted a
case study on a 73 KLOC commercial Android application developed over 5 years.
Nine of the application's developers participated. To our knowledge, it is the
first study to have professional developers examine code clones detected from
PDGs.  We describe a new PDG generation tool
&lt;a href="https://github.com/timtadh/jpdg"&gt;jpdg&lt;/a&gt; for JVM languages, which was used to
generate the dependence graphs used in the study.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Code clones&lt;/em&gt; are similar fragments of program code. They can arise from copying
and pasting, using certain design patterns or certain APIs, or adhering to
coding conventions, among other causes. Code clones create maintenance hazards,
because they often require subtle context-dependent adaptation and because other
changes must be applied to each member of a clone class. To manage clone
evolution the clones must first be found. Clones can be detected using any
program representation: source code text, tokens, abstract syntax trees (ASTs),
flow graphs, dependence graphs, etc. Each representation has advantages and
disadvantages for clone detection.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PDG-based clone detection&lt;/em&gt; finds &lt;em&gt;dependence clones&lt;/em&gt; corresponding to recurring
subgraphs of a program dependence graph (PDG).  Since PDGs are oblivious to
semantics preserving statement reorderings they are well suited to detect
&lt;em&gt;semantic&lt;/em&gt; (functionally equivalent) clones. A number of algorithms find clones
from PDGs.  However, as Bellon notes, "PDG based techniques are computationally
expensive and often report non-contiguous clones that may not be perceived as
clones by a human evaluator." Most PDG-based clone detection tools are biased,
detecting certain clones but not others.&lt;/p&gt;
&lt;p&gt;The root cause of scalability problems with PDG-based clone detection is the
number of dependence clones. The Background Section (in the
&lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;pdf&lt;/a&gt;) illustrates this with an example in which
we used an unbiased frequent subgraph mining algorithm to detect all dependence
clones in Java programs. In programs with about 70 KLOC it detected around 10
million clones before disk space was exhausted. Processing all dependence clones
is impractical even for modestly sized programs.&lt;/p&gt;
&lt;p&gt;Instead of exhaustively enumerating all dependence clones, an unbiased random
sample can be used to statistically estimate parameters of the whole
"population" of clones, such as the prevalence of clones exhibiting properties
of interest.  For these reasons, we developed a statistically unbiased method
for &lt;em&gt;sampling&lt;/em&gt; dependence clones and for &lt;em&gt;estimating&lt;/em&gt; parameters of the whole
clone population.&lt;/p&gt;
&lt;p&gt;We present &lt;a href="https://github.com/timtadh/graple"&gt;GRAPLE (GRAph samPLE)&lt;/a&gt;, a method
to generate a representative sample of recurring subgraphs of any directed
labeled graph(s). It can be used to sample subgraphs from any kind of program
graph representation.  &lt;code&gt;GRAPLE&lt;/code&gt; is not a general purpose clone detector but it
can answer questions about dependence clones that other PDG-based clone
detection tools cannot.  We conducted a preliminary case study on a commercial
application and had its developers evaluate whether the sampled subgraphs
represented code duplication.  To our knowledge, it is the first study to have
professional programmers examine dependence clones.  &lt;code&gt;GRAPLE&lt;/code&gt; has applications
in bug mining, test case selection, and bioinformatics. The sampling algorithm
also applies to frequent item sets, subsequences, and subtrees allowing code
clone sampling from tokens and ASTs.&lt;/p&gt;
&lt;h4&gt;Note&lt;/h4&gt;
&lt;p&gt;See the &lt;a href="https://hackthology.com/pdfs/swan-2016.pdf"&gt;PDF&lt;/a&gt; for the complete paper. ACM
has the exclusive rights to publish this paper. Tim Henderson and Andy Podgurski
own the copyright. Follow the &lt;a href="https://dx.doi.org/10.1145/2989238.2989241"&gt;DOI&lt;/a&gt;
for the ACM copy. This copy is posted here with the permission of ACM.&lt;/p&gt;</content><category term="Paper"></category></entry></feed>